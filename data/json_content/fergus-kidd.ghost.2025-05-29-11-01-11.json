{"db":[{"meta":{"exported_on":1748516471213,"version":"4.47.4"},"data":{"custom_theme_settings":[{"id":"628bad90d182030001125f07","theme":"casper","key":"title_font","type":"select","value":"Modern sans-serif"},{"id":"628bad90d182030001125f08","theme":"casper","key":"body_font","type":"select","value":"Elegant serif"},{"id":"628bad90d182030001125f09","theme":"casper","key":"publication_cover_style","type":"select","value":"Medium"},{"id":"628bad90d182030001125f0a","theme":"casper","key":"feed_layout","type":"select","value":"Dynamic grid"},{"id":"628bad90d182030001125f0b","theme":"casper","key":"color_scheme","type":"select","value":"Light"},{"id":"628bad90d182030001125f0c","theme":"casper","key":"header_button_background","type":"color","value":"#ffffff"},{"id":"628bad90d182030001125f0d","theme":"casper","key":"header_button_text_color","type":"select","value":"Dark"},{"id":"628bad91d182030001125f0e","theme":"casper","key":"post_image_width","type":"select","value":"Wide"},{"id":"628bad91d182030001125f0f","theme":"casper","key":"email_signup_for_logged_out_visitors","type":"select","value":"Footer"},{"id":"628bad91d182030001125f10","theme":"casper","key":"email_signup_text","type":"text","value":"Sign up for more like this."},{"id":"628bad91d182030001125f11","theme":"casper","key":"show_recent_posts","type":"boolean","value":"true"},{"id":"628ce6dd2c055d000197a156","theme":"ruby","key":"title_font","type":"select","value":"Modern sans-serif"},{"id":"628ce6dd2c055d000197a157","theme":"ruby","key":"body_font","type":"select","value":"Modern sans-serif"},{"id":"628ce6dd2c055d000197a158","theme":"ruby","key":"show_sidebar_on_homepage","type":"boolean","value":"true"},{"id":"628ce6dd2c055d000197a159","theme":"ruby","key":"show_related_posts","type":"boolean","value":"true"},{"id":"628ce6dd2c055d000197a15a","theme":"ruby","key":"show_sidebar_on_post","type":"boolean","value":"true"},{"id":"628ce6dd2c055d000197a15b","theme":"ruby","key":"show_share_links","type":"boolean","value":"true"},{"id":"628ce6dd2c055d000197a15c","theme":"ruby","key":"show_author","type":"boolean","value":"true"}],"posts":[{"id":"628bad7bd182030001125da1","uuid":"ef719382-0370-471b-834e-090f432035ad","title":"Coming soon","slug":"coming-soon","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"#/portal/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This is Fergus' Fantastic Fixes, a brand new site by Fergus Kidd that's just getting started. Things will be up and running here shortly, but you can \"],[0,[0],1,\"subscribe\"],[0,[],0,\" in the meantime if you'd like to stay up to date and receive emails when new content is published!\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>This is Fergus' Fantastic Fixes, a brand new site by Fergus Kidd that's just getting started. Things will be up and running here shortly, but you can <a href=\"#/portal/\">subscribe</a> in the meantime if you'd like to stay up to date and receive emails when new content is published!</p>","comment_id":"628bad7bd182030001125da1","plaintext":"This is Fergus' Fantastic Fixes, a brand new site by Fergus Kidd that's just\ngetting started. Things will be up and running here shortly, but you can \nsubscribe in the meantime if you'd like to stay up to date and receive emails\nwhen new content is published!","feature_image":"https://static.ghost.org/v4.0.0/images/feature-image.jpg","featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-05-23T15:51:23.000Z","updated_at":"2022-05-24T12:09:50.000Z","published_at":"2022-05-23T15:51:23.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"628bad7dd182030001125da3","uuid":"c66c446a-0f22-455e-a98f-586399ca9c82","title":"About this site","slug":"about","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://ghost.org\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Fergus' Fantastic Fixes is an independent publication launched in May 2022 by Fergus Kidd. If you subscribe today, you'll get full access to the website as well as email newsletters about new content when it's available. Your subscription makes this site possible, and allows Fergus' Fantastic Fixes to continue to exist. Thank you!\"]]],[1,\"h3\",[[0,[],0,\"Access all areas\"]]],[1,\"p\",[[0,[],0,\"By signing up, you'll get access to the full archive of everything that's been published before and everything that's still to come. Your very own private library.\"]]],[1,\"h3\",[[0,[],0,\"Fresh content, delivered\"]]],[1,\"p\",[[0,[],0,\"Stay up to date with new content sent straight to your inbox! No more worrying about whether you missed something because of a pesky algorithm or news feed.\"]]],[1,\"h3\",[[0,[],0,\"Meet people like you\"]]],[1,\"p\",[[0,[],0,\"Join a community of other subscribers who share the same interests.\"]]],[10,0],[1,\"h3\",[[0,[],0,\"Start your own thing\"]]],[1,\"p\",[[0,[],0,\"Enjoying the experience? Get started for free and set up your very own subscription business using \"],[0,[0],1,\"Ghost\"],[0,[],0,\", the same platform that powers this website.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>Fergus' Fantastic Fixes is an independent publication launched in May 2022 by Fergus Kidd. If you subscribe today, you'll get full access to the website as well as email newsletters about new content when it's available. Your subscription makes this site possible, and allows Fergus' Fantastic Fixes to continue to exist. Thank you!</p><h3 id=\"access-all-areas\">Access all areas</h3><p>By signing up, you'll get access to the full archive of everything that's been published before and everything that's still to come. Your very own private library.</p><h3 id=\"fresh-content-delivered\">Fresh content, delivered</h3><p>Stay up to date with new content sent straight to your inbox! No more worrying about whether you missed something because of a pesky algorithm or news feed.</p><h3 id=\"meet-people-like-you\">Meet people like you</h3><p>Join a community of other subscribers who share the same interests.</p><hr><h3 id=\"start-your-own-thing\">Start your own thing</h3><p>Enjoying the experience? Get started for free and set up your very own subscription business using <a href=\"https://ghost.org\">Ghost</a>, the same platform that powers this website.</p>","comment_id":"628bad7dd182030001125da3","plaintext":"Fergus' Fantastic Fixes is an independent publication launched in May 2022 by\nFergus Kidd. If you subscribe today, you'll get full access to the website as\nwell as email newsletters about new content when it's available. Your\nsubscription makes this site possible, and allows Fergus' Fantastic Fixes to\ncontinue to exist. Thank you!\n\nAccess all areas\nBy signing up, you'll get access to the full archive of everything that's been\npublished before and everything that's still to come. Your very own private\nlibrary.\n\nFresh content, delivered\nStay up to date with new content sent straight to your inbox! No more worrying\nabout whether you missed something because of a pesky algorithm or news feed.\n\nMeet people like you\nJoin a community of other subscribers who share the same interests.\n\n\n--------------------------------------------------------------------------------\n\nStart your own thing\nEnjoying the experience? Get started for free and set up your very own\nsubscription business using Ghost [https://ghost.org], the same platform that\npowers this website.","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-05-23T15:51:25.000Z","updated_at":"2022-05-23T15:52:37.000Z","published_at":"2022-05-23T15:51:24.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"628baf4ed182030001125f19","uuid":"5530b647-216e-4b05-abc1-347924701e66","title":"About Fergus","slug":"about-fergus","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/05/smaller.jpeg\",\"width\":977,\"height\":1035,\"cardWidth\":\"\"}],[\"html\",{\"html\":\"<div style=\\\"display: flex; justify-content: center;\\\" data-iframe-width=\\\"150\\\" data-iframe-height=\\\"270\\\" data-share-badge-id=\\\"d452446e-2ea0-4d42-a8e8-a406f24f055d\\\" data-share-badge-host=\\\"https://www.credly.com\\\"></div>  \\n<script type=\\\"text/javascript\\\" async src=\\\"//cdn.credly.com/assets/utilities/embed.js\\\"></script> \"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[[0,[],0,\"Fergus has an academic background in physics and space science. At University College London, Fergus worked on computer vision systems for the ExoMars rover for its planned 2018 mission to Mars. Although sadly, to date, the rover still sits on Earth, this ignited a passion for AI and technology. Fergus has eight years of experience working in AI and has accolades, including a patent in synthetic vision data generation and an OpenUK award nomination for sustainability in software. Today Fergus is Avanadeâ€™s Emerging Technology R&D Engineering lead. He works on developing assets and demonstrations around new and future technologies, focussing on innovative use cases to show the power of new technologies in the enterprise. Fergus is passionate about STEM education and works with students from school ages to postgraduate to inspire them in their academic journey and encourage the next generation of scientists and engineers.\"]]],[10,1],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/05/smaller.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"977\" height=\"1035\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/smaller.jpeg 600w, __GHOST_URL__/content/images/2022/05/smaller.jpeg 977w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Fergus has an academic background in physics and space science. At University College London, Fergus worked on computer vision systems for the ExoMars rover for its planned 2018 mission to Mars. Although sadly, to date, the rover still sits on Earth, this ignited a passion for AI and technology. Fergus has eight years of experience working in AI and has accolades, including a patent in synthetic vision data generation and an OpenUK award nomination for sustainability in software. Today Fergus is Avanadeâ€™s Emerging Technology R&amp;D Engineering lead. He works on developing assets and demonstrations around new and future technologies, focussing on innovative use cases to show the power of new technologies in the enterprise. Fergus is passionate about STEM education and works with students from school ages to postgraduate to inspire them in their academic journey and encourage the next generation of scientists and engineers.</p><!--kg-card-begin: html--><div style=\"display: flex; justify-content: center;\" data-iframe-width=\"150\" data-iframe-height=\"270\" data-share-badge-id=\"d452446e-2ea0-4d42-a8e8-a406f24f055d\" data-share-badge-host=\"https://www.credly.com\"></div>  \n<script type=\"text/javascript\" async src=\"//cdn.credly.com/assets/utilities/embed.js\"></script> <!--kg-card-end: html-->","comment_id":"628baf4ed182030001125f19","plaintext":"Fergus has an academic background in physics and space science. At University\nCollege London, Fergus worked on computer vision systems for the ExoMars rover\nfor its planned 2018 mission to Mars. Although sadly, to date, the rover still\nsits on Earth, this ignited a passion for AI and technology. Fergus has eight\nyears of experience working in AI and has accolades, including a patent in\nsynthetic vision data generation and an OpenUK award nomination for\nsustainability in software. Today Fergus is Avanadeâ€™s Emerging Technology R&D\nEngineering lead. He works on developing assets and demonstrations around new\nand future technologies, focussing on innovative use cases to show the power of\nnew technologies in the enterprise. Fergus is passionate about STEM education\nand works with students from school ages to postgraduate to inspire them in\ntheir academic journey and encourage the next generation of scientists and\nengineers.","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-05-23T15:59:10.000Z","updated_at":"2024-11-07T10:04:43.000Z","published_at":"2022-05-23T16:01:46.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"628bb170d182030001125f2e","uuid":"a3b99f5f-0e1e-44d6-bcb9-b2726093ba20","title":"3D art","slug":"3d-art","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/05/mS86MdIk.png\",\"width\":1920,\"height\":1080,\"caption\":\"30 Cannon Street\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/05/jar-o-heads.png\",\"width\":1920,\"height\":1080,\"caption\":\"Jar o'heads\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/05/Faces.png\",\"width\":1632,\"height\":1316,\"caption\":\"Heads\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"Some 3D renderings I have made as part of my exploration of 3D design tools and game engines for use in creating metaversere content.\"]]],[10,0],[10,1],[10,2],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>Some 3D renderings I have made as part of my exploration of 3D design tools and game engines for use in creating metaversere content.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/05/mS86MdIk.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/mS86MdIk.png 600w, __GHOST_URL__/content/images/size/w1000/2022/05/mS86MdIk.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/05/mS86MdIk.png 1600w, __GHOST_URL__/content/images/2022/05/mS86MdIk.png 1920w\" sizes=\"(min-width: 720px) 720px\"><figcaption>30 Cannon Street</figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/05/jar-o-heads.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/jar-o-heads.png 600w, __GHOST_URL__/content/images/size/w1000/2022/05/jar-o-heads.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/05/jar-o-heads.png 1600w, __GHOST_URL__/content/images/2022/05/jar-o-heads.png 1920w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Jar o'heads</figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/05/Faces.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1632\" height=\"1316\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/Faces.png 600w, __GHOST_URL__/content/images/size/w1000/2022/05/Faces.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/05/Faces.png 1600w, __GHOST_URL__/content/images/2022/05/Faces.png 1632w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Heads</figcaption></figure>","comment_id":"628bb170d182030001125f2e","plaintext":"Some 3D renderings I have made as part of my exploration of 3D design tools and\ngame engines for use in creating metaversere content.\n\n30 Cannon StreetJar o'headsHeads","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-05-23T16:08:16.000Z","updated_at":"2022-05-24T11:46:27.000Z","published_at":"2022-05-23T16:10:17.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"628cc6432c055d000197a0f2","uuid":"522797db-be36-409a-aaf2-9b73b26ad06e","title":"Project Malmo","slug":"project-malmo","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"caption\":\"Project Malmo\",\"src\":\"__GHOST_URL__/content/images/2022/05/image-4.png\",\"width\":936,\"height\":508}],[\"video\",{\"loop\":false,\"caption\":\"Malmo map solving with lava\",\"src\":\"__GHOST_URL__/content/media/2022/05/MapSolve.mp4\",\"fileName\":\"MapSolve.mp4\",\"width\":1920,\"height\":1080,\"duration\":22.288933,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2022/05/media-thumbnail-ember1022.jpg\",\"thumbnailWidth\":1920,\"thumbnailHeight\":1080}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"Microsoft Research recently publicised their use of Minecraft, and project Malmo, to explore agent based AI approaches to complex problems. Below is a quick GIF of a project Malmo Minecraft agent using Q-learning to learn to avoid lava.\"]]],[10,0],[1,\"p\",[[0,[],0,\"The aim of this game is to reach the blue block as quickly as possible, without dying a horrible death of course. As an added difficulty the agent has no visibility of the space, and must learn to avoid the lava using only trial, error, and reinforcement learning. Using this approach (Q-learning to be exact) the agent can quickly map out the safe path, then learn to navigate it faster.\"],[1,[],0,0],[0,[],0,\"This is still a pretty simple demonstrative algorithm, but the environment and the ability to use Minecraft characters as experimental agents in an easy to understand and visual way is really cool and I hope to build and share more complex and multi agent scenarios soon!\"]]],[10,1],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>Microsoft Research recently publicised their use of Minecraft, and project Malmo, to explore agent based AI approaches to complex problems. Below is a quick GIF of a project Malmo Minecraft agent using Q-learning to learn to avoid lava.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/05/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"936\" height=\"508\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/image-4.png 600w, __GHOST_URL__/content/images/2022/05/image-4.png 936w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Project Malmo</figcaption></figure><p>The aim of this game is to reach the blue block as quickly as possible, without dying a horrible death of course. As an added difficulty the agent has no visibility of the space, and must learn to avoid the lava using only trial, error, and reinforcement learning. Using this approach (Q-learning to be exact) the agent can quickly map out the safe path, then learn to navigate it faster.<br>This is still a pretty simple demonstrative algorithm, but the environment and the ability to use Minecraft characters as experimental agents in an easy to understand and visual way is really cool and I hope to build and share more complex and multi agent scenarios soon!</p><figure class=\"kg-card kg-video-card kg-card-hascaption\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2022/05/MapSolve.mp4\" poster=\"https://img.spacergif.org/v1/1920x1080/0a/spacer.png\" width=\"1920\" height=\"1080\" playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2022/05/media-thumbnail-ember1022.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div><figcaption>Malmo map solving with lava</figcaption></figure>","comment_id":"628cc6432c055d000197a0f2","plaintext":"Microsoft Research recently publicised their use of Minecraft, and project\nMalmo, to explore agent based AI approaches to complex problems. Below is a\nquick GIF of a project Malmo Minecraft agent using Q-learning to learn to avoid\nlava.\n\nProject MalmoThe aim of this game is to reach the blue block as quickly as\npossible, without dying a horrible death of course. As an added difficulty the\nagent has no visibility of the space, and must learn to avoid the lava using\nonly trial, error, and reinforcement learning. Using this approach (Q-learning\nto be exact) the agent can quickly map out the safe path, then learn to navigate\nit faster.\nThis is still a pretty simple demonstrative algorithm, but the environment and\nthe ability to use Minecraft characters as experimental agents in an easy to\nunderstand and visual way is really cool and I hope to build and share more\ncomplex and multi agent scenarios soon!\n\n0:00/1Ã—Malmo map solving with lava","feature_image":"__GHOST_URL__/content/images/2022/05/minecraft-lava-trap.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-05-24T11:49:23.000Z","updated_at":"2022-05-24T14:15:00.000Z","published_at":"2021-09-14T12:09:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"628ccbf62c055d000197a103","uuid":"b9ae5c0f-9888-426e-9023-d4813d1ac084","title":"NextMind Experimentation","slug":"nextmind-experimentation","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/05/image-5.png\",\"width\":936,\"height\":1284,\"caption\":\"A NextMind device tracking PIN input\"}],[\"video\",{\"loop\":false,\"src\":\"__GHOST_URL__/content/media/2022/05/block.mp4\",\"fileName\":\"block.mp4\",\"width\":1920,\"height\":1080,\"duration\":17.5175,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2022/05/media-thumbnail-ember831.jpg\",\"thumbnailWidth\":1920,\"thumbnailHeight\":1080}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"I've been lucky enough to play around with a NextMind dev kit. These awesome devices use electrodes that are pressed to the back of the head to.... well... read your mind.\"]]],[1,\"p\",[[1,[],0,0],[0,[],0,\"Once calibrated, the NextMind devices can utilise your 'focus' on a screen, to eerily predict what part of a screen you are looking at. The best way i can describe what's happening is to explain it as eye tracking software - except no cameras. Rather than looking at your eyes to see what you are focusing on, NextMind looks into your mind through the back of your head to achieve this.\"]]],[10,0],[1,\"p\",[[0,[],0,\"The result is honestly scary. With a good calibration score the accuracy is very very good. There is, understandably, a learning curve of how to focus your attention for the device, and also a slight delay in the reaction of the device.\"],[1,[],0,1],[0,[],0,\"A interesting benefit of the PIN demo show above is that a 'discreet' mode can be activated, so even onlookers won't know what is being input, it's a secret between your brain, focus, the NextMind, and your computer.\"],[1,[],0,2],[0,[],0,\"And I know what you're thinking. Maybe it knows what you're SUPPOSED to be thinking. especially in terms of a game, or a PIN number, it's reasonable to expect the system knows the expected result and could make a better guess of what you input. This falls down if you purposefully do incorrect inputs. I can confirm, even when purposefully selecting incorrect options, the NextMind matches what you select perfectly.\"]]],[1,\"p\",[[1,[],0,3],[0,[],0,\"How exactly it works we don't know, but its certainly good, and its certainly unnerving...\"]]],[10,1],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>I've been lucky enough to play around with a NextMind dev kit. These awesome devices use electrodes that are pressed to the back of the head to.... well... read your mind.</p><p><br>Once calibrated, the NextMind devices can utilise your 'focus' on a screen, to eerily predict what part of a screen you are looking at. The best way i can describe what's happening is to explain it as eye tracking software - except no cameras. Rather than looking at your eyes to see what you are focusing on, NextMind looks into your mind through the back of your head to achieve this.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/05/image-5.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"936\" height=\"1284\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/image-5.png 600w, __GHOST_URL__/content/images/2022/05/image-5.png 936w\" sizes=\"(min-width: 720px) 720px\"><figcaption>A NextMind device tracking PIN input</figcaption></figure><p>The result is honestly scary. With a good calibration score the accuracy is very very good. There is, understandably, a learning curve of how to focus your attention for the device, and also a slight delay in the reaction of the device.<br>A interesting benefit of the PIN demo show above is that a 'discreet' mode can be activated, so even onlookers won't know what is being input, it's a secret between your brain, focus, the NextMind, and your computer.<br>And I know what you're thinking. Maybe it knows what you're SUPPOSED to be thinking. especially in terms of a game, or a PIN number, it's reasonable to expect the system knows the expected result and could make a better guess of what you input. This falls down if you purposefully do incorrect inputs. I can confirm, even when purposefully selecting incorrect options, the NextMind matches what you select perfectly.</p><p><br>How exactly it works we don't know, but its certainly good, and its certainly unnerving...</p><figure class=\"kg-card kg-video-card\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2022/05/block.mp4\" poster=\"https://img.spacergif.org/v1/1920x1080/0a/spacer.png\" width=\"1920\" height=\"1080\" playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2022/05/media-thumbnail-ember831.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div></figure>","comment_id":"628ccbf62c055d000197a103","plaintext":"I've been lucky enough to play around with a NextMind dev kit. These awesome\ndevices use electrodes that are pressed to the back of the head to.... well...\nread your mind.\n\n\nOnce calibrated, the NextMind devices can utilise your 'focus' on a screen, to\neerily predict what part of a screen you are looking at. The best way i can\ndescribe what's happening is to explain it as eye tracking software - except no\ncameras. Rather than looking at your eyes to see what you are focusing on,\nNextMind looks into your mind through the back of your head to achieve this.\n\nA NextMind device tracking PIN inputThe result is honestly scary. With a good\ncalibration score the accuracy is very very good. There is, understandably, a\nlearning curve of how to focus your attention for the device, and also a slight\ndelay in the reaction of the device.\nA interesting benefit of the PIN demo show above is that a 'discreet' mode can\nbe activated, so even onlookers won't know what is being input, it's a secret\nbetween your brain, focus, the NextMind, and your computer.\nAnd I know what you're thinking. Maybe it knows what you're SUPPOSED to be\nthinking. especially in terms of a game, or a PIN number, it's reasonable to\nexpect the system knows the expected result and could make a better guess of\nwhat you input. This falls down if you purposefully do incorrect inputs. I can\nconfirm, even when purposefully selecting incorrect options, the NextMind\nmatches what you select perfectly.\n\n\nHow exactly it works we don't know, but its certainly good, and its certainly\nunnerving...\n\n0:00/1Ã—","feature_image":"__GHOST_URL__/content/images/2022/05/image-5-1.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-05-24T12:13:42.000Z","updated_at":"2022-05-24T14:13:53.000Z","published_at":"2021-11-22T12:14:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"628ccc6a2c055d000197a115","uuid":"cf50fb48-58b6-41fb-85cf-f3081e4e3172","title":"Ever seen a man eat his own head?","slug":"ever-seen-a-man-eat-his-own-head","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/05/image-6.png\",\"width\":936,\"height\":1272}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/05/image-7.png\",\"width\":936,\"height\":668}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/05/image-9.png\",\"width\":936,\"height\":1396,\"cardWidth\":\"\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/05/image-10.png\",\"width\":936,\"height\":760}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/05/image-11.png\",\"width\":936,\"height\":872}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/05/image-12.png\",\"width\":936,\"height\":1002}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/05/image-13.png\",\"width\":936,\"height\":1162}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"Well you have now...\"]]],[1,\"p\",[[0,[],0,\"I decided to make my own head out of chocolate. Why? Just because I can... Newer iPhones have great face scanning tech, so why not put it to good use? I used the Bellus 3D face Scan to do this.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Access to the face depth camera on an iPhone created a remarkably accurate likeness, and the algorithms used by Bellus helped to round it out and create a full head. Once exported to an obj file (small fee) - you can easily open up the file in blender. Here I chose to manually sculpt a few features I felt were a bit skewed,   like my ears and the tip of my nose. I also gave myself some small pupil indents to add a bit of depth to my eyes for a single colour and material.\"]]],[1,\"h2\",[[1,[],0,0],[0,[],0,\"Next Step, 3D print.\"]]],[1,\"p\",[[1,[],0,1],[0,[],0,\"The output was a mesh surface of my face rather than a 3D model. Luckily Blender is amazing and free to use, and has all the tools we could ever need, and we can quickly convert the mesh to a solid object, and export as an stl.\"],[1,[],0,2],[0,[],0,\"Once we have it as an stl, any 3D slicer program that works with your printer can slice it up and prepare it for printing. There are also companies that will print stl files for you and post you the final item!\"]]],[10,1],[1,\"p\",[[0,[],0,\"I printed my head in several different scalings for different size projects. Here's one about 10% size.\"]]],[1,\"h2\",[[1,[],0,3],[0,[],0,\"Mold it!\"]]],[10,2],[1,\"p\",[[0,[],0,\"I used a silicone rubber mix to create an imprint of the head. You could just 3D print an inverse version to create this mold in plastic, but I wanted the mold to be food safe, so chose this method of casting the head in silicone.\"]]],[10,3],[1,\"p\",[[0,[],0,\"I prepped a discarded soup container and glued the head to the base, so the neck would be facing up in the mold. Then, simply mix the silicone together as instructed and pour over. Be sure to vibrate, bang, jiggle, and shake (gently) the container to remove as many bubbles as possible. Bubbles will always be your enemy at every stage of the process to get a good reproduction. You'll then need to leave this at least overnight to set. Getting a solid plastic head out is not easy, so you will have to carefully cut a slit down one side (I'd suggest back of the head).\"]]],[1,\"h2\",[[1,[],0,4],[0,[],0,\"Food!\"]]],[1,\"p\",[[1,[],0,5],[0,[],0,\"Once the original form is extracted, you can put whatever you like in to set. Clay slip, chocolate, wax for candles, even water for funky ice heads. Be sure to retain the container to support any liquid content while it sets from pouring out of the seem you made... You can also use rubber bands to stop the mold from opening apart from the weight of the contents.\"]]],[10,4],[1,\"p\",[[0,[],0,\"Use a toothpick to swirl the chocolate or liquid around and vibrate the container again to dislodge any bubbles. I've found bubbles love to form in the nose and ears, so try to poke your stick into those areas right to the surface. You don't want a chocolate Voldemort. If you want a shell instead of a solid head, simply let set for 20/30 mins and then pour the remaining molten contents out.\"]]],[10,5],[1,\"p\",[[0,[],0,\"Unmold VERY carefully. Especially with chocolate, the heads can be very fragile, so be careful.\"]]],[10,6],[1,\"p\",[[0,[],0,\"I also made smaller lollipop size heads for a smaller tasty treat. This time I used candy melts to avoid making lots of tempered chocolate. Candy melts are a great substitution if you're looking for a nice solid and stable head!\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>Well you have now...</p><p>I decided to make my own head out of chocolate. Why? Just because I can... Newer iPhones have great face scanning tech, so why not put it to good use? I used the Bellus 3D face Scan to do this.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/05/image-6.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"936\" height=\"1272\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/image-6.png 600w, __GHOST_URL__/content/images/2022/05/image-6.png 936w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Access to the face depth camera on an iPhone created a remarkably accurate likeness, and the algorithms used by Bellus helped to round it out and create a full head. Once exported to an obj file (small fee) - you can easily open up the file in blender. Here I chose to manually sculpt a few features I felt were a bit skewed, Â  like my ears and the tip of my nose. I also gave myself some small pupil indents to add a bit of depth to my eyes for a single colour and material.</p><h2 id=\"next-step-3d-print\"><br>Next Step, 3D print.</h2><p><br>The output was a mesh surface of my face rather than a 3D model. Luckily Blender is amazing and free to use, and has all the tools we could ever need, and we can quickly convert the mesh to a solid object, and export as an stl.<br>Once we have it as an stl, any 3D slicer program that works with your printer can slice it up and prepare it for printing. There are also companies that will print stl files for you and post you the final item!</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/05/image-7.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"936\" height=\"668\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/image-7.png 600w, __GHOST_URL__/content/images/2022/05/image-7.png 936w\" sizes=\"(min-width: 720px) 720px\"></figure><p>I printed my head in several different scalings for different size projects. Here's one about 10% size.</p><h2 id=\"mold-it\"><br>Mold it!</h2><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/05/image-9.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"936\" height=\"1396\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/image-9.png 600w, __GHOST_URL__/content/images/2022/05/image-9.png 936w\" sizes=\"(min-width: 720px) 720px\"></figure><p>I used a silicone rubber mix to create an imprint of the head. You could just 3D print an inverse version to create this mold in plastic, but I wanted the mold to be food safe, so chose this method of casting the head in silicone.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/05/image-10.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"936\" height=\"760\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/image-10.png 600w, __GHOST_URL__/content/images/2022/05/image-10.png 936w\" sizes=\"(min-width: 720px) 720px\"></figure><p>I prepped a discarded soup container and glued the head to the base, so the neck would be facing up in the mold. Then, simply mix the silicone together as instructed and pour over. Be sure to vibrate, bang, jiggle, and shake (gently) the container to remove as many bubbles as possible. Bubbles will always be your enemy at every stage of the process to get a good reproduction. You'll then need to leave this at least overnight to set. Getting a solid plastic head out is not easy, so you will have to carefully cut a slit down one side (I'd suggest back of the head).</p><h2 id=\"food\"><br>Food!</h2><p><br>Once the original form is extracted, you can put whatever you like in to set. Clay slip, chocolate, wax for candles, even water for funky ice heads. Be sure to retain the container to support any liquid content while it sets from pouring out of the seem you made... You can also use rubber bands to stop the mold from opening apart from the weight of the contents.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/05/image-11.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"936\" height=\"872\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/image-11.png 600w, __GHOST_URL__/content/images/2022/05/image-11.png 936w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Use a toothpick to swirl the chocolate or liquid around and vibrate the container again to dislodge any bubbles. I've found bubbles love to form in the nose and ears, so try to poke your stick into those areas right to the surface. You don't want a chocolate Voldemort. If you want a shell instead of a solid head, simply let set for 20/30 mins and then pour the remaining molten contents out.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/05/image-12.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"936\" height=\"1002\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/image-12.png 600w, __GHOST_URL__/content/images/2022/05/image-12.png 936w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Unmold VERY carefully. Especially with chocolate, the heads can be very fragile, so be careful.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/05/image-13.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"936\" height=\"1162\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/05/image-13.png 600w, __GHOST_URL__/content/images/2022/05/image-13.png 936w\" sizes=\"(min-width: 720px) 720px\"></figure><p>I also made smaller lollipop size heads for a smaller tasty treat. This time I used candy melts to avoid making lots of tempered chocolate. Candy melts are a great substitution if you're looking for a nice solid and stable head!</p>","comment_id":"628ccc6a2c055d000197a115","plaintext":"Well you have now...\n\nI decided to make my own head out of chocolate. Why? Just because I can... Newer\niPhones have great face scanning tech, so why not put it to good use? I used the\nBellus 3D face Scan to do this.\n\nAccess to the face depth camera on an iPhone created a remarkably accurate\nlikeness, and the algorithms used by Bellus helped to round it out and create a\nfull head. Once exported to an obj file (small fee) - you can easily open up the\nfile in blender. Here I chose to manually sculpt a few features I felt were a\nbit skewed, Â  like my ears and the tip of my nose. I also gave myself some small\npupil indents to add a bit of depth to my eyes for a single colour and material.\n\n\nNext Step, 3D print.\n\nThe output was a mesh surface of my face rather than a 3D model. Luckily Blender\nis amazing and free to use, and has all the tools we could ever need, and we can\nquickly convert the mesh to a solid object, and export as an stl.\nOnce we have it as an stl, any 3D slicer program that works with your printer\ncan slice it up and prepare it for printing. There are also companies that will\nprint stl files for you and post you the final item!\n\nI printed my head in several different scalings for different size projects.\nHere's one about 10% size.\n\n\nMold it!\nI used a silicone rubber mix to create an imprint of the head. You could just 3D\nprint an inverse version to create this mold in plastic, but I wanted the mold\nto be food safe, so chose this method of casting the head in silicone.\n\nI prepped a discarded soup container and glued the head to the base, so the neck\nwould be facing up in the mold. Then, simply mix the silicone together as\ninstructed and pour over. Be sure to vibrate, bang, jiggle, and shake (gently)\nthe container to remove as many bubbles as possible. Bubbles will always be your\nenemy at every stage of the process to get a good reproduction. You'll then need\nto leave this at least overnight to set. Getting a solid plastic head out is not\neasy, so you will have to carefully cut a slit down one side (I'd suggest back\nof the head).\n\n\nFood!\n\nOnce the original form is extracted, you can put whatever you like in to set.\nClay slip, chocolate, wax for candles, even water for funky ice heads. Be sure\nto retain the container to support any liquid content while it sets from pouring\nout of the seem you made... You can also use rubber bands to stop the mold from\nopening apart from the weight of the contents.\n\nUse a toothpick to swirl the chocolate or liquid around and vibrate the\ncontainer again to dislodge any bubbles. I've found bubbles love to form in the\nnose and ears, so try to poke your stick into those areas right to the surface.\nYou don't want a chocolate Voldemort. If you want a shell instead of a solid\nhead, simply let set for 20/30 mins and then pour the remaining molten contents\nout.\n\nUnmold VERY carefully. Especially with chocolate, the heads can be very fragile,\nso be careful.\n\nI also made smaller lollipop size heads for a smaller tasty treat. This time I\nused candy melts to avoid making lots of tempered chocolate. Candy melts are a\ngreat substitution if you're looking for a nice solid and stable head!","feature_image":"__GHOST_URL__/content/images/2022/05/Picture-1.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-05-24T12:15:38.000Z","updated_at":"2022-05-24T14:15:21.000Z","published_at":"2022-03-15T12:18:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"628ccd2e2c055d000197a13f","uuid":"3d7f8476-bb31-4af3-8376-0b80a4bce079","title":"Brain Interface Robotics","slug":"brain-interface-robotics","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"video\",{\"loop\":true,\"src\":\"__GHOST_URL__/content/media/2022/05/speedRobotNetmind.mp4\",\"fileName\":\"speedRobotNetmind.mp4\",\"width\":1920,\"height\":1080,\"duration\":14.981633,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2022/05/media-thumbnail-ember1375.jpg\",\"thumbnailWidth\":1920,\"thumbnailHeight\":1080,\"caption\":\"Fergus controls a stretch RE1 with a NextMind device\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"Building on my exploration of NextMind I took the chance to see how far we could push the tech and integrate it with our Stretch RE1 robot that we lovingly named Rory.\"]]],[1,\"p\",[[1,[],0,0],[0,[],0,\"There are many potential applications for the Stretch RE1. Itâ€™s open-source approach and interchangeable tools make it a versatile and adaptable platform. Specifically, with NextMind control the obvious applications are as an assistive device for people with challenges with fine motor control who may not be able to use traditional fiddly control surfaces used to control like joysticks and would benefit from control of a robot assistant. There may be other applications that use traditional control surfaces as well as the NextMind too, for example using the NextMind to control larger movements whilst simultaneously using a control surface to control fine manipulation of the gripper or another tool head.\"],[1,[],0,1],[0,[],0,\"Stretch RE1 has some out of the box functionality provided by hello robot, especially around vision and object identification. Some of these demos even focus on simple assistive scenarios like mouth finding for assistive feeding. For Rory specifically we have been working to implement Azure cognitive services to enable more intelligence. Currently Rory can run a fully voice interactive chatbot, reply with generated speech, understand lots of commands from simple questions like â€˜what is your nameâ€™ to full voice-controlled manoeuvrability. Rory also has a great camera system that we have enabled through Azure to recognise objects, people, faces, and measure distances using the D435i depth sensor. Rory can use all of this visual information to describe what he can see and make decisions about objects and their location.\"]]],[1,\"p\",[[1,[],0,2],[0,[],0,\"On the NextMind side, we have a simple unity interface with NeuroTag controls. Using the â€˜On Triggeredâ€™ event we start a timer, and â€˜On Releaseâ€™ we call a script that sends a command to the python server via REST where the amount to move is proportional to the time the focus was maintained.\"]]],[1,\"p\",[[1,[],0,3],[0,[],0,\"This results in a way to control the movement of the robot more precisely. The ratio of time to movement can be adjusted for more fine control of the robot too, eventually weâ€™d like to have that as an option to change through the interface.\"]]],[1,\"p\",[[0,[],0,\"Watch a video of the NextMind controlled Stretch RE1 here:\"]]],[10,0],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>Building on my exploration of NextMind I took the chance to see how far we could push the tech and integrate it with our Stretch RE1 robot that we lovingly named Rory.</p><p><br>There are many potential applications for the Stretch RE1. Itâ€™s open-source approach and interchangeable tools make it a versatile and adaptable platform. Specifically, with NextMind control the obvious applications are as an assistive device for people with challenges with fine motor control who may not be able to use traditional fiddly control surfaces used to control like joysticks and would benefit from control of a robot assistant. There may be other applications that use traditional control surfaces as well as the NextMind too, for example using the NextMind to control larger movements whilst simultaneously using a control surface to control fine manipulation of the gripper or another tool head.<br>Stretch RE1 has some out of the box functionality provided by hello robot, especially around vision and object identification. Some of these demos even focus on simple assistive scenarios like mouth finding for assistive feeding. For Rory specifically we have been working to implement Azure cognitive services to enable more intelligence. Currently Rory can run a fully voice interactive chatbot, reply with generated speech, understand lots of commands from simple questions like â€˜what is your nameâ€™ to full voice-controlled manoeuvrability. Rory also has a great camera system that we have enabled through Azure to recognise objects, people, faces, and measure distances using the D435i depth sensor. Rory can use all of this visual information to describe what he can see and make decisions about objects and their location.</p><p><br>On the NextMind side, we have a simple unity interface with NeuroTag controls. Using the â€˜On Triggeredâ€™ event we start a timer, and â€˜On Releaseâ€™ we call a script that sends a command to the python server via REST where the amount to move is proportional to the time the focus was maintained.</p><p><br>This results in a way to control the movement of the robot more precisely. The ratio of time to movement can be adjusted for more fine control of the robot too, eventually weâ€™d like to have that as an option to change through the interface.</p><p>Watch a video of the NextMind controlled Stretch RE1 here:</p><figure class=\"kg-card kg-video-card kg-card-hascaption\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2022/05/speedRobotNetmind.mp4\" poster=\"https://img.spacergif.org/v1/1920x1080/0a/spacer.png\" width=\"1920\" height=\"1080\" loop autoplay muted playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2022/05/media-thumbnail-ember1375.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container kg-video-hide\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div><figcaption>Fergus controls a stretch RE1 with a NextMind device</figcaption></figure>","comment_id":"628ccd2e2c055d000197a13f","plaintext":"Building on my exploration of NextMind I took the chance to see how far we could\npush the tech and integrate it with our Stretch RE1 robot that we lovingly named\nRory.\n\n\nThere are many potential applications for the Stretch RE1. Itâ€™s open-source\napproach and interchangeable tools make it a versatile and adaptable platform.\nSpecifically, with NextMind control the obvious applications are as an assistive\ndevice for people with challenges with fine motor control who may not be able to\nuse traditional fiddly control surfaces used to control like joysticks and would\nbenefit from control of a robot assistant. There may be other applications that\nuse traditional control surfaces as well as the NextMind too, for example using\nthe NextMind to control larger movements whilst simultaneously using a control\nsurface to control fine manipulation of the gripper or another tool head.\nStretch RE1 has some out of the box functionality provided by hello robot,\nespecially around vision and object identification. Some of these demos even\nfocus on simple assistive scenarios like mouth finding for assistive feeding.\nFor Rory specifically we have been working to implement Azure cognitive services\nto enable more intelligence. Currently Rory can run a fully voice interactive\nchatbot, reply with generated speech, understand lots of commands from simple\nquestions like â€˜what is your nameâ€™ to full voice-controlled manoeuvrability.\nRory also has a great camera system that we have enabled through Azure to\nrecognise objects, people, faces, and measure distances using the D435i depth\nsensor. Rory can use all of this visual information to describe what he can see\nand make decisions about objects and their location.\n\n\nOn the NextMind side, we have a simple unity interface with NeuroTag controls.\nUsing the â€˜On Triggeredâ€™ event we start a timer, and â€˜On Releaseâ€™ we call a\nscript that sends a command to the python server via REST where the amount to\nmove is proportional to the time the focus was maintained.\n\n\nThis results in a way to control the movement of the robot more precisely. The\nratio of time to movement can be adjusted for more fine control of the robot\ntoo, eventually weâ€™d like to have that as an option to change through the\ninterface.\n\nWatch a video of the NextMind controlled Stretch RE1 here:\n\n0:00/1Ã—Fergus controls a stretch RE1 with a NextMind device","feature_image":"__GHOST_URL__/content/images/2022/05/IMG_1172D678349A-1.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-05-24T12:18:54.000Z","updated_at":"2022-05-24T14:17:21.000Z","published_at":"2022-05-24T12:19:58.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"628ccd952c055d000197a14b","uuid":"63ff5b96-5cbc-4fb0-b719-5a5d01facdf4","title":"External References","slug":"external-references","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=9P98PBFVvBc&list=PLoP-4KVd7rByKILe__roYiF4GrDIJfu-I&index=6\",\"html\":\"<iframe width=\\\"200\\\" height=\\\"113\\\" src=\\\"https://www.youtube.com/embed/9P98PBFVvBc?list=PLoP-4KVd7rByKILe__roYiF4GrDIJfu-I\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\\" referrerpolicy=\\\"strict-origin-when-cross-origin\\\" allowfullscreen></iframe>\",\"type\":\"video\",\"metadata\":{\"title\":\"AI for productivity | Boost your efficiency in the workplace | Digital IQ episode 5\",\"author_name\":\"Avanade\",\"author_url\":\"https://www.youtube.com/@Avanade\",\"height\":113,\"width\":200,\"version\":\"1.0\",\"provider_name\":\"YouTube\",\"provider_url\":\"https://www.youtube.com/\",\"thumbnail_height\":360,\"thumbnail_width\":480,\"thumbnail_url\":\"https://i.ytimg.com/vi/9P98PBFVvBc/hqdefault.jpg\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/01/Fergus-Kidd--Speaker-Card-1.jpeg\",\"width\":1920,\"height\":1080}],[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=5WYF47BgUqI\",\"html\":\"<iframe width=\\\"200\\\" height=\\\"113\\\" src=\\\"https://www.youtube.com/embed/5WYF47BgUqI?feature=oembed\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\\" allowfullscreen title=\\\"Synthetic Data for AI, and AI for Synthetic Data in Open Source | SOOCon23 Open Data\\\"></iframe>\",\"type\":\"video\",\"metadata\":{\"title\":\"Synthetic Data for AI, and AI for Synthetic Data in Open Source | SOOCon23 Open Data\",\"author_name\":\"OpenUK\",\"author_url\":\"https://www.youtube.com/@openuk_uk\",\"height\":113,\"width\":200,\"version\":\"1.0\",\"provider_name\":\"YouTube\",\"provider_url\":\"https://www.youtube.com/\",\"thumbnail_height\":360,\"thumbnail_width\":480,\"thumbnail_url\":\"https://i.ytimg.com/vi/5WYF47BgUqI/hqdefault.jpg\"}}],[\"html\",{\"html\":\"<iframe src=\\\"https://www.linkedin.com/embed/feed/update/urn:li:ugcPost:6926834762665848832\\\" height=\\\"777\\\" width=\\\"504\\\" frameborder=\\\"0\\\" allowfullscreen=\\\"\\\" title=\\\"Embedded post\\\"></iframe>\"}],[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=ZivHzUjSiVQ\",\"html\":\"<iframe width=\\\"200\\\" height=\\\"113\\\" src=\\\"https://www.youtube.com/embed/ZivHzUjSiVQ?feature=oembed\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>\",\"type\":\"video\",\"metadata\":{\"title\":\"Thinking Machines  Azure Digital Twins - Fergus Kidd\",\"author_name\":\"Global AI Community\",\"author_url\":\"https://www.youtube.com/c/GlobalAICommunity\",\"height\":113,\"width\":200,\"version\":\"1.0\",\"provider_name\":\"YouTube\",\"provider_url\":\"https://www.youtube.com/\",\"thumbnail_height\":360,\"thumbnail_width\":480,\"thumbnail_url\":\"https://i.ytimg.com/vi/ZivHzUjSiVQ/hqdefault.jpg\"}}],[\"header\",{\"size\":\"medium\",\"style\":\"image\",\"buttonEnabled\":false,\"header\":\"Other Written Media\",\"backgroundImageSrc\":\"__GHOST_URL__/content/images/2022/05/Faces-2.png\"}],[\"bookmark\",{\"url\":\"https://www.avanade.com/en/blogs/techs-and-specs/author/fergus-kidd\",\"metadata\":{\"url\":\"https://www.avanade.com/en/blogs/techs-and-specs/author/fergus-kidd\",\"title\":\"Fergus Kidd - Avanade Insights Blog Posts\",\"description\":\"Fergus Kidd is an Engineer in Avanadeâ€™s Emerging Technology Engineering team.\",\"author\":\"Artificial Intelligence\",\"publisher\":\"<iframe src=\\\"//www.googletagmanager.com/ns.html?id=GTM-TKMT4Q\\\" height=\\\"0\\\" width=\\\"0\\\" style=\\\"display:none;visibility:hidden\\\"></iframe> Loading... Solutions Industries Technologies and Capabilities Client Stories Thinking CAREERS About Avanade Media Center Contact Us Search Languages Toggle navigation Solutions Industries Technologies and Capabilities Client Stories Thinking CAREERS About Avanade Media Center Contact Us Techs and Specs Categories Agile & DevOps (22) Azure (6) Data and analytics (12) Dynamics 365 (5) Experience design (XD) (2) Quantum Computing (4) Security (7) Sitecore (24) Software development (13) View all Categories and Tags Authors About Blog Subscribe Follow Us GO TO AVANADE SITE CONTACT US Toggle navigation Follow Us SUBSCRIBE Get weekly updates of our latest stories from Techs and Specs. Thanks for subscribing. Watch your inbox for blog alerts and updates. Opt in or manage your email preferences at any time through our Email Preference Center. Techs and Specs Tips and insights to build and deploy technology effectively in the digital enterprise. Loading... Techs and Specs Authors Fergus Kidd Fergus Kidd Fergus Kidd is an Engineer in Avanadeâ€™s Emerging Technology Engineering team. He works on developing assets and demonstrations around new and future technologies, focussing on innovative use cases to show the power of new technologies in the enterprise. Fergus has an academic background in physics, and a professional background in data science, and artificial intelligence. Top topics Fergus writes about: Artificial Intelligence Azure Blockchain Cloud Coding Data digital ethics Innovation quantum computing Software Development More Contact Latest Software development GitHub Copilot â€“ GPT-3 powered coding Posted on January 12, 2022 GitHub Copilot is a sensational new tool for software developers that is built right into Visual Studio code, but what can it do? READ MORE Artificial Intelligence Software Development Coding Innovation More Azure Azure Confidential Ledger â€“ attestability for the masses Posted on July 14, 2021 Azure Confidential Ledger is a lightweight and flexible managed decentralized data platform without the need for a managed SQL database or dedicated SQL server. READ MORE Blockchain Azure Cloud Data More Data and analytics Thinking machines: Azure digital twins Posted on April 28, 2021 Discover how Azure Digital Twins support the birth of thinking machines and provide the memory to supplement context and fuel computing power. READ MORE Artificial Intelligence Azure Data More Quantum Computing MIT Future Compute: A round-up from the Emerging Technology engineers Posted on March 26, 2021 MIT's Future Compute event was held in February 2021 where Emerging Tech engineers learned how Quantum computing, 5G speed, and AI of are transforming business. READ MORE quantum computing Innovation Cloud More Quantum Computing Introducing Azure Quantum: Quantum hardware unveiled by Microsoft Posted on March 8, 2021 Fergus Kidd spotlights Microsoftâ€™s recent release of Azure Quantum and how scientific gains continue to be made on its own quantum computing technology READ MORE quantum computing Azure Innovation More Software development Ethical and productivity implications of intelligent code creation Posted on March 2, 2021 Learn why intelligent code creation provides efficiency in your software development lifecycle and helps avoid ethical issues. READ MORE Coding digital ethics Software Development Artificial Intelligence More Load More Articles Techs and Specs Newsletter Stay up to date with our latest news. SUBSCRIBE Get weekly updates of our latest stories from Techs and Specs. Thanks for subscribing. Watch your inbox for blog alerts and updates. Next steps Talk to us about how we can bring the power of digital innovation to your business. CONTACT US Awards and Recognition Learn more about how we can partner to help you realize results â€“ and recognition. Avanade Solutions\\nIndustries\\nTechnologies and Capabilities\\nClient Stories\\nThinking Careers\\nAbout Avanade\\nMedia Center\\nAvanade Trust Center Responsible Disclosure Follow Us <img src=\\\"/-/media/images/icons/socialmedia/facebook-lightgrey.svg?la=en&amp;ver=2\\\" alt=\\\"\\\" class=\\\"card-image\\\"> <img src=\\\"/-/media/images/icons/socialmedia/twitter-lightgrey.svg?la=en&amp;ver=2\\\" alt=\\\"\\\" class=\\\"card-image\\\"> <img src=\\\"/-/media/images/icons/socialmedia/youtube-lightgrey.svg?la=en&amp;ver=2\\\" alt=\\\"\\\" class=\\\"card-image\\\"> <img src=\\\"/-/media/images/icons/socialmedia/linkedin-lightgrey.svg?la=en&amp;ver=2\\\" alt=\\\"\\\" class=\\\"card-image\\\"> <img src=\\\"/-/media/images/icons/socialmedia/pinterest-lightgrey.svg?la=en&amp;ver=2\\\" alt=\\\"\\\" class=\\\"card-image\\\"> <img src=\\\"/-/media/images/icons/socialmedia/instagram-lightgrey.svg?la=en&amp;ver=1\\\" alt=\\\"\\\" class=\\\"card-image\\\"> Contact Us Global Headquarters\\n1191 Second Avenue Suite 100 Seattle, WA 98101 Phone: +1 206 239 5600 Phone: +1 206 239 5600 Toll Free: +1 844 282 6233 Toll Free: +1 844 282 6233 Send us a Message All Locations Site Map Privacy Statement Cookie Policy Terms of Use Do Not Sell My Personal Information â€“ For California Residents Only Accessibility Statement Procurement Code of Business Ethics Â© 2022 Avanade Inc. All Rights Reserved. Share this page Share this page Facebook Twitter LinkedIn Pinterest Email\",\"thumbnail\":\"https://www.avanade.com/-/media/images/blogs/avanade-insights/fergus-kidd.jpg?la=en&ver=2\",\"icon\":\"https://www.avanade.com/images/mstile-310x310.png\"}}]],\"markups\":[],\"sections\":[[1,\"h2\",[[0,[],0,\"2024\"],[1,[],0,0]]],[10,0],[1,\"h2\",[[1,[],0,1],[0,[],0,\"2023\"]]],[10,1],[10,2],[1,\"h2\",[[0,[],0,\"2022\"]]],[10,3],[1,\"h2\",[[0,[],0,\"2021\"]]],[10,4],[10,5],[10,6],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<h2 id=\"2024\">2024<br></h2><figure class=\"kg-card kg-embed-card\"><iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/9P98PBFVvBc?list=PLoP-4KVd7rByKILe__roYiF4GrDIJfu-I\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></figure><h2 id=\"2023\"><br>2023</h2><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/01/Fergus-Kidd--Speaker-Card-1.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/01/Fergus-Kidd--Speaker-Card-1.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2023/01/Fergus-Kidd--Speaker-Card-1.jpeg 1000w, __GHOST_URL__/content/images/size/w1600/2023/01/Fergus-Kidd--Speaker-Card-1.jpeg 1600w, __GHOST_URL__/content/images/2023/01/Fergus-Kidd--Speaker-Card-1.jpeg 1920w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-embed-card\"><iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/5WYF47BgUqI?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Synthetic Data for AI, and AI for Synthetic Data in Open Source | SOOCon23 Open Data\"></iframe></figure><h2 id=\"2022\">2022</h2><!--kg-card-begin: html--><iframe src=\"https://www.linkedin.com/embed/feed/update/urn:li:ugcPost:6926834762665848832\" height=\"777\" width=\"504\" frameborder=\"0\" allowfullscreen=\"\" title=\"Embedded post\"></iframe><!--kg-card-end: html--><h2 id=\"2021\">2021</h2><figure class=\"kg-card kg-embed-card\"><iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/ZivHzUjSiVQ?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></figure><div class=\"kg-card kg-header-card kg-width-full kg-size-medium kg-style-image\" style=\"background-image: url(https://fergusblog.azurewebsites.net/content/images/2022/05/Faces-2.png)\" data-kg-background-image=\"https://fergusblog.azurewebsites.net/content/images/2022/05/Faces-2.png\"><h2 class=\"kg-header-card-header\" id=\"other-written-media\">Other Written Media</h2></div><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.avanade.com/en/blogs/techs-and-specs/author/fergus-kidd\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Fergus Kidd - Avanade Insights Blog Posts</div><div class=\"kg-bookmark-description\">Fergus Kidd is an Engineer in Avanadeâ€™s Emerging Technology Engineering team.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.avanade.com/images/mstile-310x310.png\" alt=\"\"><span class=\"kg-bookmark-author\">&lt;iframe src&#x3D;&quot;//www.googletagmanager.com/ns.html?id&#x3D;GTM-TKMT4Q&quot; height&#x3D;&quot;0&quot; width&#x3D;&quot;0&quot; style&#x3D;&quot;display:none;visibility:hidden&quot;&gt;&lt;/iframe&gt; Loading... Solutions Industries Technologies and Capabilities Client Stories Thinking CAREERS About Avanade Media Center Contact Us Search Languages Toggle navigation Solutions Industries Technologies and Capabilities Client Stories Thinking CAREERS About Avanade Media Center Contact Us Techs and Specs Categories Agile &amp; DevOps (22) Azure (6) Data and analytics (12) Dynamics 365 (5) Experience design (XD) (2) Quantum Computing (4) Security (7) Sitecore (24) Software development (13) View all Categories and Tags Authors About Blog Subscribe Follow Us GO TO AVANADE SITE CONTACT US Toggle navigation Follow Us SUBSCRIBE Get weekly updates of our latest stories from Techs and Specs. Thanks for subscribing. Watch your inbox for blog alerts and updates. Opt in or manage your email preferences at any time through our Email Preference Center. Techs and Specs Tips and insights to build and deploy technology effectively in the digital enterprise. Loading... Techs and Specs Authors Fergus Kidd Fergus Kidd Fergus Kidd is an Engineer in Avanadeâ€™s Emerging Technology Engineering team. He works on developing assets and demonstrations around new and future technologies, focussing on innovative use cases to show the power of new technologies in the enterprise. Fergus has an academic background in physics, and a professional background in data science, and artificial intelligence. Top topics Fergus writes about: Artificial Intelligence Azure Blockchain Cloud Coding Data digital ethics Innovation quantum computing Software Development More Contact Latest Software development GitHub Copilot â€“ GPT-3 powered coding Posted on January 12, 2022 GitHub Copilot is a sensational new tool for software developers that is built right into Visual Studio code, but what can it do? READ MORE Artificial Intelligence Software Development Coding Innovation More Azure Azure Confidential Ledger â€“ attestability for the masses Posted on July 14, 2021 Azure Confidential Ledger is a lightweight and flexible managed decentralized data platform without the need for a managed SQL database or dedicated SQL server. READ MORE Blockchain Azure Cloud Data More Data and analytics Thinking machines: Azure digital twins Posted on April 28, 2021 Discover how Azure Digital Twins support the birth of thinking machines and provide the memory to supplement context and fuel computing power. READ MORE Artificial Intelligence Azure Data More Quantum Computing MIT Future Compute: A round-up from the Emerging Technology engineers Posted on March 26, 2021 MIT&#x27;s Future Compute event was held in February 2021 where Emerging Tech engineers learned how Quantum computing, 5G speed, and AI of are transforming business. READ MORE quantum computing Innovation Cloud More Quantum Computing Introducing Azure Quantum: Quantum hardware unveiled by Microsoft Posted on March 8, 2021 Fergus Kidd spotlights Microsoftâ€™s recent release of Azure Quantum and how scientific gains continue to be made on its own quantum computing technology READ MORE quantum computing Azure Innovation More Software development Ethical and productivity implications of intelligent code creation Posted on March 2, 2021 Learn why intelligent code creation provides efficiency in your software development lifecycle and helps avoid ethical issues. READ MORE Coding digital ethics Software Development Artificial Intelligence More Load More Articles Techs and Specs Newsletter Stay up to date with our latest news. SUBSCRIBE Get weekly updates of our latest stories from Techs and Specs. Thanks for subscribing. Watch your inbox for blog alerts and updates. Next steps Talk to us about how we can bring the power of digital innovation to your business. CONTACT US Awards and Recognition Learn more about how we can partner to help you realize results â€“ and recognition. Avanade SolutionsIndustriesTechnologies and CapabilitiesClient StoriesThinking CareersAbout AvanadeMedia CenterAvanade Trust Center Responsible Disclosure Follow Us &lt;img src&#x3D;&quot;/-/media/images/icons/socialmedia/facebook-lightgrey.svg?la&#x3D;en&amp;amp;ver&#x3D;2&quot; alt&#x3D;&quot;&quot; class&#x3D;&quot;card-image&quot;&gt; &lt;img src&#x3D;&quot;/-/media/images/icons/socialmedia/twitter-lightgrey.svg?la&#x3D;en&amp;amp;ver&#x3D;2&quot; alt&#x3D;&quot;&quot; class&#x3D;&quot;card-image&quot;&gt; &lt;img src&#x3D;&quot;/-/media/images/icons/socialmedia/youtube-lightgrey.svg?la&#x3D;en&amp;amp;ver&#x3D;2&quot; alt&#x3D;&quot;&quot; class&#x3D;&quot;card-image&quot;&gt; &lt;img src&#x3D;&quot;/-/media/images/icons/socialmedia/linkedin-lightgrey.svg?la&#x3D;en&amp;amp;ver&#x3D;2&quot; alt&#x3D;&quot;&quot; class&#x3D;&quot;card-image&quot;&gt; &lt;img src&#x3D;&quot;/-/media/images/icons/socialmedia/pinterest-lightgrey.svg?la&#x3D;en&amp;amp;ver&#x3D;2&quot; alt&#x3D;&quot;&quot; class&#x3D;&quot;card-image&quot;&gt; &lt;img src&#x3D;&quot;/-/media/images/icons/socialmedia/instagram-lightgrey.svg?la&#x3D;en&amp;amp;ver&#x3D;1&quot; alt&#x3D;&quot;&quot; class&#x3D;&quot;card-image&quot;&gt; Contact Us Global Headquarters1191 Second Avenue Suite 100 Seattle, WA 98101 Phone: +1 206 239 5600 Phone: +1 206 239 5600 Toll Free: +1 844 282 6233 Toll Free: +1 844 282 6233 Send us a Message All Locations Site Map Privacy Statement Cookie Policy Terms of Use Do Not Sell My Personal Information â€“ For California Residents Only Accessibility Statement Procurement Code of Business Ethics Â© 2022 Avanade Inc. All Rights Reserved. Share this page Share this page Facebook Twitter LinkedIn Pinterest Email</span><span class=\"kg-bookmark-publisher\">Artificial Intelligence</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://www.avanade.com/-/media/images/blogs/avanade-insights/fergus-kidd.jpg?la&#x3D;en&amp;ver&#x3D;2\" alt=\"\"></div></a></figure>","comment_id":"628ccd952c055d000197a14b","plaintext":"2024\n\n\n2023\n2022\n2021\nOther Written Media\nFergus Kidd - Avanade Insights Blog PostsFergus Kidd is an Engineer in\nAvanadeâ€™s\nEmerging Technology Engineering team.<iframe\nsrc=\"//www.googletagmanager.com/ns.html?id=GTM-TKMT4Q\" height=\"0\" width=\"0\"\nstyle=\"display:none;visibility:hidden\"></iframe> Loading... Solutions\nIndustries\nTechnologies and Capabilities Client Stories Thinking CAREERS About Avanade\nMedia Center Contact Us Search Languages Toggle navigation Solutions Industries\nTechnologies and Capabilities Client Stories Thinking CAREERS About Avanade\nMedia Center Contact Us Techs and Specs Categories Agile & DevOps (22) Azure\n(6)\nData and analytics (12) Dynamics 365 (5) Experience design (XD) (2) Quantum\nComputing (4) Security (7) Sitecore (24) Software development (13) View all\nCategories and Tags Authors About Blog Subscribe Follow Us GO TO AVANADE SITE\nCONTACT US Toggle navigation Follow Us SUBSCRIBE Get weekly updates of our\nlatest stories from Techs and Specs. Thanks for subscribing. Watch your inbox\nfor blog alerts and updates. Opt in or manage your email preferences at any\ntime\nthrough our Email Preference Center. Techs and Specs Tips and insights to build\nand deploy technology effectively in the digital enterprise. Loading... Techs\nand Specs Authors Fergus Kidd Fergus Kidd Fergus Kidd is an Engineer in\nAvanadeâ€™s Emerging Technology Engineering team. He works on developing assets\nand demonstrations around new and future technologies, focussing on innovative\nuse cases to show the power of new technologies in the enterprise. Fergus has\nan\nacademic background in physics, and a professional background in data science,\nand artificial intelligence. Top topics Fergus writes about: Artificial\nIntelligence Azure Blockchain Cloud Coding Data digital ethics Innovation\nquantum computing Software Development More Contact Latest Software development\nGitHub Copilot â€“ GPT-3 powered coding Posted on January 12, 2022 GitHub Copilot\nis a sensational new tool for software developers that is built right into\nVisual Studio code, but what can it do? READ MORE Artificial Intelligence\nSoftware Development Coding Innovation More Azure Azure Confidential Ledger â€“\nattestability for the masses Posted on July 14, 2021 Azure Confidential Ledger\nis a lightweight and flexible managed decentralized data platform without the\nneed for a managed SQL database or dedicated SQL server. READ MORE Blockchain\nAzure Cloud Data More Data and analytics Thinking machines: Azure digital twins\nPosted on April 28, 2021 Discover how Azure Digital Twins support the birth of\nthinking machines and provide the memory to supplement context and fuel\ncomputing power. READ MORE Artificial Intelligence Azure Data More Quantum\nComputing MIT Future Compute: A round-up from the Emerging Technology engineers\nPosted on March 26, 2021 MIT's Future Compute event was held in February 2021\nwhere Emerging Tech engineers learned how Quantum computing, 5G speed, and AI\nof\nare transforming business. READ MORE quantum computing Innovation Cloud More\nQuantum Computing Introducing Azure Quantum: Quantum hardware unveiled by\nMicrosoft Posted on March 8, 2021 Fergus Kidd spotlights Microsoftâ€™s recent\nrelease of Azure Quantum and how scientific gains continue to be made on its\nown\nquantum computing technology READ MORE quantum computing Azure Innovation More\nSoftware development Ethical and productivity implications of intelligent code\ncreation Posted on March 2, 2021 Learn why intelligent code creation provides\nefficiency in your software development lifecycle and helps avoid ethical\nissues. READ MORE Coding digital ethics Software Development Artificial\nIntelligence More Load More Articles Techs and Specs Newsletter Stay up to date\nwith our latest news. SUBSCRIBE Get weekly updates of our latest stories from\nTechs and Specs. Thanks for subscribing. Watch your inbox for blog alerts and\nupdates. Next steps Talk to us about how we can bring the power of digital\ninnovation to your business. CONTACT US Awards and Recognition Learn more about\nhow we can partner to help you realize results â€“ and recognition. Avanade\nSolutionsIndustriesTechnologies and CapabilitiesClient StoriesThinking\nCareersAbout AvanadeMedia CenterAvanade Trust Center Responsible Disclosure\nFollow Us <img\nsrc=\"/-/media/images/icons/socialmedia/facebook-lightgrey.svg?la=en&ver=2\"\nalt=\"\" class=\"card-image\"> <img\nsrc=\"/-/media/images/icons/socialmedia/twitter-lightgrey.svg?la=en&ver=2\"\nalt=\"\" class=\"card-image\"> <img\nsrc=\"/-/media/images/icons/socialmedia/youtube-lightgrey.svg?la=en&ver=2\"\nalt=\"\" class=\"card-image\"> <img\nsrc=\"/-/media/images/icons/socialmedia/linkedin-lightgrey.svg?la=en&ver=2\"\nalt=\"\" class=\"card-image\"> <img\nsrc=\"/-/media/images/icons/socialmedia/pinterest-lightgrey.svg?la=en&ver=2\"\nalt=\"\" class=\"card-image\"> <img\nsrc=\"/-/media/images/icons/socialmedia/instagram-lightgrey.svg?la=en&ver=1\"\nalt=\"\" class=\"card-image\"> Contact Us Global Headquarters1191 Second Avenue\nSuite 100 Seattle, WA 98101 Phone: +1 206 239 5600 Phone: +1 206 239 5600 Toll\nFree: +1 844 282 6233 Toll Free: +1 844 282 6233 Send us a Message All\nLocations\nSite Map Privacy Statement Cookie Policy Terms of Use Do Not Sell My Personal\nInformation â€“ For California Residents Only Accessibility Statement Procurement\nCode of Business Ethics Â© 2022 Avanade Inc. All Rights Reserved. Share this\npage\nShare this page Facebook Twitter LinkedIn Pinterest EmailArtificial Intelligence\n[https://www.avanade.com/en/blogs/techs-and-specs/author/fergus-kidd]","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-05-24T12:20:37.000Z","updated_at":"2024-06-10T14:54:04.000Z","published_at":"2022-05-24T12:20:44.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"62dfbcfbff7f9e0001ab81e4","uuid":"390d44bc-9845-44c6-957f-c1b8f11d334a","title":"DALL-E 2","slug":"dall-e-2","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-23-21.08.38---An-oil-portait-painting-of-a-young-male-beekeeper-in-a-veil-surrounded-by-bees-in-the-style-of-grant-wood.png\",\"width\":1024,\"height\":1024,\"caption\":\"A young beekeeper in a veil surrounded with bees - in the style of Grant Wood\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/07/image.webp\",\"width\":1024,\"height\":1024,\"caption\":\"A photograph of a young beekeeper in a veil surrounded with bees&nbsp;\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/07/Screenshot-2022-07-26-at-11.38.02.png\",\"width\":2500,\"height\":610,\"caption\":\"Four options for: 'The president shaking hands with a purple alien at a carnival, digital art'\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/07/Screenshot-2022-07-26-at-11.39.05.png\",\"width\":2518,\"height\":624,\"caption\":\"Variations on The president shaking hands with a purple alien at a carnival, digital art\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.39.18---The-president-shaking-hands-with-a-purple-alien-at-a-carnival--digital-art.png\",\"width\":1024,\"height\":1024,\"caption\":\"The president shaking hands with a purple alien at a carnival, digital art\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.47.48---the-pokemon-pikachu-eating-spaghetti-bolognaise---oil-painting.png\",\"width\":1024,\"height\":1024,\"caption\":\"The Pokemon Pikachu eating spaghetti bolognaise - oil painting\"}],[\"image\",{\"caption\":\"a wooden etching of a car on a hill\",\"src\":\"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.53.43---a-wooden-etching-of-a-car-on-a-hill.png\",\"width\":1024,\"height\":1024}],[\"image\",{\"caption\":\"A watercolour painting of english countryside with cows in the field, and a flying pig in a superhero cape in the sky\",\"src\":\"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.58.02---a-watercolour-painting-of-english-countryside-with-cows-in-the-field--and-a-flying-pig-in-a-superhero-cape-in-the-sky.png\",\"width\":1024,\"height\":1024}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-12.01.32---a-3D-render-of-a-banana-on-fire-in-a-bucket-of-ice-with-a-cat-watching.png\",\"width\":1024,\"height\":1024,\"caption\":\"A 3D render of a banana on fire in a bucket of ice with a cat watching\"}]],\"markups\":[[\"a\",[\"href\",\"https://openai.com/dall-e-2/\"]],[\"a\",[\"href\",\"https://randommer.io/random-things-to-draw\"]],[\"a\",[\"href\",\"https://openai.com/dall-e-2/#:~:text=Our%20content%20policy%20does%20not,systems%20to%20guard%20against%20misuse.\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"All images on this page were generated with AI. That's right. The wait is finally over. I got access to \"],[0,[0],1,\"DALL-E 2 from Open AI\"],[0,[],0,\". This mind boggling image generation is state of the art, and runs completely on Azure. Getting access to it has been both incredibly exciting, but also a little bit terrifying.\"]]],[1,\"h3\",[[0,[],0,\"What is it?\"]]],[1,\"p\",[[0,[],0,\"DALL-E is a combination of a powerful Natural Language Processing AI, meshed with an even more powerful image generation AI. The combination creates a system that can take text based descriptions and turn them into incredible, original images. It can even understand the context of the setting, scene, and style of Image. I'll give you a few examples.\"]]],[1,\"h3\",[[0,[],0,\"How does it work?\"]]],[1,\"p\",[[0,[],0,\"Well, that might be the secret sauce. Much like Open AI's GPT-3, DALL-E 2 has been trained on a huge corpus of information. It is able to generate context about a huge number of words, and convert  them into images which (mostly) make sense based on our descriptions of them. The more words, and the better descriptions we can give, the more accurate the images can be.\"]]],[1,\"h3\",[[0,[],0,\"Shut up and show me\"]]],[1,\"p\",[[0,[],0,\"I wanted to pick a few topics that made sense to me, to demonstrate the originality and ingenuity of DALL-E 2.\"]]],[10,0],[1,\"p\",[[0,[],0,\"This incredible portrait of a beekeeper is genuinely very creative - and looks like it could be hung in a gallery. Using specific examples of art styles is really powerful in DALL-E 2.\"]]],[1,\"p\",[[0,[],0,\"We can also ask for photo realism, here's the same prompt again with a change to the style:\"]]],[10,1],[1,\"p\",[[0,[],0,\"At first glance this looks incredibly real. It could be a young woman beekeeping. It's not until we look closely that we see issues. Some to note are details of the eyes and teeth, lack of honeycomb structure on the frame, and the rather manic and poorly defined appearance of the bees. Although it certainly nailed the dirty suits!\"]]],[1,\"h3\",[[0,[],0,\"Is it really unique?\"]]],[1,\"p\",[[0,[],0,\"DALL-E 2 thrives on it's uniqueness. In my opinion the more abstract the input, the better it does at generation. To test this let's head to a random idea generator and generate something that surely has never been seen before.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"I started by heading to \"],[0,[1],1,\"https://randommer.io/random-things-to-draw\"],[0,[],0,\" and picking 'the president shaking hands with an alien', but let's go further. Where are they? Surely not the oval office. What colour is the alien? Using other suitable random toolings I came up with:\"]]],[1,\"p\",[[0,[],0,\"'The president shaking hands with a purple alien at a carnival'. We'll add the tag 'digital art' for this one.\"]]],[10,2],[1,\"p\",[[0,[],0,\"It's interesting that 2/4 of the 'presidents' are older white males, but two are aliens. We never actually said what planet or country the 'president' came from. For the human versions we can see there is some level of bias there. Both with appearance, but also the presence of the US flag in the second image. This is likely due to a human association of the English word 'president' with the USA, and then subsequently older white males, being ingrained into the corpus of data.\"]]],[1,\"h3\",[[0,[],0,\"Variations\"]]],[1,\"p\",[[0,[],0,\"DALL-E 2 can also start with an image and look at variations of the image. Let's start with the first human president example.\"]]],[10,3],[1,\"p\",[[0,[],0,\"In this way we can quickly generate mew versions of our preferred image. In this case the tent remains similar, we keep a (mostly) human president, and get a few styles of poses and aliens.\"]]],[10,4],[1,\"p\",[[0,[],0,\"In this case though, I think the original selection was best, but that's just boring human opinions.\"]]],[1,\"h3\",[[0,[],0,\"Safety First\"]]],[1,\"p\",[[0,[],0,\"Open AI has enforced a strict usage policy and guidelines that will automatically flag prompts that are deemed inappropriate, and stops image generation. At the moment this seems to stop us using direct likenesses of real people, so we can't generate some meme worthy celebrity moments that never existed. \"],[0,[2],1,\"Read more about the content policy\"],[0,[],0,\".\"]]],[1,\"h3\",[[0,[],0,\"A few more, for the sake of it.\"]]],[10,5],[1,\"p\",[[0,[],0,\"Note the subtleties, like not depicting Pikachu on a chair, and understanding its relative size to a human plate of pasta.\"]]],[10,6],[1,\"p\",[[0,[],0,\"I like this one, because it's perfectly valid to think that the etching itself should be placed on a hill, rather than the etching being of a car on a hill. Being specific with your wording is key, but that's true for humans too!\"]]],[1,\"p\",[]],[10,7],[1,\"p\",[[0,[],0,\"The only limit is the creativity of your inputs\"]]],[10,8],[1,\"p\",[[0,[],0,\"Have any ideas for crazy images? Let me know.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>All images on this page were generated with AI. That's right. The wait is finally over. I got access to <a href=\"https://openai.com/dall-e-2/\">DALL-E 2 from Open AI</a>. This mind boggling image generation is state of the art, and runs completely on Azure. Getting access to it has been both incredibly exciting, but also a little bit terrifying.</p><h3 id=\"what-is-it\">What is it?</h3><p>DALL-E is a combination of a powerful Natural Language Processing AI, meshed with an even more powerful image generation AI. The combination creates a system that can take text based descriptions and turn them into incredible, original images. It can even understand the context of the setting, scene, and style of Image. I'll give you a few examples.</p><h3 id=\"how-does-it-work\">How does it work?</h3><p>Well, that might be the secret sauce. Much like Open AI's GPT-3, DALL-E 2 has been trained on a huge corpus of information. It is able to generate context about a huge number of words, and convert Â them into images which (mostly) make sense based on our descriptions of them. The more words, and the better descriptions we can give, the more accurate the images can be.</p><h3 id=\"shut-up-and-show-me\">Shut up and show me</h3><p>I wanted to pick a few topics that made sense to me, to demonstrate the originality and ingenuity of DALL-E 2.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-23-21.08.38---An-oil-portait-painting-of-a-young-male-beekeeper-in-a-veil-surrounded-by-bees-in-the-style-of-grant-wood.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1024\" height=\"1024\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/07/DALL-E-2022-07-23-21.08.38---An-oil-portait-painting-of-a-young-male-beekeeper-in-a-veil-surrounded-by-bees-in-the-style-of-grant-wood.png 600w, __GHOST_URL__/content/images/size/w1000/2022/07/DALL-E-2022-07-23-21.08.38---An-oil-portait-painting-of-a-young-male-beekeeper-in-a-veil-surrounded-by-bees-in-the-style-of-grant-wood.png 1000w, __GHOST_URL__/content/images/2022/07/DALL-E-2022-07-23-21.08.38---An-oil-portait-painting-of-a-young-male-beekeeper-in-a-veil-surrounded-by-bees-in-the-style-of-grant-wood.png 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>A young beekeeper in a veil surrounded with bees - in the style of Grant Wood</figcaption></figure><p>This incredible portrait of a beekeeper is genuinely very creative - and looks like it could be hung in a gallery. Using specific examples of art styles is really powerful in DALL-E 2.</p><p>We can also ask for photo realism, here's the same prompt again with a change to the style:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/07/image.webp\" class=\"kg-image\" alt loading=\"lazy\" width=\"1024\" height=\"1024\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/07/image.webp 600w, __GHOST_URL__/content/images/size/w1000/2022/07/image.webp 1000w, __GHOST_URL__/content/images/2022/07/image.webp 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>A photograph of a young beekeeper in a veil surrounded with bees&nbsp;</figcaption></figure><p>At first glance this looks incredibly real. It could be a young woman beekeeping. It's not until we look closely that we see issues. Some to note are details of the eyes and teeth, lack of honeycomb structure on the frame, and the rather manic and poorly defined appearance of the bees. Although it certainly nailed the dirty suits!</p><h3 id=\"is-it-really-unique\">Is it really unique?</h3><p>DALL-E 2 thrives on it's uniqueness. In my opinion the more abstract the input, the better it does at generation. To test this let's head to a random idea generator and generate something that surely has never been seen before.</p><p></p><p>I started by heading to <a href=\"https://randommer.io/random-things-to-draw\">https://randommer.io/random-things-to-draw</a> and picking 'the president shaking hands with an alien', but let's go further. Where are they? Surely not the oval office. What colour is the alien? Using other suitable random toolings I came up with:</p><p>'The president shaking hands with a purple alien at a carnival'. We'll add the tag 'digital art' for this one.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/07/Screenshot-2022-07-26-at-11.38.02.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"488\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/07/Screenshot-2022-07-26-at-11.38.02.png 600w, __GHOST_URL__/content/images/size/w1000/2022/07/Screenshot-2022-07-26-at-11.38.02.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/07/Screenshot-2022-07-26-at-11.38.02.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/07/Screenshot-2022-07-26-at-11.38.02.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Four options for: 'The president shaking hands with a purple alien at a carnival, digital art'</figcaption></figure><p>It's interesting that 2/4 of the 'presidents' are older white males, but two are aliens. We never actually said what planet or country the 'president' came from. For the human versions we can see there is some level of bias there. Both with appearance, but also the presence of the US flag in the second image. This is likely due to a human association of the English word 'president' with the USA, and then subsequently older white males, being ingrained into the corpus of data.</p><h3 id=\"variations\">Variations</h3><p>DALL-E 2 can also start with an image and look at variations of the image. Let's start with the first human president example.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/07/Screenshot-2022-07-26-at-11.39.05.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"496\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/07/Screenshot-2022-07-26-at-11.39.05.png 600w, __GHOST_URL__/content/images/size/w1000/2022/07/Screenshot-2022-07-26-at-11.39.05.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/07/Screenshot-2022-07-26-at-11.39.05.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/07/Screenshot-2022-07-26-at-11.39.05.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Variations on The president shaking hands with a purple alien at a carnival, digital art</figcaption></figure><p>In this way we can quickly generate mew versions of our preferred image. In this case the tent remains similar, we keep a (mostly) human president, and get a few styles of poses and aliens.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.39.18---The-president-shaking-hands-with-a-purple-alien-at-a-carnival--digital-art.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1024\" height=\"1024\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/07/DALL-E-2022-07-26-11.39.18---The-president-shaking-hands-with-a-purple-alien-at-a-carnival--digital-art.png 600w, __GHOST_URL__/content/images/size/w1000/2022/07/DALL-E-2022-07-26-11.39.18---The-president-shaking-hands-with-a-purple-alien-at-a-carnival--digital-art.png 1000w, __GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.39.18---The-president-shaking-hands-with-a-purple-alien-at-a-carnival--digital-art.png 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The president shaking hands with a purple alien at a carnival, digital art</figcaption></figure><p>In this case though, I think the original selection was best, but that's just boring human opinions.</p><h3 id=\"safety-first\">Safety First</h3><p>Open AI has enforced a strict usage policy and guidelines that will automatically flag prompts that are deemed inappropriate, and stops image generation. At the moment this seems to stop us using direct likenesses of real people, so we can't generate some meme worthy celebrity moments that never existed. <a href=\"https://openai.com/dall-e-2/#:~:text=Our%20content%20policy%20does%20not,systems%20to%20guard%20against%20misuse.\">Read more about the content policy</a>.</p><h3 id=\"a-few-more-for-the-sake-of-it\">A few more, for the sake of it.</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.47.48---the-pokemon-pikachu-eating-spaghetti-bolognaise---oil-painting.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1024\" height=\"1024\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/07/DALL-E-2022-07-26-11.47.48---the-pokemon-pikachu-eating-spaghetti-bolognaise---oil-painting.png 600w, __GHOST_URL__/content/images/size/w1000/2022/07/DALL-E-2022-07-26-11.47.48---the-pokemon-pikachu-eating-spaghetti-bolognaise---oil-painting.png 1000w, __GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.47.48---the-pokemon-pikachu-eating-spaghetti-bolognaise---oil-painting.png 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The Pokemon Pikachu eating spaghetti bolognaise - oil painting</figcaption></figure><p>Note the subtleties, like not depicting Pikachu on a chair, and understanding its relative size to a human plate of pasta.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.53.43---a-wooden-etching-of-a-car-on-a-hill.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1024\" height=\"1024\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/07/DALL-E-2022-07-26-11.53.43---a-wooden-etching-of-a-car-on-a-hill.png 600w, __GHOST_URL__/content/images/size/w1000/2022/07/DALL-E-2022-07-26-11.53.43---a-wooden-etching-of-a-car-on-a-hill.png 1000w, __GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.53.43---a-wooden-etching-of-a-car-on-a-hill.png 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>a wooden etching of a car on a hill</figcaption></figure><p>I like this one, because it's perfectly valid to think that the etching itself should be placed on a hill, rather than the etching being of a car on a hill. Being specific with your wording is key, but that's true for humans too!</p><p></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.58.02---a-watercolour-painting-of-english-countryside-with-cows-in-the-field--and-a-flying-pig-in-a-superhero-cape-in-the-sky.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1024\" height=\"1024\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/07/DALL-E-2022-07-26-11.58.02---a-watercolour-painting-of-english-countryside-with-cows-in-the-field--and-a-flying-pig-in-a-superhero-cape-in-the-sky.png 600w, __GHOST_URL__/content/images/size/w1000/2022/07/DALL-E-2022-07-26-11.58.02---a-watercolour-painting-of-english-countryside-with-cows-in-the-field--and-a-flying-pig-in-a-superhero-cape-in-the-sky.png 1000w, __GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.58.02---a-watercolour-painting-of-english-countryside-with-cows-in-the-field--and-a-flying-pig-in-a-superhero-cape-in-the-sky.png 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>A watercolour painting of english countryside with cows in the field, and a flying pig in a superhero cape in the sky</figcaption></figure><p>The only limit is the creativity of your inputs</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-12.01.32---a-3D-render-of-a-banana-on-fire-in-a-bucket-of-ice-with-a-cat-watching.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1024\" height=\"1024\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/07/DALL-E-2022-07-26-12.01.32---a-3D-render-of-a-banana-on-fire-in-a-bucket-of-ice-with-a-cat-watching.png 600w, __GHOST_URL__/content/images/size/w1000/2022/07/DALL-E-2022-07-26-12.01.32---a-3D-render-of-a-banana-on-fire-in-a-bucket-of-ice-with-a-cat-watching.png 1000w, __GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-12.01.32---a-3D-render-of-a-banana-on-fire-in-a-bucket-of-ice-with-a-cat-watching.png 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>A 3D render of a banana on fire in a bucket of ice with a cat watching</figcaption></figure><p>Have any ideas for crazy images? Let me know.</p>","comment_id":"62dfbcfbff7f9e0001ab81e4","plaintext":"All images on this page were generated with AI. That's right. The wait is\nfinally over. I got access to DALL-E 2 from Open AI\n[https://openai.com/dall-e-2/]. This mind boggling image generation is state of\nthe art, and runs completely on Azure. Getting access to it has been both\nincredibly exciting, but also a little bit terrifying.\n\nWhat is it?\nDALL-E is a combination of a powerful Natural Language Processing AI, meshed\nwith an even more powerful image generation AI. The combination creates a system\nthat can take text based descriptions and turn them into incredible, original\nimages. It can even understand the context of the setting, scene, and style of\nImage. I'll give you a few examples.\n\nHow does it work?\nWell, that might be the secret sauce. Much like Open AI's GPT-3, DALL-E 2 has\nbeen trained on a huge corpus of information. It is able to generate context\nabout a huge number of words, and convert Â them into images which (mostly) make\nsense based on our descriptions of them. The more words, and the better\ndescriptions we can give, the more accurate the images can be.\n\nShut up and show me\nI wanted to pick a few topics that made sense to me, to demonstrate the\noriginality and ingenuity of DALL-E 2.\n\nA young beekeeper in a veil surrounded with bees - in the style of Grant Wood\nThis incredible portrait of a beekeeper is genuinely very creative - and looks\nlike it could be hung in a gallery. Using specific examples of art styles is\nreally powerful in DALL-E 2.\n\nWe can also ask for photo realism, here's the same prompt again with a change to\nthe style:\n\nA photograph of a young beekeeper in a veil surrounded with beesAt first glance\nthis looks incredibly real. It could be a young woman beekeeping. It's not until\nwe look closely that we see issues. Some to note are details of the eyes and\nteeth, lack of honeycomb structure on the frame, and the rather manic and poorly\ndefined appearance of the bees. Although it certainly nailed the dirty suits!\n\nIs it really unique?\nDALL-E 2 thrives on it's uniqueness. In my opinion the more abstract the input,\nthe better it does at generation. To test this let's head to a random idea\ngenerator and generate something that surely has never been seen before.\n\n\n\nI started by heading to https://randommer.io/random-things-to-draw and picking\n'the president shaking hands with an alien', but let's go further. Where are\nthey? Surely not the oval office. What colour is the alien? Using other suitable\nrandom toolings I came up with:\n\n'The president shaking hands with a purple alien at a carnival'. We'll add the\ntag 'digital art' for this one.\n\nFour options for: 'The president shaking hands with a purple alien at a\ncarnival, digital art'It's interesting that 2/4 of the 'presidents' are older\nwhite males, but two are aliens. We never actually said what planet or country\nthe 'president' came from. For the human versions we can see there is some level\nof bias there. Both with appearance, but also the presence of the US flag in the\nsecond image. This is likely due to a human association of the English word\n'president' with the USA, and then subsequently older white males, being\ningrained into the corpus of data.\n\nVariations\nDALL-E 2 can also start with an image and look at variations of the image. Let's\nstart with the first human president example.\n\nVariations on The president shaking hands with a purple alien at a carnival,\ndigital artIn this way we can quickly generate mew versions of our preferred\nimage. In this case the tent remains similar, we keep a (mostly) human\npresident, and get a few styles of poses and aliens.\n\nThe president shaking hands with a purple alien at a carnival, digital artIn\nthis case though, I think the original selection was best, but that's just\nboring human opinions.\n\nSafety First\nOpen AI has enforced a strict usage policy and guidelines that will\nautomatically flag prompts that are deemed inappropriate, and stops image\ngeneration. At the moment this seems to stop us using direct likenesses of real\npeople, so we can't generate some meme worthy celebrity moments that never\nexisted. Read more about the content policy\n[https://openai.com/dall-e-2/#:~:text=Our%20content%20policy%20does%20not,systems%20to%20guard%20against%20misuse.]\n.\n\nA few more, for the sake of it.\nThe Pokemon Pikachu eating spaghetti bolognaise - oil paintingNote the\nsubtleties, like not depicting Pikachu on a chair, and understanding its\nrelative size to a human plate of pasta.\n\na wooden etching of a car on a hillI like this one, because it's perfectly valid\nto think that the etching itself should be placed on a hill, rather than the\netching being of a car on a hill. Being specific with your wording is key, but\nthat's true for humans too!\n\n\n\nA watercolour painting of english countryside with cows in the field, and a\nflying pig in a superhero cape in the skyThe only limit is the creativity of\nyour inputs\n\nA 3D render of a banana on fire in a bucket of ice with a cat watchingHave any\nideas for crazy images? Let me know.","feature_image":"__GHOST_URL__/content/images/2022/07/DALL-E-2022-07-26-11.11.16---a-robot-draws-a-picture--digital-art-1.png","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-07-26T10:07:55.000Z","updated_at":"2022-07-26T11:04:13.000Z","published_at":"2022-07-26T11:02:57.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"62fa557dff7f9e0001ab82ad","uuid":"121d55cd-e4e9-4662-9991-e477aee42371","title":"The Altspace Media Player using Azure Storage","slug":"altspace-media-player-and-azure-storage","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.09.53.png\",\"width\":562,\"height\":862,\"caption\":\"Create a storage account on Azure\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.23.06.png\",\"width\":672,\"height\":442}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.10.04.png\",\"width\":250,\"height\":90}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.10.26.png\",\"width\":216,\"height\":100}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.10.39.png\",\"width\":4396,\"height\":384}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.10.47.png\",\"width\":530,\"height\":974}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.11.09.png\",\"width\":1764,\"height\":1610}]],\"markups\":[[\"a\",[\"href\",\"https://multimedia-console.altvr.com/\"]],[\"a\",[\"href\",\"portal.azure.com\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/windows/mixed-reality/altspace-vr/tutorials/multimedia-console\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"If you've hosted meet-ups or events in AltSpace, you've probably tried the \"],[0,[0],1,\"Altspace multimedia console\"],[0,[],0,\". Whilst it's a great tool, it requires a complete publicly available hosted video or photo file. You also can't use popular video platforms like Youtube as these are not supported (for a number of reasons). There are some great freely available hosting sites, but it dawned on me, why cant we just host our own files on Azure? Well. You can. And, it's simple.\"]]],[1,\"h2\",[[0,[],0,\"1 - Create Storage Account\"]]],[1,\"p\",[[0,[],0,\"Head to \"],[0,[1],1,\"portal.azure.com\"],[0,[],0,\", and assuming you're all signed up with a subscription, select 'create a resource'. You're looking for a standard storage account like this. Name it whatever you like, but the standard settings for everything else will do. Easy!\"]]],[10,0],[1,\"h2\",[[0,[],0,\"2 - Create a Container\"]]],[1,\"p\",[[0,[],0,\"Once your storage account has spun up, go to your mew account and look on the left hand side. Under 'data storage' you should see this\"]]],[1,\"p\",[]],[10,1],[1,\"p\",[[0,[],0,\"Select 'containers' and then you should see a button in the top left to add a container.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Name this whatever you like, it won't really matter. I call mine 'blobs' because that's what we are looking to put in there. \"]]],[1,\"h2\",[[0,[],0,\"3 - Upload Your Files\"]]],[1,\"p\",[[0,[],0,\"Click on your newly named container, and then you'll see an upload button.\"]]],[10,3],[1,\"p\",[[0,[],0,\"Select this and add any files you want, as long as they are images or videos.\"]]],[1,\"h2\",[[0,[],0,\"4 - Generate a SAS token.\"]]],[1,\"p\",[[0,[],0,\"To share the file within Altspace, we meed to generate a Shared Access Signature (SAS) token. To do this, there are three dots to the very right of your uploaded file name.\"]]],[10,4],[1,\"p\",[[0,[],0,\"Click these three dots to get this menu:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Here we can select 'Generate SAS'.\"]]],[1,\"p\",[[0,[],0,\"This will take you to a mew page to generate the SAS token. The only field we meed to worry about is the 'Expiry'. This is when the SAS token will expire. This needs to be after when you want to see the images or videos in Altspace, but it's super convenient that the access will revoke itself after the time. In this example, i set the expiry time for a month after the time I created it. If you aren't worried about sharing your content, you could just set this for several years!\"]]],[10,6],[1,\"p\",[[0,[],0,\"Once you've figured out how long you'd like the url to be accessible, click 'Generate SAS token and URL'.\"]]],[1,\"p\",[[0,[],0,\"This will give you two fields, a Blob SAS token, and a Blob SAS url. For the media player we'll use the url, so copy this, and then head back to the media player tool and use that as normal. If you're not sure how to do this, check out the \"],[0,[2],1,\"Microsoft docs\"],[0,[],0,\" for more information!\"]]],[1,\"p\",[[0,[],0,\"Once your SAS token has expired, all that will happen is Altspace won't be able to load the video or file, so this solution is perfect for sharing anything you want to retain control of, hosting the files privately, and only making them 'publicly' readable for a limited time.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>If you've hosted meet-ups or events in AltSpace, you've probably tried the <a href=\"https://multimedia-console.altvr.com/\">Altspace multimedia console</a>. Whilst it's a great tool, it requires a complete publicly available hosted video or photo file. You also can't use popular video platforms like Youtube as these are not supported (for a number of reasons). There are some great freely available hosting sites, but it dawned on me, why cant we just host our own files on Azure? Well. You can. And, it's simple.</p><h2 id=\"1create-storage-account\">1 - Create Storage Account</h2><p>Head to <a href=\"portal.azure.com\">portal.azure.com</a>, and assuming you're all signed up with a subscription, select 'create a resource'. You're looking for a standard storage account like this. Name it whatever you like, but the standard settings for everything else will do. Easy!</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.09.53.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"562\" height=\"862\"><figcaption>Create a storage account on Azure</figcaption></figure><h2 id=\"2create-a-container\">2 - Create a Container</h2><p>Once your storage account has spun up, go to your mew account and look on the left hand side. Under 'data storage' you should see this</p><p></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.23.06.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"672\" height=\"442\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/08/Screenshot-2022-08-15-at-15.23.06.png 600w, __GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.23.06.png 672w\"></figure><p>Select 'containers' and then you should see a button in the top left to add a container.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.10.04.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"250\" height=\"90\"></figure><p>Name this whatever you like, it won't really matter. I call mine 'blobs' because that's what we are looking to put in there. </p><h2 id=\"3upload-your-files\">3 - Upload Your Files</h2><p>Click on your newly named container, and then you'll see an upload button.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.10.26.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"216\" height=\"100\"></figure><p>Select this and add any files you want, as long as they are images or videos.</p><h2 id=\"4generate-a-sas-token\">4 - Generate a SAS token.</h2><p>To share the file within Altspace, we meed to generate a Shared Access Signature (SAS) token. To do this, there are three dots to the very right of your uploaded file name.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.10.39.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"175\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/08/Screenshot-2022-08-15-at-15.10.39.png 600w, __GHOST_URL__/content/images/size/w1000/2022/08/Screenshot-2022-08-15-at-15.10.39.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/08/Screenshot-2022-08-15-at-15.10.39.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/08/Screenshot-2022-08-15-at-15.10.39.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Click these three dots to get this menu:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.10.47.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"530\" height=\"974\"></figure><p>Here we can select 'Generate SAS'.</p><p>This will take you to a mew page to generate the SAS token. The only field we meed to worry about is the 'Expiry'. This is when the SAS token will expire. This needs to be after when you want to see the images or videos in Altspace, but it's super convenient that the access will revoke itself after the time. In this example, i set the expiry time for a month after the time I created it. If you aren't worried about sharing your content, you could just set this for several years!</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.11.09.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1764\" height=\"1610\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/08/Screenshot-2022-08-15-at-15.11.09.png 600w, __GHOST_URL__/content/images/size/w1000/2022/08/Screenshot-2022-08-15-at-15.11.09.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/08/Screenshot-2022-08-15-at-15.11.09.png 1600w, __GHOST_URL__/content/images/2022/08/Screenshot-2022-08-15-at-15.11.09.png 1764w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Once you've figured out how long you'd like the url to be accessible, click 'Generate SAS token and URL'.</p><p>This will give you two fields, a Blob SAS token, and a Blob SAS url. For the media player we'll use the url, so copy this, and then head back to the media player tool and use that as normal. If you're not sure how to do this, check out the <a href=\"https://docs.microsoft.com/en-us/windows/mixed-reality/altspace-vr/tutorials/multimedia-console\">Microsoft docs</a> for more information!</p><p>Once your SAS token has expired, all that will happen is Altspace won't be able to load the video or file, so this solution is perfect for sharing anything you want to retain control of, hosting the files privately, and only making them 'publicly' readable for a limited time.</p>","comment_id":"62fa557dff7f9e0001ab82ad","plaintext":"If you've hosted meet-ups or events in AltSpace, you've probably tried the \nAltspace multimedia console [https://multimedia-console.altvr.com/]. Whilst it's\na great tool, it requires a complete publicly available hosted video or photo\nfile. You also can't use popular video platforms like Youtube as these are not\nsupported (for a number of reasons). There are some great freely available\nhosting sites, but it dawned on me, why cant we just host our own files on\nAzure? Well. You can. And, it's simple.\n\n1 - Create Storage Account\nHead to portal.azure.com, and assuming you're all signed up with a subscription,\nselect 'create a resource'. You're looking for a standard storage account like\nthis. Name it whatever you like, but the standard settings for everything else\nwill do. Easy!\n\nCreate a storage account on Azure2 - Create a Container\nOnce your storage account has spun up, go to your mew account and look on the\nleft hand side. Under 'data storage' you should see this\n\n\n\nSelect 'containers' and then you should see a button in the top left to add a\ncontainer.\n\nName this whatever you like, it won't really matter. I call mine 'blobs' because\nthat's what we are looking to put in there. \n\n3 - Upload Your Files\nClick on your newly named container, and then you'll see an upload button.\n\nSelect this and add any files you want, as long as they are images or videos.\n\n4 - Generate a SAS token.\nTo share the file within Altspace, we meed to generate a Shared Access Signature\n(SAS) token. To do this, there are three dots to the very right of your uploaded\nfile name.\n\nClick these three dots to get this menu:\n\nHere we can select 'Generate SAS'.\n\nThis will take you to a mew page to generate the SAS token. The only field we\nmeed to worry about is the 'Expiry'. This is when the SAS token will expire.\nThis needs to be after when you want to see the images or videos in Altspace,\nbut it's super convenient that the access will revoke itself after the time. In\nthis example, i set the expiry time for a month after the time I created it. If\nyou aren't worried about sharing your content, you could just set this for\nseveral years!\n\nOnce you've figured out how long you'd like the url to be accessible, click\n'Generate SAS token and URL'.\n\nThis will give you two fields, a Blob SAS token, and a Blob SAS url. For the\nmedia player we'll use the url, so copy this, and then head back to the media\nplayer tool and use that as normal. If you're not sure how to do this, check out\nthe Microsoft docs\n[https://docs.microsoft.com/en-us/windows/mixed-reality/altspace-vr/tutorials/multimedia-console] \nfor more information!\n\nOnce your SAS token has expired, all that will happen is Altspace won't be able\nto load the video or file, so this solution is perfect for sharing anything you\nwant to retain control of, hosting the files privately, and only making them\n'publicly' readable for a limited time.","feature_image":"__GHOST_URL__/content/images/2022/08/Fergus_Kidd2022-08-15_15-16-42.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-08-15T14:17:33.000Z","updated_at":"2022-08-15T14:38:13.000Z","published_at":"2022-08-15T14:38:13.000Z","custom_excerpt":"If you've hosted meet-ups or events in AltSpace, you've probably tried the Altspace multimedia console. Let's host the media in Azure.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"63089e40ff7f9e0001ab8318","uuid":"064c9982-f724-4c66-ba5e-eba70ae6d357","title":"Unevenly Distributed - Creating with AI","slug":"untitled","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=bAIRk7xKigM&t=941s\",\"html\":\"<iframe width=\\\"200\\\" height=\\\"113\\\" src=\\\"https://www.youtube.com/embed/bAIRk7xKigM?start=941&feature=oembed\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen title=\\\"Creating With AI - Unevenly Distributed podcast chat with Fergus Kidd\\\"></iframe>\",\"type\":\"video\",\"metadata\":{\"title\":\"Creating With AI - Unevenly Distributed podcast chat with Fergus Kidd\",\"author_name\":\"Nexavise\",\"author_url\":\"https://www.youtube.com/channel/UCN7pok8MEvP--M8jALGVwzQ\",\"height\":113,\"width\":200,\"version\":\"1.0\",\"provider_name\":\"YouTube\",\"provider_url\":\"https://www.youtube.com/\",\"thumbnail_height\":360,\"thumbnail_width\":480,\"thumbnail_url\":\"https://i.ytimg.com/vi/bAIRk7xKigM/hqdefault.jpg\"}}],[\"bookmark\",{\"url\":\"https://nexavise.com/creating-with-ai-unevenly-distributed-podcast-chat-with-fergus-kidd\",\"metadata\":{\"url\":\"https://nexavise.com/creating-with-ai-unevenly-distributed-podcast-chat-with-fergus-kidd\",\"title\":\"Creating With AI - Unevenly Distributed podcast chat with Fergus Kidd - Nexavise\",\"description\":\"As AI research continues to evolve, new tools like DALL-E, GPT-3, and Midjourney are showing us the potential for AI to help us synthesize new art, data, and objects of all kinds. Fergus Kidd is Avanadeâ€™s Emerging Technology Research and Development Engineering Lead, and in this talk we dig into howâ€¦\",\"author\":\"Jeff Vilimek\",\"publisher\":\"Nexavise\",\"thumbnail\":\"https://secure.gravatar.com/avatar/4b13e16c3b101031f89f181ad77d5c3a?s=96&d=mm&r=g\",\"icon\":\"https://nexavise.com/wp-content/uploads/2022/04/cropped-nexavise-logo-black-512-270x270.png\"}}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"As AI research continues to evolve, new tools like DALL-E, GPT-3, and Midjourney are showing us the potential for AI to help us synthesize new art, data, and objects of all kinds. Fergus Kidd is Avanadeâ€™s Emerging Technology Research and Development Engineering Lead, and in this talk we dig into how that works, examples, and whatâ€™s next.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Learn more about the Unevenly Distributed podcast and Jeff at Nexavise:\"]]],[10,1],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>As AI research continues to evolve, new tools like DALL-E, GPT-3, and Midjourney are showing us the potential for AI to help us synthesize new art, data, and objects of all kinds. Fergus Kidd is Avanadeâ€™s Emerging Technology Research and Development Engineering Lead, and in this talk we dig into how that works, examples, and whatâ€™s next.</p><figure class=\"kg-card kg-embed-card\"><iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/bAIRk7xKigM?start=941&feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Creating With AI - Unevenly Distributed podcast chat with Fergus Kidd\"></iframe></figure><p>Learn more about the Unevenly Distributed podcast and Jeff at Nexavise:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://nexavise.com/creating-with-ai-unevenly-distributed-podcast-chat-with-fergus-kidd\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Creating With AI - Unevenly Distributed podcast chat with Fergus Kidd - Nexavise</div><div class=\"kg-bookmark-description\">As AI research continues to evolve, new tools like DALL-E, GPT-3, and Midjourney are showing us the potential for AI to help us synthesize new art, data, and objects of all kinds. Fergus Kidd is Avanadeâ€™s Emerging Technology Research and Development Engineering Lead, and in this talk we dig into howâ€¦</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://nexavise.com/wp-content/uploads/2022/04/cropped-nexavise-logo-black-512-270x270.png\" alt=\"\"><span class=\"kg-bookmark-author\">Nexavise</span><span class=\"kg-bookmark-publisher\">Jeff Vilimek</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://secure.gravatar.com/avatar/4b13e16c3b101031f89f181ad77d5c3a?s&#x3D;96&amp;d&#x3D;mm&amp;r&#x3D;g\" alt=\"\"></div></a></figure>","comment_id":"63089e40ff7f9e0001ab8318","plaintext":"As AI research continues to evolve, new tools like DALL-E, GPT-3, and Midjourney\nare showing us the potential for AI to help us synthesize new art, data, and\nobjects of all kinds. Fergus Kidd is Avanadeâ€™s Emerging Technology Research and\nDevelopment Engineering Lead, and in this talk we dig into how that works,\nexamples, and whatâ€™s next.\n\nLearn more about the Unevenly Distributed podcast and Jeff at Nexavise:\n\nCreating With AI - Unevenly Distributed podcast chat with Fergus Kidd -\nNexavise\nAs AI research continues to evolve, new tools like DALL-E, GPT-3, and\nMidjourney\nare showing us the potential for AI to help us synthesize new art, data, and\nobjects of all kinds. Fergus Kidd is Avanadeâ€™s Emerging Technology Research and\nDevelopment Engineering Lead, and in this talk we dig into howâ€¦NexaviseJeff\nVilimek\n[https://nexavise.com/creating-with-ai-unevenly-distributed-podcast-chat-with-fergus-kidd]","feature_image":"__GHOST_URL__/content/images/2022/08/Screenshot-2022-08-26-at-11.23.15.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-08-26T10:19:44.000Z","updated_at":"2022-08-26T10:23:58.000Z","published_at":"2022-08-26T10:23:58.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"6308a077ff7f9e0001ab8330","uuid":"20688a5f-82bf-4800-9fa5-3a4c5ed504ce","title":"Meta Me","slug":"meta-me","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"header\",{\"size\":\"small\",\"style\":\"accent\",\"buttonEnabled\":false,\"header\":\"Environment\",\"subheader\":\"Making the backdrop\"}],[\"image\",{\"caption\":\"a 3D render of my webcam backdrop\",\"src\":\"__GHOST_URL__/content/images/2022/08/SG1.png\",\"width\":2686,\"height\":1765}],[\"header\",{\"size\":\"small\",\"style\":\"accent\",\"buttonEnabled\":false,\"header\":\"Metahuman\",\"subheader\":\"Modelling myself (badly)\"}],[\"video\",{\"loop\":true,\"src\":\"__GHOST_URL__/content/media/2022/08/Screen-Recording-2022-08-26-at-11.47.06.mp4\",\"fileName\":\"Screen Recording 2022-08-26 at 11.47.06.mp4\",\"width\":1186,\"height\":1476,\"duration\":40.693,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2022/08/media-thumbnail-ember708.jpg\",\"thumbnailWidth\":1186,\"thumbnailHeight\":1476,\"caption\":\"A default MetaHuman in the idle animation loop\"}],[\"video\",{\"loop\":true,\"src\":\"__GHOST_URL__/content/media/2022/08/Screen-Recording-2022-08-26-at-11.59.53.mp4\",\"fileName\":\"Screen Recording 2022-08-26 at 11.59.53.mp4\",\"width\":1186,\"height\":1476,\"duration\":8.8,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2022/08/media-thumbnail-ember753.jpg\",\"thumbnailWidth\":1186,\"thumbnailHeight\":1476,\"caption\":\"My attempt at a MetaHuman of myself in the 'Happy' animation loop\"}],[\"header\",{\"size\":\"small\",\"style\":\"accent\",\"buttonEnabled\":false,\"header\":\"Animating\",\"subheader\":\"in UE4\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/08/PendingTake.0114.jpeg\",\"width\":1920,\"height\":1080,\"caption\":\"Meta me in my meta room\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/08/Photo-on-12-08-2022-at-15.58.jpg\",\"width\":1440,\"height\":960,\"caption\":\"Real me, with a terrible webcam\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.epicgames.com/id/authorize?response_type=code&redirect_uri=https%3A%2F%2Fmetahuman.unrealengine.com%2Feos-login-redirect&scope=basic_profile&client_id=xyza7891OORp4qeFMsqG8MGwJLsun9Tb\"]],[\"a\",[\"href\",\"https://statics.teams.cdn.office.net/evergreen-assets/safelinks/1/atp-safelinks.html\"]]],\"sections\":[[1,\"h3\",[[0,[],0,\"Making a MetaHuman of myself and animating it with UE4\"]]],[1,\"p\",[[0,[],0,\"I've been playing around with \"],[0,[0],1,\"MetaHumans\"],[0,[],0,\" from Epic Games for Unreal Engine 4 and 5. Essentially it's a platform to create super realistic animatable humans, primarily aimed at game designers creating the next generations of games. I was extremely interested in the potential for both gaming, but also in terms of Metaverse, digital assistants, and what's next in the blurring of the lines between the digital and the real.\"]]],[1,\"p\",[[0,[],0,\"I wanted to see if I could make something familiar, so I picked.... me. Mainly because I won't sue myself for my likeness, but also because how we represent ourselves in digital spaces is actually interesting. \"]]],[10,0],[1,\"p\",[[0,[],0,\"To get started I started building out my bedroom in blender. This is where I work when I am not in an office, so it made the most sense to me to model the backdrop that you normally see when I am talking or presenting. Thanks to early Metaverse work and interest in 3D art, this was actually pretty easy.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Thanks to free resources online I was even able to drop in accurate furniture, and even my PS5 for my shelf. I love Blender as a render tool, as using cycles you can get very accurate lighting effects, here I modelled the window with sunlight outside, and you can get the totally awesome mirror reflections from the ray tracing capabilities.\"]]],[1,\"p\",[[0,[],0,\"I'm not as familiar with textures, lighting, and camera setups in UE5, so the same model looked a little different when I imported it, but the basics are there!\"]]],[10,2],[1,\"p\",[[0,[],0,\"MetaHuman creator makes it very easy to create incredibly high detail human characters that are fully animatable. The very hard part is making them look like somebody real, but I tried my best.\"]]],[10,3],[1,\"p\",[[0,[],0,\"There are lots of settings, skin, textures, etc. but it is currently difficult to sculpt the faces with high precision. So, the best I could get to for my own face is....\"]]],[10,4],[1,\"p\",[[0,[],0,\"For me the benefit of MetaHumans is the incredible detail you get by default. Yes it's not easy to model specific people, but that isn't what the tool was designed for. It was designed to make characters more quickly and easily with a high level of detail and ease of animation.\"]]],[10,5],[1,\"p\",[[0,[],0,\"I'm no gaming or unreal expert, so I used this \"],[0,[1],1,\"tutorial\"],[0,[],0,\" very heavily to animate my MetaHuman using live-link to an iPhone.\"]]],[1,\"p\",[[0,[],0,\"First I dropped my MetaHuman and my environment into UE4, and played around with the lighting. Notice the different in texture appearance, lack of mirror, and other lighting issues compared to the blender render higher up. I've put in a real picture of me for comparison too.\"]]],[10,6],[1,\"p\",[]],[10,7],[1,\"h2\",[[0,[],0,\"Animating\"]]],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<h3 id=\"making-a-metahuman-of-myself-and-animating-it-with-ue4\">Making a MetaHuman of myself and animating it with UE4</h3><p>I've been playing around with <a href=\"https://www.epicgames.com/id/authorize?response_type=code&amp;redirect_uri=https%3A%2F%2Fmetahuman.unrealengine.com%2Feos-login-redirect&amp;scope=basic_profile&amp;client_id=xyza7891OORp4qeFMsqG8MGwJLsun9Tb\">MetaHumans</a> from Epic Games for Unreal Engine 4 and 5. Essentially it's a platform to create super realistic animatable humans, primarily aimed at game designers creating the next generations of games. I was extremely interested in the potential for both gaming, but also in terms of Metaverse, digital assistants, and what's next in the blurring of the lines between the digital and the real.</p><p>I wanted to see if I could make something familiar, so I picked.... me. Mainly because I won't sue myself for my likeness, but also because how we represent ourselves in digital spaces is actually interesting. </p><div class=\"kg-card kg-header-card kg-width-full kg-size-small kg-style-accent\" style=\"\" data-kg-background-image=\"\"><h2 class=\"kg-header-card-header\" id=\"environment\">Environment</h2><h3 class=\"kg-header-card-subheader\" id=\"making-the-backdrop\">Making the backdrop</h3></div><p>To get started I started building out my bedroom in blender. This is where I work when I am not in an office, so it made the most sense to me to model the backdrop that you normally see when I am talking or presenting. Thanks to early Metaverse work and interest in 3D art, this was actually pretty easy.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/08/SG1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1314\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/08/SG1.png 600w, __GHOST_URL__/content/images/size/w1000/2022/08/SG1.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/08/SG1.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/08/SG1.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>a 3D render of my webcam backdrop</figcaption></figure><p>Thanks to free resources online I was even able to drop in accurate furniture, and even my PS5 for my shelf. I love Blender as a render tool, as using cycles you can get very accurate lighting effects, here I modelled the window with sunlight outside, and you can get the totally awesome mirror reflections from the ray tracing capabilities.</p><p>I'm not as familiar with textures, lighting, and camera setups in UE5, so the same model looked a little different when I imported it, but the basics are there!</p><div class=\"kg-card kg-header-card kg-width-full kg-size-small kg-style-accent\" style=\"\" data-kg-background-image=\"\"><h2 class=\"kg-header-card-header\" id=\"metahuman\">Metahuman</h2><h3 class=\"kg-header-card-subheader\" id=\"modelling-myself-badly\">Modelling myself (badly)</h3></div><p>MetaHuman creator makes it very easy to create incredibly high detail human characters that are fully animatable. The very hard part is making them look like somebody real, but I tried my best.</p><figure class=\"kg-card kg-video-card kg-card-hascaption\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2022/08/Screen-Recording-2022-08-26-at-11.47.06.mp4\" poster=\"https://img.spacergif.org/v1/1186x1476/0a/spacer.png\" width=\"1186\" height=\"1476\" loop autoplay muted playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2022/08/media-thumbnail-ember708.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container kg-video-hide\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div><figcaption>A default MetaHuman in the idle animation loop</figcaption></figure><p>There are lots of settings, skin, textures, etc. but it is currently difficult to sculpt the faces with high precision. So, the best I could get to for my own face is....</p><figure class=\"kg-card kg-video-card kg-card-hascaption\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2022/08/Screen-Recording-2022-08-26-at-11.59.53.mp4\" poster=\"https://img.spacergif.org/v1/1186x1476/0a/spacer.png\" width=\"1186\" height=\"1476\" loop autoplay muted playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2022/08/media-thumbnail-ember753.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container kg-video-hide\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div><figcaption>My attempt at a MetaHuman of myself in the 'Happy' animation loop</figcaption></figure><p>For me the benefit of MetaHumans is the incredible detail you get by default. Yes it's not easy to model specific people, but that isn't what the tool was designed for. It was designed to make characters more quickly and easily with a high level of detail and ease of animation.</p><div class=\"kg-card kg-header-card kg-width-full kg-size-small kg-style-accent\" style=\"\" data-kg-background-image=\"\"><h2 class=\"kg-header-card-header\" id=\"animating\">Animating</h2><h3 class=\"kg-header-card-subheader\" id=\"in-ue4\">in UE4</h3></div><p>I'm no gaming or unreal expert, so I used this <a href=\"https://statics.teams.cdn.office.net/evergreen-assets/safelinks/1/atp-safelinks.html\">tutorial</a> very heavily to animate my MetaHuman using live-link to an iPhone.</p><p>First I dropped my MetaHuman and my environment into UE4, and played around with the lighting. Notice the different in texture appearance, lack of mirror, and other lighting issues compared to the blender render higher up. I've put in a real picture of me for comparison too.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/08/PendingTake.0114.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/08/PendingTake.0114.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2022/08/PendingTake.0114.jpeg 1000w, __GHOST_URL__/content/images/size/w1600/2022/08/PendingTake.0114.jpeg 1600w, __GHOST_URL__/content/images/2022/08/PendingTake.0114.jpeg 1920w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Meta me in my meta room</figcaption></figure><p></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/08/Photo-on-12-08-2022-at-15.58.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"1440\" height=\"960\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/08/Photo-on-12-08-2022-at-15.58.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/08/Photo-on-12-08-2022-at-15.58.jpg 1000w, __GHOST_URL__/content/images/2022/08/Photo-on-12-08-2022-at-15.58.jpg 1440w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Real me, with a terrible webcam</figcaption></figure><h2 id=\"animating\">Animating</h2>","comment_id":"6308a077ff7f9e0001ab8330","plaintext":"Making a MetaHuman of myself and animating it with UE4\nI've been playing around with MetaHumans\n[https://www.epicgames.com/id/authorize?response_type=code&redirect_uri=https%3A%2F%2Fmetahuman.unrealengine.com%2Feos-login-redirect&scope=basic_profile&client_id=xyza7891OORp4qeFMsqG8MGwJLsun9Tb] \nfrom Epic Games for Unreal Engine 4 and 5. Essentially it's a platform to create\nsuper realistic animatable humans, primarily aimed at game designers creating\nthe next generations of games. I was extremely interested in the potential for\nboth gaming, but also in terms of Metaverse, digital assistants, and what's next\nin the blurring of the lines between the digital and the real.\n\nI wanted to see if I could make something familiar, so I picked.... me. Mainly\nbecause I won't sue myself for my likeness, but also because how we represent\nourselves in digital spaces is actually interesting. \n\nEnvironment\nMaking the backdrop\nTo get started I started building out my bedroom in blender. This is where I\nwork when I am not in an office, so it made the most sense to me to model the\nbackdrop that you normally see when I am talking or presenting. Thanks to early\nMetaverse work and interest in 3D art, this was actually pretty easy.\n\na 3D render of my webcam backdropThanks to free resources online I was even able\nto drop in accurate furniture, and even my PS5 for my shelf. I love Blender as a\nrender tool, as using cycles you can get very accurate lighting effects, here I\nmodelled the window with sunlight outside, and you can get the totally awesome\nmirror reflections from the ray tracing capabilities.\n\nI'm not as familiar with textures, lighting, and camera setups in UE5, so the\nsame model looked a little different when I imported it, but the basics are\nthere!\n\nMetahuman\nModelling myself (badly)\nMetaHuman creator makes it very easy to create incredibly high detail human\ncharacters that are fully animatable. The very hard part is making them look\nlike somebody real, but I tried my best.\n\n0:00/1Ã—A default MetaHuman in the idle animation loopThere are lots of settings,\nskin, textures, etc. but it is currently difficult to sculpt the faces with high\nprecision. So, the best I could get to for my own face is....\n\n0:00/1Ã—My attempt at a MetaHuman of myself in the 'Happy' animation loopFor me\nthe benefit of MetaHumans is the incredible detail you get by default. Yes it's\nnot easy to model specific people, but that isn't what the tool was designed\nfor. It was designed to make characters more quickly and easily with a high\nlevel of detail and ease of animation.\n\nAnimating\nin UE4\nI'm no gaming or unreal expert, so I used this tutorial\n[https://statics.teams.cdn.office.net/evergreen-assets/safelinks/1/atp-safelinks.html] \nvery heavily to animate my MetaHuman using live-link to an iPhone.\n\nFirst I dropped my MetaHuman and my environment into UE4, and played around with\nthe lighting. Notice the different in texture appearance, lack of mirror, and\nother lighting issues compared to the blender render higher up. I've put in a\nreal picture of me for comparison too.\n\nMeta me in my meta room\n\nReal me, with a terrible webcamAnimating","feature_image":"__GHOST_URL__/content/images/2022/08/9Nx25_dY.jpeg","featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-08-26T10:29:11.000Z","updated_at":"2022-08-26T11:17:22.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"63359a3b3018ba000144183f","uuid":"c9e39d44-cf7a-4500-af23-32729157d277","title":"DALL-E-2 for Material Textures","slug":"dall-e-2-2","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"gallery\",{\"images\":[{\"fileName\":\"DALLÂ·E 2022-09-28 11.18.30 - 3d textrue of a rock.png\",\"row\":0,\"width\":1024,\"height\":1024,\"src\":\"__GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.18.30---3d-textrue-of-a-rock.png\"},{\"fileName\":\"DALLÂ·E 2022-09-28 11.23.05 - sand texture.png\",\"row\":0,\"width\":1024,\"height\":1024,\"src\":\"__GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.23.05---sand-texture.png\"},{\"fileName\":\"DALLÂ·E 2022-09-28 11.28.02 - grass texture.png\",\"row\":0,\"width\":1024,\"height\":1024,\"src\":\"__GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.28.02---grass-texture.png\"},{\"fileName\":\"DALLÂ·E 2022-09-28 11.33.31 - plain white paint texture.png\",\"row\":1,\"width\":1024,\"height\":1024,\"src\":\"__GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.33.31---plain-white-paint-texture.png\"},{\"fileName\":\"DALLÂ·E 2022-09-28 11.37.10 - wooden floor texture.png\",\"row\":1,\"width\":1024,\"height\":1024,\"src\":\"__GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.37.10---wooden-floor-texture.png\"},{\"fileName\":\"DALLÂ·E 2022-09-29 14.26.09 - texture of ceramic tile.png\",\"row\":1,\"width\":1024,\"height\":1024,\"src\":\"__GHOST_URL__/content/images/2022/09/DALL-E-2022-09-29-14.26.09---texture-of-ceramic-tile.png\"}],\"caption\":\"I asked DALL-E-2 to produce several textures I might use in a project by simply thank thank typing 'texture of ....'\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/09/BlenderDalleTextures.png\",\"width\":1920,\"height\":1080}]],\"markups\":[[\"a\",[\"href\",\"https://80.lv/articles/tutorial-creating-materials-with-dall-e-2-substance-3d/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Playing around with DALL-E-2 a lot, both for work and fun, suddenly gave me an idea. What if DALL-E-2 could be useful for much more. I do a lot of 3D modelling too for fun, but a huge issue with amateur digital art is textures, and where to get them from. There a few great free resources, but many are credit based, and limit you in terms of quality and quantity. This can be quite annoying when you're only working on a prototype, or just want to see how something might look in a certain photo texture. Going to a search engine is a next step, but then you have to worry about copy write and watermarks. \"]]],[1,\"p\",[[0,[],0,\"Could DALL-E-2 be an easier way to generate testing textures? Clearly we won't be able to get depth information or anything without a lot of hard work, but could these work for background or supplementary textures?\"]]],[1,\"p\",[[0,[],0,\"Let's give it a shot!\"]]],[10,0],[1,\"p\",[[0,[],0,\"By simply asking for a texture, DALL-E-2 seems to know that we meed a nice flat image that looks fairly uniform. We can easily change details like the colour or grain of wood, simply by asking DALL-E for a variation. It could be a very quick and cost effective way to narrow down what thank you of texture you want before making an investment on your final choice. \"]]],[1,\"h3\",[[0,[],0,\"In practice?\"]]],[1,\"p\",[[0,[],0,\"What do these textures look like in practice? I threw some into a quick blender render to show you. A bit of playing with texture mapping made the wood texture a but more useful.\"]]],[10,1],[1,\"p\",[[0,[],0,\"You can see I did't even try to clean up the watermark in anyway, but you can quickly get a good idea of how different materials interact with light, and surfaces in your scene. \"]]],[1,\"h3\",[[0,[],0,\"What it can't do\"]]],[1,\"p\",[[0,[],0,\"DALL-E-2 does not seem to be able to generate the thank you're of seemless texture we would normally look for. That's ok for quick testing, \"]]],[1,\"p\",[[0,[],0,\"There's nothing stopping you from taking this idea further, and making a DALL-E-2 texture seemless, and then generating all the other versions we'd meed for high detail texturing though. DALL-E-2 actually comes in useful again, to join it's own seems up with in-painting. I haven't tried this yet, but there is a tutorial you can follow here: \"],[0,[0],1,\"https://80.lv/articles/tutorial-creating-materials-with-dall-e-2-substance-3d/\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>Playing around with DALL-E-2 a lot, both for work and fun, suddenly gave me an idea. What if DALL-E-2 could be useful for much more. I do a lot of 3D modelling too for fun, but a huge issue with amateur digital art is textures, and where to get them from. There a few great free resources, but many are credit based, and limit you in terms of quality and quantity. This can be quite annoying when you're only working on a prototype, or just want to see how something might look in a certain photo texture. Going to a search engine is a next step, but then you have to worry about copy write and watermarks. </p><p>Could DALL-E-2 be an easier way to generate testing textures? Clearly we won't be able to get depth information or anything without a lot of hard work, but could these work for background or supplementary textures?</p><p>Let's give it a shot!</p><figure class=\"kg-card kg-gallery-card kg-width-wide kg-card-hascaption\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.18.30---3d-textrue-of-a-rock.png\" width=\"1024\" height=\"1024\" loading=\"lazy\" alt srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/DALL-E-2022-09-28-11.18.30---3d-textrue-of-a-rock.png 600w, __GHOST_URL__/content/images/size/w1000/2022/09/DALL-E-2022-09-28-11.18.30---3d-textrue-of-a-rock.png 1000w, __GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.18.30---3d-textrue-of-a-rock.png 1024w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.23.05---sand-texture.png\" width=\"1024\" height=\"1024\" loading=\"lazy\" alt srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/DALL-E-2022-09-28-11.23.05---sand-texture.png 600w, __GHOST_URL__/content/images/size/w1000/2022/09/DALL-E-2022-09-28-11.23.05---sand-texture.png 1000w, __GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.23.05---sand-texture.png 1024w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.28.02---grass-texture.png\" width=\"1024\" height=\"1024\" loading=\"lazy\" alt srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/DALL-E-2022-09-28-11.28.02---grass-texture.png 600w, __GHOST_URL__/content/images/size/w1000/2022/09/DALL-E-2022-09-28-11.28.02---grass-texture.png 1000w, __GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.28.02---grass-texture.png 1024w\" sizes=\"(min-width: 720px) 720px\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.33.31---plain-white-paint-texture.png\" width=\"1024\" height=\"1024\" loading=\"lazy\" alt srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/DALL-E-2022-09-28-11.33.31---plain-white-paint-texture.png 600w, __GHOST_URL__/content/images/size/w1000/2022/09/DALL-E-2022-09-28-11.33.31---plain-white-paint-texture.png 1000w, __GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.33.31---plain-white-paint-texture.png 1024w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.37.10---wooden-floor-texture.png\" width=\"1024\" height=\"1024\" loading=\"lazy\" alt srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/DALL-E-2022-09-28-11.37.10---wooden-floor-texture.png 600w, __GHOST_URL__/content/images/size/w1000/2022/09/DALL-E-2022-09-28-11.37.10---wooden-floor-texture.png 1000w, __GHOST_URL__/content/images/2022/09/DALL-E-2022-09-28-11.37.10---wooden-floor-texture.png 1024w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"__GHOST_URL__/content/images/2022/09/DALL-E-2022-09-29-14.26.09---texture-of-ceramic-tile.png\" width=\"1024\" height=\"1024\" loading=\"lazy\" alt srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/DALL-E-2022-09-29-14.26.09---texture-of-ceramic-tile.png 600w, __GHOST_URL__/content/images/size/w1000/2022/09/DALL-E-2022-09-29-14.26.09---texture-of-ceramic-tile.png 1000w, __GHOST_URL__/content/images/2022/09/DALL-E-2022-09-29-14.26.09---texture-of-ceramic-tile.png 1024w\" sizes=\"(min-width: 720px) 720px\"></div></div></div><figcaption>I asked DALL-E-2 to produce several textures I might use in a project by simply thank thank typing 'texture of ....'</figcaption></figure><p>By simply asking for a texture, DALL-E-2 seems to know that we meed a nice flat image that looks fairly uniform. We can easily change details like the colour or grain of wood, simply by asking DALL-E for a variation. It could be a very quick and cost effective way to narrow down what thank you of texture you want before making an investment on your final choice. </p><h3 id=\"in-practice\">In practice?</h3><p>What do these textures look like in practice? I threw some into a quick blender render to show you. A bit of playing with texture mapping made the wood texture a but more useful.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/09/BlenderDalleTextures.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/BlenderDalleTextures.png 600w, __GHOST_URL__/content/images/size/w1000/2022/09/BlenderDalleTextures.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/09/BlenderDalleTextures.png 1600w, __GHOST_URL__/content/images/2022/09/BlenderDalleTextures.png 1920w\" sizes=\"(min-width: 720px) 720px\"></figure><p>You can see I did't even try to clean up the watermark in anyway, but you can quickly get a good idea of how different materials interact with light, and surfaces in your scene. </p><h3 id=\"what-it-cant-do\">What it can't do</h3><p>DALL-E-2 does not seem to be able to generate the thank you're of seemless texture we would normally look for. That's ok for quick testing, </p><p>There's nothing stopping you from taking this idea further, and making a DALL-E-2 texture seemless, and then generating all the other versions we'd meed for high detail texturing though. DALL-E-2 actually comes in useful again, to join it's own seems up with in-painting. I haven't tried this yet, but there is a tutorial you can follow here: <a href=\"https://80.lv/articles/tutorial-creating-materials-with-dall-e-2-substance-3d/\">https://80.lv/articles/tutorial-creating-materials-with-dall-e-2-substance-3d/</a></p>","comment_id":"63359a3b3018ba000144183f","plaintext":"Playing around with DALL-E-2 a lot, both for work and fun, suddenly gave me an\nidea. What if DALL-E-2 could be useful for much more. I do a lot of 3D modelling\ntoo for fun, but a huge issue with amateur digital art is textures, and where to\nget them from. There a few great free resources, but many are credit based, and\nlimit you in terms of quality and quantity. This can be quite annoying when\nyou're only working on a prototype, or just want to see how something might look\nin a certain photo texture. Going to a search engine is a next step, but then\nyou have to worry about copy write and watermarks. \n\nCould DALL-E-2 be an easier way to generate testing textures? Clearly we won't\nbe able to get depth information or anything without a lot of hard work, but\ncould these work for background or supplementary textures?\n\nLet's give it a shot!\n\nI asked DALL-E-2 to produce several textures I might use in a project by simply\nthank thank typing 'texture of ....'By simply asking for a texture, DALL-E-2\nseems to know that we meed a nice flat image that looks fairly uniform. We can\neasily change details like the colour or grain of wood, simply by asking DALL-E\nfor a variation. It could be a very quick and cost effective way to narrow down\nwhat thank you of texture you want before making an investment on your final\nchoice. \n\nIn practice?\nWhat do these textures look like in practice? I threw some into a quick blender\nrender to show you. A bit of playing with texture mapping made the wood texture\na but more useful.\n\nYou can see I did't even try to clean up the watermark in anyway, but you can\nquickly get a good idea of how different materials interact with light, and\nsurfaces in your scene. \n\nWhat it can't do\nDALL-E-2 does not seem to be able to generate the thank you're of seemless\ntexture we would normally look for. That's ok for quick testing, \n\nThere's nothing stopping you from taking this idea further, and making a\nDALL-E-2 texture seemless, and then generating all the other versions we'd meed\nfor high detail texturing though. DALL-E-2 actually comes in useful again, to\njoin it's own seems up with in-painting. I haven't tried this yet, but there is\na tutorial you can follow here: \nhttps://80.lv/articles/tutorial-creating-materials-with-dall-e-2-substance-3d/","feature_image":"__GHOST_URL__/content/images/2022/09/BlenderDalleTextures-1.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-09-29T13:14:35.000Z","updated_at":"2022-09-29T15:43:51.000Z","published_at":"2022-09-29T13:31:37.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"63653904c9b3c50001bc675f","uuid":"fc2252db-d486-4e18-8a87-57b734008939","title":"AI Generated Textures - now in full","slug":"ai-generated-textures-now-in-full","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"callout\",{\"calloutEmoji\":\"ðŸ’¡\",\"calloutText\":\"Checkout the <a href=\\\"https://github.com/FergusKidd/Seamless-Texture-Generation-with-DALL-E-2\\\">GitHub repo</a> and try it yourself\",\"backgroundColor\":\"blue\"}],[\"image\",{\"caption\":\"DALL-E-2 generated texture of a yellow brick wall\",\"src\":\"__GHOST_URL__/content/images/2022/11/ddf54043-fe11-4336-83ea-dbe7943b27f3.png\",\"width\":389,\"height\":389}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/11/0b83c4c4-1504-4638-acab-53380fb3d7a2.png\",\"width\":389,\"height\":389,\"caption\":\"Seams reorganised to the middle\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/11/7bcff35f-8758-4c15-90a7-bf6197faa669.png\",\"width\":389,\"height\":389,\"caption\":\"Alpha mask for the seams\"}],[\"image\",{\"caption\":\"The second output from DALL-E-2, a now seamless texture\",\"src\":\"__GHOST_URL__/content/images/2022/11/Output_texture.png\",\"width\":1024,\"height\":1024}],[\"image\",{\"caption\":\"Some of the layers made using Materialize\",\"src\":\"__GHOST_URL__/content/images/2022/11/Screenshot-2022-11-04-at-16.31.18.png\",\"width\":2202,\"height\":456}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/11/Yellow-Brick-Cube.jpg\",\"width\":1445,\"height\":1273,\"caption\":\"The now 3D texture rendered with lighting and depth\"}],[\"callout\",{\"calloutEmoji\":\"ðŸ’¡\",\"calloutText\":\"The other nice thing about using the API is that the images don't seem to include the usual watermarking in the corner\",\"backgroundColor\":\"grey\"}],[\"image\",{\"caption\":\"White fur\",\"src\":\"__GHOST_URL__/content/images/2022/11/3eea4cd5-2316-45b1-acf4-4686ca56dbed.png\",\"width\":1570,\"height\":498}],[\"image\",{\"caption\":\"An Alien rocky planet surface\",\"src\":\"__GHOST_URL__/content/images/2022/11/70007d2d-8e5b-4703-930d-32b6c7891f3c.png\",\"width\":1570,\"height\":498}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/11/9238e4bf-e371-4136-8dd9-ced98cd7f9a0.png\",\"width\":1570,\"height\":498}],[\"image\",{\"caption\":\"Grass\",\"src\":\"__GHOST_URL__/content/images/2022/11/97c61a59-95c0-4c50-9c05-391c7879ccd5.png\",\"width\":1570,\"height\":498}]],\"markups\":[[\"a\",[\"href\",\"https://ferguskidd.com/dall-e-2-2/\"]],[\"a\",[\"href\",\"https://openai.com/blog/dall-e-api-now-available-in-public-beta/\"]],[\"a\",[\"href\",\"https://boundingboxsoftware.com/materialize/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"We discussed \"],[0,[0],1,\"AI generated textures using DALL-E-2\"],[0,[],0,\" before. However there have been two significant updates.\"]]],[3,\"ol\",[[[0,[],0,\"The \"],[0,[1],1,\"Dall-E-2 API\"],[0,[],0,\" is finally here!\"]],[[0,[],0,\"I figured out how to easily get from a flat texture to a full 3D one using \"],[0,[2],1,\"Materialize\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"The Dall-E-2 API make it infinitely easier to modify image in the way we meed to. Instead of manually doing all the image generation, we can thread it into a single python operation. \"]]],[10,0],[1,\"p\",[[0,[],0,\"I'll show you step by step what we generate. First let's ask through the API for a texture we might want to see. In this case I'm asking for a yellow brick texture. \"]]],[10,1],[1,\"p\",[[0,[],0,\"Pretty good start. The problem is if we tile this nicely over a large 3D surface, it won't look good because the edges won't match up. It 'tiles' horribly when we replicate the image many times. These always will. Let's move the 'seams' to the middle so we can see it better.\"]]],[10,2],[1,\"p\",[[0,[],0,\"All we've done here is move the outside in, by chopping the image into 4 and swapping the positions. Now we can see the seam in the middle as if we were looking at 4 tiled images. It's not the worst seam ever, DALL-E-2 has done a very good job, but we can easily tell it's not right.\"]]],[1,\"p\",[[0,[],0,\"The next step we can do is to create an alpha mask on this image as we know where the seam is, it's in a nice cross shape in the centre.\"]]],[10,3],[1,\"p\",[[0,[],0,\"This is a representation of the mask, with areas to keep showing through (in blue_ and areas to re-do in purple. So what can we do with this? Well the great thing about the DALL-E-2 API, is that we can also edit images. Sending the original image with the seams in the middle off with this mask and the original prompt will allow us to regenerate the seam area for hopefully a lovely smooth transition.\"]]],[10,4],[1,\"p\",[[0,[],0,\"And there we have it. You can clearly see some new artefacts like the blue smudge on the brick in the middle, where the mask was. DALL-E-2 has regenerated the gaps we marked out, so now the image is pretty much seamless and tile-able. Great!\"]]],[1,\"h2\",[[0,[],0,\"Let's make it 3D\"]]],[1,\"p\",[[0,[],0,\"For this to really be useable it doesn't just need to be seamless, it needs to have depth, roughness, metallic layers. All sorts of additional information about how light would interact with this texture in the real world, so we can better simulate it in a render engine. I previously thought this would be a manual and labour intensive process until i found \"],[0,[2],1,\"Materialzie\"],[0,[],0,\" from Bounding Box Software. This FREE tool takes 2D flat textures from a photo, or in our case AI, and makes them pop by generating the other information we need.\"]]],[10,5],[1,\"p\",[[0,[],0,\"This is a huge improvement on our flat tile-able image. Let's have a look at what that looks like rendered.\"]]],[10,6],[1,\"p\",[[0,[],0,\"And there we go! A now usable seamless 3D texture, completely AI generated. This opens up a word of opportunities for amazing new on-demand texture rendering!\"]]],[10,7],[1,\"p\",[[0,[],0,\"Here's some more example outputs just for fun:\"]]],[10,8],[10,9],[1,\"p\",[[0,[],0,\"Notice we may want to do some colour stabilisation accross the segments on these in the future.\"]]],[10,10],[1,\"p\",[[0,[],0,\"A good example of how things with regular patterns or directionality can go very wrong very quickly...\"]]],[10,11],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>We discussed <a href=\"https://ferguskidd.com/dall-e-2-2/\">AI generated textures using DALL-E-2</a> before. However there have been two significant updates.</p><ol><li>The <a href=\"https://openai.com/blog/dall-e-api-now-available-in-public-beta/\">Dall-E-2 API</a> is finally here!</li><li>I figured out how to easily get from a flat texture to a full 3D one using <a href=\"https://boundingboxsoftware.com/materialize/\">Materialize</a>.</li></ol><p>The Dall-E-2 API make it infinitely easier to modify image in the way we meed to. Instead of manually doing all the image generation, we can thread it into a single python operation. </p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">ðŸ’¡</div><div class=\"kg-callout-text\">Checkout the <a href=\"https://github.com/FergusKidd/Seamless-Texture-Generation-with-DALL-E-2\">GitHub repo</a> and try it yourself</div></div><p>I'll show you step by step what we generate. First let's ask through the API for a texture we might want to see. In this case I'm asking for a yellow brick texture. </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/11/ddf54043-fe11-4336-83ea-dbe7943b27f3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"389\" height=\"389\"><figcaption>DALL-E-2 generated texture of a yellow brick wall</figcaption></figure><p>Pretty good start. The problem is if we tile this nicely over a large 3D surface, it won't look good because the edges won't match up. It 'tiles' horribly when we replicate the image many times. These always will. Let's move the 'seams' to the middle so we can see it better.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/11/0b83c4c4-1504-4638-acab-53380fb3d7a2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"389\" height=\"389\"><figcaption>Seams reorganised to the middle</figcaption></figure><p>All we've done here is move the outside in, by chopping the image into 4 and swapping the positions. Now we can see the seam in the middle as if we were looking at 4 tiled images. It's not the worst seam ever, DALL-E-2 has done a very good job, but we can easily tell it's not right.</p><p>The next step we can do is to create an alpha mask on this image as we know where the seam is, it's in a nice cross shape in the centre.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/11/7bcff35f-8758-4c15-90a7-bf6197faa669.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"389\" height=\"389\"><figcaption>Alpha mask for the seams</figcaption></figure><p>This is a representation of the mask, with areas to keep showing through (in blue_ and areas to re-do in purple. So what can we do with this? Well the great thing about the DALL-E-2 API, is that we can also edit images. Sending the original image with the seams in the middle off with this mask and the original prompt will allow us to regenerate the seam area for hopefully a lovely smooth transition.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/11/Output_texture.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1024\" height=\"1024\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/11/Output_texture.png 600w, __GHOST_URL__/content/images/size/w1000/2022/11/Output_texture.png 1000w, __GHOST_URL__/content/images/2022/11/Output_texture.png 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The second output from DALL-E-2, a now seamless texture</figcaption></figure><p>And there we have it. You can clearly see some new artefacts like the blue smudge on the brick in the middle, where the mask was. DALL-E-2 has regenerated the gaps we marked out, so now the image is pretty much seamless and tile-able. Great!</p><h2 id=\"lets-make-it-3d\">Let's make it 3D</h2><p>For this to really be useable it doesn't just need to be seamless, it needs to have depth, roughness, metallic layers. All sorts of additional information about how light would interact with this texture in the real world, so we can better simulate it in a render engine. I previously thought this would be a manual and labour intensive process until i found <a href=\"https://boundingboxsoftware.com/materialize/\">Materialzie</a> from Bounding Box Software. This FREE tool takes 2D flat textures from a photo, or in our case AI, and makes them pop by generating the other information we need.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/11/Screenshot-2022-11-04-at-16.31.18.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"414\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/11/Screenshot-2022-11-04-at-16.31.18.png 600w, __GHOST_URL__/content/images/size/w1000/2022/11/Screenshot-2022-11-04-at-16.31.18.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/11/Screenshot-2022-11-04-at-16.31.18.png 1600w, __GHOST_URL__/content/images/2022/11/Screenshot-2022-11-04-at-16.31.18.png 2202w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Some of the layers made using Materialize</figcaption></figure><p>This is a huge improvement on our flat tile-able image. Let's have a look at what that looks like rendered.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/11/Yellow-Brick-Cube.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"1445\" height=\"1273\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/11/Yellow-Brick-Cube.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/11/Yellow-Brick-Cube.jpg 1000w, __GHOST_URL__/content/images/2022/11/Yellow-Brick-Cube.jpg 1445w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The now 3D texture rendered with lighting and depth</figcaption></figure><p>And there we go! A now usable seamless 3D texture, completely AI generated. This opens up a word of opportunities for amazing new on-demand texture rendering!</p><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-emoji\">ðŸ’¡</div><div class=\"kg-callout-text\">The other nice thing about using the API is that the images don't seem to include the usual watermarking in the corner</div></div><p>Here's some more example outputs just for fun:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/11/3eea4cd5-2316-45b1-acf4-4686ca56dbed.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1570\" height=\"498\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/11/3eea4cd5-2316-45b1-acf4-4686ca56dbed.png 600w, __GHOST_URL__/content/images/size/w1000/2022/11/3eea4cd5-2316-45b1-acf4-4686ca56dbed.png 1000w, __GHOST_URL__/content/images/2022/11/3eea4cd5-2316-45b1-acf4-4686ca56dbed.png 1570w\" sizes=\"(min-width: 720px) 720px\"><figcaption>White fur</figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/11/70007d2d-8e5b-4703-930d-32b6c7891f3c.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1570\" height=\"498\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/11/70007d2d-8e5b-4703-930d-32b6c7891f3c.png 600w, __GHOST_URL__/content/images/size/w1000/2022/11/70007d2d-8e5b-4703-930d-32b6c7891f3c.png 1000w, __GHOST_URL__/content/images/2022/11/70007d2d-8e5b-4703-930d-32b6c7891f3c.png 1570w\" sizes=\"(min-width: 720px) 720px\"><figcaption>An Alien rocky planet surface</figcaption></figure><p>Notice we may want to do some colour stabilisation accross the segments on these in the future.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/11/9238e4bf-e371-4136-8dd9-ced98cd7f9a0.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1570\" height=\"498\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/11/9238e4bf-e371-4136-8dd9-ced98cd7f9a0.png 600w, __GHOST_URL__/content/images/size/w1000/2022/11/9238e4bf-e371-4136-8dd9-ced98cd7f9a0.png 1000w, __GHOST_URL__/content/images/2022/11/9238e4bf-e371-4136-8dd9-ced98cd7f9a0.png 1570w\" sizes=\"(min-width: 720px) 720px\"></figure><p>A good example of how things with regular patterns or directionality can go very wrong very quickly...</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/11/97c61a59-95c0-4c50-9c05-391c7879ccd5.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1570\" height=\"498\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/11/97c61a59-95c0-4c50-9c05-391c7879ccd5.png 600w, __GHOST_URL__/content/images/size/w1000/2022/11/97c61a59-95c0-4c50-9c05-391c7879ccd5.png 1000w, __GHOST_URL__/content/images/2022/11/97c61a59-95c0-4c50-9c05-391c7879ccd5.png 1570w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Grass</figcaption></figure>","comment_id":"63653904c9b3c50001bc675f","plaintext":"We discussed AI generated textures using DALL-E-2\n[https://ferguskidd.com/dall-e-2-2/] before. However there have been two\nsignificant updates.\n\n 1. The Dall-E-2 API\n    [https://openai.com/blog/dall-e-api-now-available-in-public-beta/] is\n    finally here!\n 2. I figured out how to easily get from a flat texture to a full 3D one using \n    Materialize [https://boundingboxsoftware.com/materialize/].\n\nThe Dall-E-2 API make it infinitely easier to modify image in the way we meed\nto. Instead of manually doing all the image generation, we can thread it into a\nsingle python operation. \n\nðŸ’¡Checkout the GitHub repo\n[https://github.com/FergusKidd/Seamless-Texture-Generation-with-DALL-E-2] and\ntry it yourselfI'll show you step by step what we generate. First let's ask\nthrough the API for a texture we might want to see. In this case I'm asking for\na yellow brick texture. \n\nDALL-E-2 generated texture of a yellow brick wallPretty good start. The problem\nis if we tile this nicely over a large 3D surface, it won't look good because\nthe edges won't match up. It 'tiles' horribly when we replicate the image many\ntimes. These always will. Let's move the 'seams' to the middle so we can see it\nbetter.\n\nSeams reorganised to the middleAll we've done here is move the outside in, by\nchopping the image into 4 and swapping the positions. Now we can see the seam in\nthe middle as if we were looking at 4 tiled images. It's not the worst seam\never, DALL-E-2 has done a very good job, but we can easily tell it's not right.\n\nThe next step we can do is to create an alpha mask on this image as we know\nwhere the seam is, it's in a nice cross shape in the centre.\n\nAlpha mask for the seamsThis is a representation of the mask, with areas to keep\nshowing through (in blue_ and areas to re-do in purple. So what can we do with\nthis? Well the great thing about the DALL-E-2 API, is that we can also edit\nimages. Sending the original image with the seams in the middle off with this\nmask and the original prompt will allow us to regenerate the seam area for\nhopefully a lovely smooth transition.\n\nThe second output from DALL-E-2, a now seamless textureAnd there we have it. You\ncan clearly see some new artefacts like the blue smudge on the brick in the\nmiddle, where the mask was. DALL-E-2 has regenerated the gaps we marked out, so\nnow the image is pretty much seamless and tile-able. Great!\n\nLet's make it 3D\nFor this to really be useable it doesn't just need to be seamless, it needs to\nhave depth, roughness, metallic layers. All sorts of additional information\nabout how light would interact with this texture in the real world, so we can\nbetter simulate it in a render engine. I previously thought this would be a\nmanual and labour intensive process until i found Materialzie\n[https://boundingboxsoftware.com/materialize/] from Bounding Box Software. This\nFREE tool takes 2D flat textures from a photo, or in our case AI, and makes them\npop by generating the other information we need.\n\nSome of the layers made using MaterializeThis is a huge improvement on our flat\ntile-able image. Let's have a look at what that looks like rendered.\n\nThe now 3D texture rendered with lighting and depthAnd there we go! A now usable\nseamless 3D texture, completely AI generated. This opens up a word of\nopportunities for amazing new on-demand texture rendering!\n\nðŸ’¡The other nice thing about using the API is that the images don't seem to\ninclude the usual watermarking in the cornerHere's some more example outputs\njust for fun:\n\nWhite furAn Alien rocky planet surfaceNotice we may want to do some colour\nstabilisation accross the segments on these in the future.\n\nA good example of how things with regular patterns or directionality can go very\nwrong very quickly...\n\nGrass","feature_image":"__GHOST_URL__/content/images/2022/11/Yellow-Brick-Cube-1.jpg","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-11-04T16:08:36.000Z","updated_at":"2022-11-04T19:25:06.000Z","published_at":"2022-11-04T16:48:18.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"639c46c456bf880001dc451e","uuid":"cf6f43ef-99e7-4119-801d-8552aac83579","title":"0 Data Vision Model","slug":"0-data-vision-model","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/12/Ball.png\",\"width\":1920,\"height\":1080}],[\"button\",{\"alignment\":\"center\",\"buttonText\":\"Visit the GiHub repo\",\"buttonUrl\":\"https://github.com/FergusKidd/Blender-to-Azure-custom-vision\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/12/Screenshot-2022-12-16-at-10.22.42-1.png\",\"width\":4400,\"height\":2184}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/12/Screenshot-2022-12-16-at-10.48.51.png\",\"width\":4042,\"height\":2404}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/12/Screenshot-2022-12-16-at-10.51.49.png\",\"width\":4064,\"height\":2434}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/12/Screenshot-2022-12-16-at-10.55.35.png\",\"width\":4060,\"height\":2438}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/12/Screenshot-2022-12-16-at-11.00.16.png\",\"width\":4058,\"height\":2440}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"Gaming technology and the technology that supports it has come a long way. With more powerful GPUs in our devices than ever, revolutions in chip manufacturing, and efficiencies in calculations allow us to do incredible things like run ray tracing engines in real time, simulate realistic physics, and so much more. All of this work has huge impact on gaming and immersive experience, but outside of creative industries what can we do?\"]]],[1,\"p\",[[0,[],0,\"I've done a lot of work and learning around the metaverse. As part of this learning I've made worlds, designed objects, and learnt a lot about 3D rendering. One thing I did to support some of this research was to 3D model my own building for some experimentation around MetaHumans - more on that soon. Once I had a render of my bedroom behind my workstation, I uploaded it as a Teams camera background. After talking with a few people, I realised not everyone cottoned on that it was even a digital render (the real thing is much messier normally).\"]]],[1,\"h2\",[[0,[],0,\"This gave me an idea.\"]]],[1,\"p\",[[0,[],0,\"If people can't really tell what's real once the stream has been compressed and sent over the internet, could a machine vision model? I've always been interested in synthetic data creation for machine vision, but usually my starting point has been something real, like a photograph. What if we could make a working machine learning model with no real data at all?\"]]],[1,\"p\",[[0,[],0,\"The use case I chose was to find a tennis ball. Why? Well, it's about the size that our stretch RE1 robot can grab well, and it won't matter if it's dropped or bumps into anything, so, I was hoping the resulting model could be used to help the robot find and grab the tennis ball in the room. That's a challenge for another day though.\"]]],[1,\"p\",[[0,[],0,\"I started off with the 3D model of my room, and settled on the view from behind my workstation.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Then dropped a tennis ball onto the bed. (virtually). I wrote a script that can move the ball around the room, change the lighting conditions, and then automatically tag the ball location and upload it straight to Azure custom vision.\"]]],[10,1],[1,\"p\",[[0,[],0,\"This uses the cycles engine, with a de-noised image generation capped at about 30 seconds on my machine. The beauty of this is you can easily change or limit the quality output, or scale your compute power to generate many thousands of quality pre-tagged images much faster than you could ever photograph a real scene and manually tag it, assuming you don't have to build an environment from scratch like I did here.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Playing around with the lighting conditions allows you to simulate lots of different conditions, which from my past research, really help to build a more resilient object detection model.\"]]],[1,\"p\",[[0,[],0,\"All that's left to do is to click train, and test the results!\"]]],[1,\"h2\",[[0,[],0,\"The real thing\"]]],[1,\"p\",[[0,[],0,\"I used the minimum number of images required by Azure custom vision, just to push it to the max, but in reality with this method, you can keep adding camera views, lighting setups and noising to you heart's content.\"]]],[1,\"h3\",[[0,[],0,\"First, let's test a real image from a similar viewpoint:\"]]],[10,3],[1,\"p\",[[0,[],0,\"There we have it. The model has never seen a real tennis ball, but can identify the real one with 93.1% confidence, even with the lowest amount of synthetic data possibly generated.\"]]],[1,\"h3\",[[0,[],0,\"What if we show it an image it wasn't trained on, with more objects than expected?\"]]],[10,4],[1,\"p\",[[0,[],0,\"Well, we had to reduce the confidence limit, but we have still correctly identified all the tennis balls, even one from the reflection in the mirror.\"]]],[1,\"h3\",[[0,[],0,\"Next, the real thing from a different viewpoint:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Not the highest confidences again, but still correct guesses - so this approach is still useable for a first pass where the context of the images meeds to change.\"]]],[1,\"h3\",[[0,[],0,\"Finally, the same object but in a totally different context:\"]]],[10,6],[1,\"p\",[[0,[],0,\"So, in a totally different context, it still works!\"]]],[1,\"p\",[[0,[],0,\"Imagine what we could do with many more renders, with no meed for any manual tagging, and looking at as many objects as we liked. There are great things going on in this sector from the likes of Unity as well, but for me, Blender gives the greatest flexibility on 3D rendering, with beautiful rendering available completely for free.\"]]],[1,\"p\",[]],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>Gaming technology and the technology that supports it has come a long way. With more powerful GPUs in our devices than ever, revolutions in chip manufacturing, and efficiencies in calculations allow us to do incredible things like run ray tracing engines in real time, simulate realistic physics, and so much more. All of this work has huge impact on gaming and immersive experience, but outside of creative industries what can we do?</p><p>I've done a lot of work and learning around the metaverse. As part of this learning I've made worlds, designed objects, and learnt a lot about 3D rendering. One thing I did to support some of this research was to 3D model my own building for some experimentation around MetaHumans - more on that soon. Once I had a render of my bedroom behind my workstation, I uploaded it as a Teams camera background. After talking with a few people, I realised not everyone cottoned on that it was even a digital render (the real thing is much messier normally).</p><h2 id=\"this-gave-me-an-idea\">This gave me an idea.</h2><p>If people can't really tell what's real once the stream has been compressed and sent over the internet, could a machine vision model? I've always been interested in synthetic data creation for machine vision, but usually my starting point has been something real, like a photograph. What if we could make a working machine learning model with no real data at all?</p><p>The use case I chose was to find a tennis ball. Why? Well, it's about the size that our stretch RE1 robot can grab well, and it won't matter if it's dropped or bumps into anything, so, I was hoping the resulting model could be used to help the robot find and grab the tennis ball in the room. That's a challenge for another day though.</p><p>I started off with the 3D model of my room, and settled on the view from behind my workstation.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/12/Ball.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/12/Ball.png 600w, __GHOST_URL__/content/images/size/w1000/2022/12/Ball.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/12/Ball.png 1600w, __GHOST_URL__/content/images/2022/12/Ball.png 1920w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Then dropped a tennis ball onto the bed. (virtually). I wrote a script that can move the ball around the room, change the lighting conditions, and then automatically tag the ball location and upload it straight to Azure custom vision.</p><div class=\"kg-card kg-button-card kg-align-center\"><a href=\"https://github.com/FergusKidd/Blender-to-Azure-custom-vision\" class=\"kg-btn kg-btn-accent\">Visit the GiHub repo</a></div><p>This uses the cycles engine, with a de-noised image generation capped at about 30 seconds on my machine. The beauty of this is you can easily change or limit the quality output, or scale your compute power to generate many thousands of quality pre-tagged images much faster than you could ever photograph a real scene and manually tag it, assuming you don't have to build an environment from scratch like I did here.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/12/Screenshot-2022-12-16-at-10.22.42-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"993\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/12/Screenshot-2022-12-16-at-10.22.42-1.png 600w, __GHOST_URL__/content/images/size/w1000/2022/12/Screenshot-2022-12-16-at-10.22.42-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/12/Screenshot-2022-12-16-at-10.22.42-1.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/12/Screenshot-2022-12-16-at-10.22.42-1.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Playing around with the lighting conditions allows you to simulate lots of different conditions, which from my past research, really help to build a more resilient object detection model.</p><p>All that's left to do is to click train, and test the results!</p><h2 id=\"the-real-thing\">The real thing</h2><p>I used the minimum number of images required by Azure custom vision, just to push it to the max, but in reality with this method, you can keep adding camera views, lighting setups and noising to you heart's content.</p><h3 id=\"first-lets-test-a-real-image-from-a-similar-viewpoint\">First, let's test a real image from a similar viewpoint:</h3><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/12/Screenshot-2022-12-16-at-10.48.51.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1190\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/12/Screenshot-2022-12-16-at-10.48.51.png 600w, __GHOST_URL__/content/images/size/w1000/2022/12/Screenshot-2022-12-16-at-10.48.51.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/12/Screenshot-2022-12-16-at-10.48.51.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/12/Screenshot-2022-12-16-at-10.48.51.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>There we have it. The model has never seen a real tennis ball, but can identify the real one with 93.1% confidence, even with the lowest amount of synthetic data possibly generated.</p><h3 id=\"what-if-we-show-it-an-image-it-wasnt-trained-on-with-more-objects-than-expected\">What if we show it an image it wasn't trained on, with more objects than expected?</h3><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/12/Screenshot-2022-12-16-at-10.51.49.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1198\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/12/Screenshot-2022-12-16-at-10.51.49.png 600w, __GHOST_URL__/content/images/size/w1000/2022/12/Screenshot-2022-12-16-at-10.51.49.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/12/Screenshot-2022-12-16-at-10.51.49.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/12/Screenshot-2022-12-16-at-10.51.49.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Well, we had to reduce the confidence limit, but we have still correctly identified all the tennis balls, even one from the reflection in the mirror.</p><h3 id=\"next-the-real-thing-from-a-different-viewpoint\">Next, the real thing from a different viewpoint:</h3><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/12/Screenshot-2022-12-16-at-10.55.35.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1201\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/12/Screenshot-2022-12-16-at-10.55.35.png 600w, __GHOST_URL__/content/images/size/w1000/2022/12/Screenshot-2022-12-16-at-10.55.35.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/12/Screenshot-2022-12-16-at-10.55.35.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/12/Screenshot-2022-12-16-at-10.55.35.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Not the highest confidences again, but still correct guesses - so this approach is still useable for a first pass where the context of the images meeds to change.</p><h3 id=\"finally-the-same-object-but-in-a-totally-different-context\">Finally, the same object but in a totally different context:</h3><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/12/Screenshot-2022-12-16-at-11.00.16.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1203\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/12/Screenshot-2022-12-16-at-11.00.16.png 600w, __GHOST_URL__/content/images/size/w1000/2022/12/Screenshot-2022-12-16-at-11.00.16.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/12/Screenshot-2022-12-16-at-11.00.16.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/12/Screenshot-2022-12-16-at-11.00.16.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>So, in a totally different context, it still works!</p><p>Imagine what we could do with many more renders, with no meed for any manual tagging, and looking at as many objects as we liked. There are great things going on in this sector from the likes of Unity as well, but for me, Blender gives the greatest flexibility on 3D rendering, with beautiful rendering available completely for free.</p><p></p>","comment_id":"639c46c456bf880001dc451e","plaintext":"Gaming technology and the technology that supports it has come a long way. With\nmore powerful GPUs in our devices than ever, revolutions in chip manufacturing,\nand efficiencies in calculations allow us to do incredible things like run ray\ntracing engines in real time, simulate realistic physics, and so much more. All\nof this work has huge impact on gaming and immersive experience, but outside of\ncreative industries what can we do?\n\nI've done a lot of work and learning around the metaverse. As part of this\nlearning I've made worlds, designed objects, and learnt a lot about 3D\nrendering. One thing I did to support some of this research was to 3D model my\nown building for some experimentation around MetaHumans - more on that soon.\nOnce I had a render of my bedroom behind my workstation, I uploaded it as a\nTeams camera background. After talking with a few people, I realised not\neveryone cottoned on that it was even a digital render (the real thing is much\nmessier normally).\n\nThis gave me an idea.\nIf people can't really tell what's real once the stream has been compressed and\nsent over the internet, could a machine vision model? I've always been\ninterested in synthetic data creation for machine vision, but usually my\nstarting point has been something real, like a photograph. What if we could make\na working machine learning model with no real data at all?\n\nThe use case I chose was to find a tennis ball. Why? Well, it's about the size\nthat our stretch RE1 robot can grab well, and it won't matter if it's dropped or\nbumps into anything, so, I was hoping the resulting model could be used to help\nthe robot find and grab the tennis ball in the room. That's a challenge for\nanother day though.\n\nI started off with the 3D model of my room, and settled on the view from behind\nmy workstation.\n\nThen dropped a tennis ball onto the bed. (virtually). I wrote a script that can\nmove the ball around the room, change the lighting conditions, and then\nautomatically tag the ball location and upload it straight to Azure custom\nvision.\n\nVisit the GiHub repo\n[https://github.com/FergusKidd/Blender-to-Azure-custom-vision]This uses the\ncycles engine, with a de-noised image generation capped at about 30 seconds on\nmy machine. The beauty of this is you can easily change or limit the quality\noutput, or scale your compute power to generate many thousands of quality\npre-tagged images much faster than you could ever photograph a real scene and\nmanually tag it, assuming you don't have to build an environment from scratch\nlike I did here.\n\nPlaying around with the lighting conditions allows you to simulate lots of\ndifferent conditions, which from my past research, really help to build a more\nresilient object detection model.\n\nAll that's left to do is to click train, and test the results!\n\nThe real thing\nI used the minimum number of images required by Azure custom vision, just to\npush it to the max, but in reality with this method, you can keep adding camera\nviews, lighting setups and noising to you heart's content.\n\nFirst, let's test a real image from a similar viewpoint:\nThere we have it. The model has never seen a real tennis ball, but can identify\nthe real one with 93.1% confidence, even with the lowest amount of synthetic\ndata possibly generated.\n\nWhat if we show it an image it wasn't trained on, with more objects than\nexpected?\nWell, we had to reduce the confidence limit, but we have still correctly\nidentified all the tennis balls, even one from the reflection in the mirror.\n\nNext, the real thing from a different viewpoint:\nNot the highest confidences again, but still correct guesses - so this approach\nis still useable for a first pass where the context of the images meeds to\nchange.\n\nFinally, the same object but in a totally different context:\nSo, in a totally different context, it still works!\n\nImagine what we could do with many more renders, with no meed for any manual\ntagging, and looking at as many objects as we liked. There are great things\ngoing on in this sector from the likes of Unity as well, but for me, Blender\ngives the greatest flexibility on 3D rendering, with beautiful rendering\navailable completely for free.","feature_image":"__GHOST_URL__/content/images/2022/12/Screenshot-2022-12-16-at-10.22.42.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-12-16T10:21:56.000Z","updated_at":"2022-12-16T11:02:34.000Z","published_at":"2022-12-16T11:02:35.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"63c1772dcf151900013c1ef2","uuid":"93b008e6-b69f-4695-9d0a-e2414417021a","title":"Generated Textures, but with depth","slug":"generated-textures-but-with-depth","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"button\",{\"alignment\":\"center\",\"buttonText\":\"Their GitHub\",\"buttonUrl\":\"https://github.com/carson-katri/dream-textures\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/01/Cereal.png\",\"width\":1920,\"height\":1080}],[\"video\",{\"loop\":true,\"src\":\"__GHOST_URL__/content/media/2023/01/CerealExample.mp4\",\"fileName\":\"CerealExample.mp4\",\"width\":1920,\"height\":1080,\"duration\":17.650967,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2023/01/media-thumbnail-ember447.jpg\",\"thumbnailWidth\":1920,\"thumbnailHeight\":1080,\"cardWidth\":null}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"One very cool open source project has taken Stable diffusion into Blender. This means you can generate AI art, whether it be concept art, or a texture, right where you need it.\"]]],[10,0],[1,\"p\",[[0,[],0,\"One epic feature of this project that was just released though is a new model that can deal with the depth it receives from Blender. So not only does it generate something new, it can also understand your model and wrap the texture around it. It seems to do this by asking the model for a mew flat image, knowing what surfaces it meeds to wrap. It makes for a nice addition, but can drastically reduce the quality of the image if you aren't zoomed in on your Blender viewport.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here I've asked for a simple texture of a cereal box on a simple mesh. The output is obviously not perfect, generative AI has never yet been great at rendering text nicely, but it's very clearly a cereal box that we have rendered out. \"]]],[1,\"p\",[[0,[],0,\"As a distant background object this would be more than enough to quickly fill out a scene, or make interesting new models on the fly. It can use cloud compute if you have a Dream Studio, or locally on your own GPU.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Here it is in action, going from a blank cuboid to a fully textured cereal box!\"]]],[1,\"p\",[[0,[],0,\"Blender is using my Nvidia 3080 to render this nice and quickly locally. The nice addition here is that makes it completely free! \"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>One very cool open source project has taken Stable diffusion into Blender. This means you can generate AI art, whether it be concept art, or a texture, right where you need it.</p><div class=\"kg-card kg-button-card kg-align-center\"><a href=\"https://github.com/carson-katri/dream-textures\" class=\"kg-btn kg-btn-accent\">Their GitHub</a></div><p>One epic feature of this project that was just released though is a new model that can deal with the depth it receives from Blender. So not only does it generate something new, it can also understand your model and wrap the texture around it. It seems to do this by asking the model for a mew flat image, knowing what surfaces it meeds to wrap. It makes for a nice addition, but can drastically reduce the quality of the image if you aren't zoomed in on your Blender viewport.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/01/Cereal.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1920\" height=\"1080\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/01/Cereal.png 600w, __GHOST_URL__/content/images/size/w1000/2023/01/Cereal.png 1000w, __GHOST_URL__/content/images/size/w1600/2023/01/Cereal.png 1600w, __GHOST_URL__/content/images/2023/01/Cereal.png 1920w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Here I've asked for a simple texture of a cereal box on a simple mesh. The output is obviously not perfect, generative AI has never yet been great at rendering text nicely, but it's very clearly a cereal box that we have rendered out. </p><p>As a distant background object this would be more than enough to quickly fill out a scene, or make interesting new models on the fly. It can use cloud compute if you have a Dream Studio, or locally on your own GPU.</p><figure class=\"kg-card kg-video-card\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2023/01/CerealExample.mp4\" poster=\"https://img.spacergif.org/v1/1920x1080/0a/spacer.png\" width=\"1920\" height=\"1080\" loop autoplay muted playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2023/01/media-thumbnail-ember447.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container kg-video-hide\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div></figure><p>Here it is in action, going from a blank cuboid to a fully textured cereal box!</p><p>Blender is using my Nvidia 3080 to render this nice and quickly locally. The nice addition here is that makes it completely free! </p>","comment_id":"63c1772dcf151900013c1ef2","plaintext":"One very cool open source project has taken Stable diffusion into Blender. This\nmeans you can generate AI art, whether it be concept art, or a texture, right\nwhere you need it.\n\nTheir GitHub [https://github.com/carson-katri/dream-textures]One epic feature of\nthis project that was just released though is a new model that can deal with the\ndepth it receives from Blender. So not only does it generate something new, it\ncan also understand your model and wrap the texture around it. It seems to do\nthis by asking the model for a mew flat image, knowing what surfaces it meeds to\nwrap. It makes for a nice addition, but can drastically reduce the quality of\nthe image if you aren't zoomed in on your Blender viewport.\n\nHere I've asked for a simple texture of a cereal box on a simple mesh. The\noutput is obviously not perfect, generative AI has never yet been great at\nrendering text nicely, but it's very clearly a cereal box that we have rendered\nout. \n\nAs a distant background object this would be more than enough to quickly fill\nout a scene, or make interesting new models on the fly. It can use cloud compute\nif you have a Dream Studio, or locally on your own GPU.\n\n0:00/1Ã—Here it is in action, going from a blank cuboid to a fully textured\ncereal box!\n\nBlender is using my Nvidia 3080 to render this nice and quickly locally. The\nnice addition here is that makes it completely free!","feature_image":"__GHOST_URL__/content/images/2023/01/Dream-Textures.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2023-01-13T15:22:21.000Z","updated_at":"2023-01-13T15:58:14.000Z","published_at":"2023-01-13T15:54:28.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"6408a781cf151900013c1f33","uuid":"0bafb45b-171e-4c50-b03d-e79a7a2d1675","title":"Voice enabled Chat GPT","slug":"voice-enabled-chat-gpt","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"button\",{\"alignment\":\"center\",\"buttonText\":\"GitHub\",\"buttonUrl\":\"https://github.com/FergusKidd/Speech-and-Chat-GPT\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/03/IMG_02CE018B7DAA-1.jpeg\",\"width\":2098,\"height\":4032}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"OpenAI released it's model and API that powers the product we see when we use Chat GPT. This means we can start to implement our own solutions around the function by adding it into our custom apps or services. One thing I wanted to do right away was to get a version working for robots. \"]]],[1,\"p\",[[0,[],0,\"That's right, a version that will work with voice recognition and voice synthesis. Check it out on GitHub - it's a basic implementation for now.\"]]],[10,0],[1,\"p\",[[0,[],0,\"I use the voice services from Microsoft Azure to take input from the device's microphone (could be your laptop, could be a robot) and send it off to OpenAI's  GPT 3.5 model for chat. The nice thing about this is that you can include the messaging history to allow for a more natural conversation, as OpenAI can \\\"remember\\\" what you have been talking about. It should work mostly the same, however there are obvious limitations like code generation. You wouldn't want anyone to dictate code to you for example....\"],[1,[],0,0],[1,[],0,1],[0,[],0,\"Anyway, I hope this will be useful to someone learning about GPT and speech, and I'll enable it on our robot, Rory, soon, to make 'him' even more knowledgeble than before.\"]]],[10,1],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>OpenAI released it's model and API that powers the product we see when we use Chat GPT. This means we can start to implement our own solutions around the function by adding it into our custom apps or services. One thing I wanted to do right away was to get a version working for robots. </p><p>That's right, a version that will work with voice recognition and voice synthesis. Check it out on GitHub - it's a basic implementation for now.</p><div class=\"kg-card kg-button-card kg-align-center\"><a href=\"https://github.com/FergusKidd/Speech-and-Chat-GPT\" class=\"kg-btn kg-btn-accent\">GitHub</a></div><p>I use the voice services from Microsoft Azure to take input from the device's microphone (could be your laptop, could be a robot) and send it off to OpenAI's Â GPT 3.5 model for chat. The nice thing about this is that you can include the messaging history to allow for a more natural conversation, as OpenAI can \"remember\" what you have been talking about. It should work mostly the same, however there are obvious limitations like code generation. You wouldn't want anyone to dictate code to you for example....<br><br>Anyway, I hope this will be useful to someone learning about GPT and speech, and I'll enable it on our robot, Rory, soon, to make 'him' even more knowledgeble than before.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/03/IMG_02CE018B7DAA-1.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"3844\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/03/IMG_02CE018B7DAA-1.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2023/03/IMG_02CE018B7DAA-1.jpeg 1000w, __GHOST_URL__/content/images/size/w1600/2023/03/IMG_02CE018B7DAA-1.jpeg 1600w, __GHOST_URL__/content/images/2023/03/IMG_02CE018B7DAA-1.jpeg 2098w\" sizes=\"(min-width: 720px) 720px\"></figure>","comment_id":"6408a781cf151900013c1f33","plaintext":"OpenAI released it's model and API that powers the product we see when we use\nChat GPT. This means we can start to implement our own solutions around the\nfunction by adding it into our custom apps or services. One thing I wanted to do\nright away was to get a version working for robots. \n\nThat's right, a version that will work with voice recognition and voice\nsynthesis. Check it out on GitHub - it's a basic implementation for now.\n\nGitHub [https://github.com/FergusKidd/Speech-and-Chat-GPT]I use the voice\nservices from Microsoft Azure to take input from the device's microphone (could\nbe your laptop, could be a robot) and send it off to OpenAI's Â GPT 3.5 model for\nchat. The nice thing about this is that you can include the messaging history to\nallow for a more natural conversation, as OpenAI can \"remember\" what you have\nbeen talking about. It should work mostly the same, however there are obvious\nlimitations like code generation. You wouldn't want anyone to dictate code to\nyou for example....\n\nAnyway, I hope this will be useful to someone learning about GPT and speech, and\nI'll enable it on our robot, Rory, soon, to make 'him' even more knowledgeble\nthan before.","feature_image":"__GHOST_URL__/content/images/2023/03/Default_a_very_cool_slender_robot_that_is_intelligent_in_a_sci_fi_set_1_ef10e3db-379d-4178-8e1d-68234ff54a48_1.jpg","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2023-03-08T15:19:29.000Z","updated_at":"2023-03-08T15:27:30.000Z","published_at":"2023-03-08T15:27:30.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"6419a314cf151900013c1f60","uuid":"9d4987dc-1fe8-4f23-bf75-f97b777d8992","title":"Me","slug":"me","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\" <!DOCTYPE html>\\n    <html>\\n        <head></head>\\n        <body>\\n            <iframe src=\\\"https://google.com\\\" width=\\\"100%\\\" height=\\\"400\\\" />\\n        </body>\\n    </html>\\n\"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<!--kg-card-begin: html--> <!DOCTYPE html>\n    <html>\n        <head></head>\n        <body>\n            <iframe src=\"https://google.com\" width=\"100%\" height=\"400\" />\n        </body>\n    </html>\n<!--kg-card-end: html-->","comment_id":"6419a314cf151900013c1f60","plaintext":null,"feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2023-03-21T12:29:08.000Z","updated_at":"2023-03-21T12:52:16.000Z","published_at":"2023-03-21T12:51:03.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"64884ff86f01a50001988206","uuid":"f882ad28-9f32-49c1-bb08-2cf9851e0d82","title":"A Robotic Wonderland: ICRA 2023","slug":"a-robotic-wonderland-icra-2023","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/06/IMG_0544.jpg\",\"width\":1284,\"height\":1247}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/06/IMG_0543-1.jpg\",\"width\":1284,\"height\":2256}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/06/IMG_0545.jpg\",\"width\":1201,\"height\":1268}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"As a self-proclaimed robotics enthusiast, I could not have been more thrilled to attend and present a late breaking poster at the International Conference on Robotics and Automation (ICRA) 2023. The event exceeded my expectations, with fascinating keynotes, talks, and demonstrations that showcased the cutting edge of robotics technology.\"]]],[1,\"p\",[[0,[],0,\"One of the most striking aspects of ICRA 2023 was the sheer number of robotic dogs running around the conference venue! It seemed like every other exhibitor had their own version of a robotic canine companion, each with unique features and abilities and custom sensor set. Some were designed for search and rescue missions, while others were more focused on indoor mapping with LIDAR. Regardless of their purpose, seeing so many robot dogs in one place was an unforgettable experience.\"]]],[10,0],[1,\"p\",[[1,[],0,0],[0,[],0,\"However, the most intriguing encounters at the conference were the human-robot interactions. As I roamed the exhibition hall, I stumbled upon a stall showcasing a humanoid robot designed to mimic human expressions and emotions from Ameca. \"]]],[10,1],[1,\"p\",[[1,[],0,1],[0,[],0,\"It was a fascinating and, admittedly, somewhat creepy experience to witness this human-like robot move so naturally with expression. It was a stark reminder of the rapid advancements being made in the field of robotics and how far we can come when we link this with technologies in like voice fonts and generative AI.\"],[1,[],0,2],[1,[],0,3],[0,[],0,\"Another highlight of ICRA 2023 was the incredible array of keynote speakers and talks. Some of the most influential figures in the robotics and automation industry shared their insights on the current state of the field, as well as their predictions for the future. Topics ranged from ethical considerations in agricultural robotics to the latest developments in machine learning and computer vision.\"],[1,[],0,4],[1,[],0,5],[0,[],0,\"A surprise at ICRA 2023 was the sheer number of people showcasing simulation technologies. There was a notable emphasis on using virtual environments to train and test robotic systems, with several exhibitors demonstrating their simulation platforms. These tools are becoming increasingly important as the complexity and capabilities of robots continue to grow, allowing developers to refine their creations in a controlled environment before deploying them in the real world. Some of these had fantastic graphical layers that can be used for training computer vision models with synthetic data.\"],[1,[],0,6],[1,[],0,7],[0,[],0,\"ICRA 2023 was a robotic wonderland that left me in awe of the innovations and advancements in the field of robotics and automation. From the charming robot dogs to the thought-provoking keynotes and talks, the conference was a testament to the exciting future that lies ahead in this rapidly evolving industry. I can't wait to see what ICRA 2024 has in store in Japan!\"]]],[10,2],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>As a self-proclaimed robotics enthusiast, I could not have been more thrilled to attend and present a late breaking poster at the International Conference on Robotics and Automation (ICRA) 2023. The event exceeded my expectations, with fascinating keynotes, talks, and demonstrations that showcased the cutting edge of robotics technology.</p><p>One of the most striking aspects of ICRA 2023 was the sheer number of robotic dogs running around the conference venue! It seemed like every other exhibitor had their own version of a robotic canine companion, each with unique features and abilities and custom sensor set. Some were designed for search and rescue missions, while others were more focused on indoor mapping with LIDAR. Regardless of their purpose, seeing so many robot dogs in one place was an unforgettable experience.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/06/IMG_0544.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"1284\" height=\"1247\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/06/IMG_0544.jpg 600w, __GHOST_URL__/content/images/size/w1000/2023/06/IMG_0544.jpg 1000w, __GHOST_URL__/content/images/2023/06/IMG_0544.jpg 1284w\" sizes=\"(min-width: 720px) 720px\"></figure><p><br>However, the most intriguing encounters at the conference were the human-robot interactions. As I roamed the exhibition hall, I stumbled upon a stall showcasing a humanoid robot designed to mimic human expressions and emotions from Ameca. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/06/IMG_0543-1.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"1284\" height=\"2256\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/06/IMG_0543-1.jpg 600w, __GHOST_URL__/content/images/size/w1000/2023/06/IMG_0543-1.jpg 1000w, __GHOST_URL__/content/images/2023/06/IMG_0543-1.jpg 1284w\" sizes=\"(min-width: 720px) 720px\"></figure><p><br>It was a fascinating and, admittedly, somewhat creepy experience to witness this human-like robot move so naturally with expression. It was a stark reminder of the rapid advancements being made in the field of robotics and how far we can come when we link this with technologies in like voice fonts and generative AI.<br><br>Another highlight of ICRA 2023 was the incredible array of keynote speakers and talks. Some of the most influential figures in the robotics and automation industry shared their insights on the current state of the field, as well as their predictions for the future. Topics ranged from ethical considerations in agricultural robotics to the latest developments in machine learning and computer vision.<br><br>A surprise at ICRA 2023 was the sheer number of people showcasing simulation technologies. There was a notable emphasis on using virtual environments to train and test robotic systems, with several exhibitors demonstrating their simulation platforms. These tools are becoming increasingly important as the complexity and capabilities of robots continue to grow, allowing developers to refine their creations in a controlled environment before deploying them in the real world. Some of these had fantastic graphical layers that can be used for training computer vision models with synthetic data.<br><br>ICRA 2023 was a robotic wonderland that left me in awe of the innovations and advancements in the field of robotics and automation. From the charming robot dogs to the thought-provoking keynotes and talks, the conference was a testament to the exciting future that lies ahead in this rapidly evolving industry. I can't wait to see what ICRA 2024 has in store in Japan!</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/06/IMG_0545.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"1201\" height=\"1268\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/06/IMG_0545.jpg 600w, __GHOST_URL__/content/images/size/w1000/2023/06/IMG_0545.jpg 1000w, __GHOST_URL__/content/images/2023/06/IMG_0545.jpg 1201w\" sizes=\"(min-width: 720px) 720px\"></figure>","comment_id":"64884ff86f01a50001988206","plaintext":"As a self-proclaimed robotics enthusiast, I could not have been more thrilled to\nattend and present a late breaking poster at the International Conference on\nRobotics and Automation (ICRA) 2023. The event exceeded my expectations, with\nfascinating keynotes, talks, and demonstrations that showcased the cutting edge\nof robotics technology.\n\nOne of the most striking aspects of ICRA 2023 was the sheer number of robotic\ndogs running around the conference venue! It seemed like every other exhibitor\nhad their own version of a robotic canine companion, each with unique features\nand abilities and custom sensor set. Some were designed for search and rescue\nmissions, while others were more focused on indoor mapping with LIDAR.\nRegardless of their purpose, seeing so many robot dogs in one place was an\nunforgettable experience.\n\n\nHowever, the most intriguing encounters at the conference were the human-robot\ninteractions. As I roamed the exhibition hall, I stumbled upon a stall\nshowcasing a humanoid robot designed to mimic human expressions and emotions\nfrom Ameca. \n\n\nIt was a fascinating and, admittedly, somewhat creepy experience to witness this\nhuman-like robot move so naturally with expression. It was a stark reminder of\nthe rapid advancements being made in the field of robotics and how far we can\ncome when we link this with technologies in like voice fonts and generative AI.\n\nAnother highlight of ICRA 2023 was the incredible array of keynote speakers and\ntalks. Some of the most influential figures in the robotics and automation\nindustry shared their insights on the current state of the field, as well as\ntheir predictions for the future. Topics ranged from ethical considerations in\nagricultural robotics to the latest developments in machine learning and\ncomputer vision.\n\nA surprise at ICRA 2023 was the sheer number of people showcasing simulation\ntechnologies. There was a notable emphasis on using virtual environments to\ntrain and test robotic systems, with several exhibitors demonstrating their\nsimulation platforms. These tools are becoming increasingly important as the\ncomplexity and capabilities of robots continue to grow, allowing developers to\nrefine their creations in a controlled environment before deploying them in the\nreal world. Some of these had fantastic graphical layers that can be used for\ntraining computer vision models with synthetic data.\n\nICRA 2023 was a robotic wonderland that left me in awe of the innovations and\nadvancements in the field of robotics and automation. From the charming robot\ndogs to the thought-provoking keynotes and talks, the conference was a testament\nto the exciting future that lies ahead in this rapidly evolving industry. I\ncan't wait to see what ICRA 2024 has in store in Japan!","feature_image":"__GHOST_URL__/content/images/2023/06/IMG_0546.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2023-06-13T11:16:08.000Z","updated_at":"2023-06-13T11:25:00.000Z","published_at":"2023-06-13T11:25:01.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"648852936f01a5000198823d","uuid":"267d108a-04f6-4b49-b62f-88ae0f2a3b8a","title":"Voice Font - Now With More Neural","slug":"voice-font-now-with-more-neural","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"audio\",{\"loop\":false,\"src\":\"__GHOST_URL__/content/media/2023/06/audio-file.mp3\",\"title\":\"Audio file\",\"duration\":28.272,\"mimeType\":\"audio/mpeg\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"One exciting thing I worked on very early on was creating my own custom voice font with Microsoft's custom voice font to enable text to speech. In the years since we have seen some amazing advancements in neural text to speech which give a much better output.\"],[1,[],0,0],[1,[],0,1],[0,[],0,\"When I found out neural custom voices were available I couldn't resist trying one out. I was eager to see how accurate and natural the results would be compared to my previous attempts, which although recognisable, were not quite there.\"],[1,[],0,2],[1,[],0,3],[0,[],0,\"For those who are unfamiliar, Microsoft Neural TTS is a cloud-based service that aims to provide highly realistic and expressive speech synthesis using deep neural networks. Voice Fonts, on the other hand, allow users to create custom voices by training the TTS engine on their own voice recordings.\"],[1,[],0,4],[1,[],0,5],[0,[],0,\"To begin my experiment, I signed up for the Microsoft Azure Cognitive Services and followed the provided guidelines for recording my 50 utterances. The process was relatively straightforward, with clear instructions on how to maintain consistent pitch, tone, and pacing throughout the recordings. These are recorded right in the portal, which is super easy. I did use a much better quality microphone than my attempts several years ago. The training is pretty quick, but to use the font you need to record a short statement that you agree to have your voice replicated. \"],[1,[],0,6],[0,[],0,\"The moment of truth arrived as I typed a sample text into the Neural TTS interface and selected my custom voice. Listen to the results yourself:\"]]],[1,\"h3\",[[1,[],0,7],[1,[],0,8],[0,[],0,\"An excerpt from Mary Shelley's Frankenstein\"]]],[10,0],[1,\"p\",[[1,[],0,9],[0,[],0,\"This quick experiment has left me eager to explore further and see how much more realistic the Voice Font can become with additional recordings on the professional version, although up to 1000 are required. I'm also looking forward to seeing how Microsoft and other companies continue to push the boundaries of TTS technology in the coming years. It looks like there will be a version native in the next apple iOS iteration later this year. The future of personalized, expressive speech synthesis is here, and it'll be interesting to see exactly how it gets used... For good and bad...\"]]],[1,\"p\",[[1,[],0,10]]]],\"ghostVersion\":\"4.0\"}","html":"<p>One exciting thing I worked on very early on was creating my own custom voice font with Microsoft's custom voice font to enable text to speech. In the years since we have seen some amazing advancements in neural text to speech which give a much better output.<br><br>When I found out neural custom voices were available I couldn't resist trying one out. I was eager to see how accurate and natural the results would be compared to my previous attempts, which although recognisable, were not quite there.<br><br>For those who are unfamiliar, Microsoft Neural TTS is a cloud-based service that aims to provide highly realistic and expressive speech synthesis using deep neural networks. Voice Fonts, on the other hand, allow users to create custom voices by training the TTS engine on their own voice recordings.<br><br>To begin my experiment, I signed up for the Microsoft Azure Cognitive Services and followed the provided guidelines for recording my 50 utterances. The process was relatively straightforward, with clear instructions on how to maintain consistent pitch, tone, and pacing throughout the recordings. These are recorded right in the portal, which is super easy. I did use a much better quality microphone than my attempts several years ago. The training is pretty quick, but to use the font you need to record a short statement that you agree to have your voice replicated. <br>The moment of truth arrived as I typed a sample text into the Neural TTS interface and selected my custom voice. Listen to the results yourself:</p><h3 id=\"an-excerpt-from-mary-shelleys-frankenstein\"><br><br>An excerpt from Mary Shelley's Frankenstein</h3><div class=\"kg-card kg-audio-card\"><img src=\"\" alt=\"audio-thumbnail\" class=\"kg-audio-thumbnail kg-audio-hide\"><div class=\"kg-audio-thumbnail placeholder\"><svg width=\"24\" height=\"24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M7.5 15.33a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0ZM15 13.83a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm-2.25.75a2.25 2.25 0 1 1 4.5 0 2.25 2.25 0 0 1-4.5 0Z\"/><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.486 6.81A2.25 2.25 0 0 1 17.25 9v5.579a.75.75 0 0 1-1.5 0v-5.58a.75.75 0 0 0-.932-.727.755.755 0 0 1-.059.013l-4.465.744a.75.75 0 0 0-.544.72v6.33a.75.75 0 0 1-1.5 0v-6.33a2.25 2.25 0 0 1 1.763-2.194l4.473-.746Z\"/><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3 1.5a.75.75 0 0 0-.75.75v19.5a.75.75 0 0 0 .75.75h18a.75.75 0 0 0 .75-.75V5.133a.75.75 0 0 0-.225-.535l-.002-.002-3-2.883A.75.75 0 0 0 18 1.5H3ZM1.409.659A2.25 2.25 0 0 1 3 0h15a2.25 2.25 0 0 1 1.568.637l.003.002 3 2.883a2.25 2.25 0 0 1 .679 1.61V21.75A2.25 2.25 0 0 1 21 24H3a2.25 2.25 0 0 1-2.25-2.25V2.25c0-.597.237-1.169.659-1.591Z\"/></svg></div><div class=\"kg-audio-player-container\"><audio src=\"__GHOST_URL__/content/media/2023/06/audio-file.mp3\" preload=\"metadata\"></audio><div class=\"kg-audio-title\">Audio file</div><div class=\"kg-audio-player\"><button class=\"kg-audio-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-audio-pause-icon kg-audio-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-audio-current-time\">0:00</span><div class=\"kg-audio-time\">/<span class=\"kg-audio-duration\">0:28</span></div><input type=\"range\" class=\"kg-audio-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-audio-playback-rate\">1&#215;</button><button class=\"kg-audio-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-audio-mute-icon kg-audio-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-audio-volume-slider\" max=\"100\" value=\"100\"></div></div></div><p><br>This quick experiment has left me eager to explore further and see how much more realistic the Voice Font can become with additional recordings on the professional version, although up to 1000 are required. I'm also looking forward to seeing how Microsoft and other companies continue to push the boundaries of TTS technology in the coming years. It looks like there will be a version native in the next apple iOS iteration later this year. The future of personalized, expressive speech synthesis is here, and it'll be interesting to see exactly how it gets used... For good and bad...</p>","comment_id":"648852936f01a5000198823d","plaintext":"One exciting thing I worked on very early on was creating my own custom voice\nfont with Microsoft's custom voice font to enable text to speech. In the years\nsince we have seen some amazing advancements in neural text to speech which give\na much better output.\n\nWhen I found out neural custom voices were available I couldn't resist trying\none out. I was eager to see how accurate and natural the results would be\ncompared to my previous attempts, which although recognisable, were not quite\nthere.\n\nFor those who are unfamiliar, Microsoft Neural TTS is a cloud-based service that\naims to provide highly realistic and expressive speech synthesis using deep\nneural networks. Voice Fonts, on the other hand, allow users to create custom\nvoices by training the TTS engine on their own voice recordings.\n\nTo begin my experiment, I signed up for the Microsoft Azure Cognitive Services\nand followed the provided guidelines for recording my 50 utterances. The process\nwas relatively straightforward, with clear instructions on how to maintain\nconsistent pitch, tone, and pacing throughout the recordings. These are recorded\nright in the portal, which is super easy. I did use a much better quality\nmicrophone than my attempts several years ago. The training is pretty quick, but\nto use the font you need to record a short statement that you agree to have your\nvoice replicated. \nThe moment of truth arrived as I typed a sample text into the Neural TTS\ninterface and selected my custom voice. Listen to the results yourself:\n\n\n\nAn excerpt from Mary Shelley's Frankenstein\nAudio file0:00/0:281Ã—\nThis quick experiment has left me eager to explore further and see how much more\nrealistic the Voice Font can become with additional recordings on the\nprofessional version, although up to 1000 are required. I'm also looking forward\nto seeing how Microsoft and other companies continue to push the boundaries of\nTTS technology in the coming years. It looks like there will be a version native\nin the next apple iOS iteration later this year. The future of personalized,\nexpressive speech synthesis is here, and it'll be interesting to see exactly how\nit gets used... For good and bad...","feature_image":"__GHOST_URL__/content/images/2023/06/Screenshot-2023-06-13-at-12.34.49.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2023-06-13T11:27:15.000Z","updated_at":"2023-06-13T12:44:16.000Z","published_at":"2023-06-13T12:44:16.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"6540d51d3ffd0500016acfe1","uuid":"a1acf067-f8db-4c71-a55c-c1c638e7a61a","title":"Are you even real?","slug":"are-you-even-real","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"video\",{\"loop\":false,\"src\":\"__GHOST_URL__/content/media/2023/10/Untitled-Video--2-.mp4\",\"fileName\":\"Untitled Video (2).mp4\",\"width\":1920,\"height\":1080,\"duration\":18.2,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2023/10/media-thumbnail-ember115.jpg\",\"thumbnailWidth\":1920,\"thumbnailHeight\":1080}]],\"markups\":[[\"a\",[\"href\",\"https://app.heygen.com/\"]]],\"sections\":[[10,0],[1,\"p\",[[0,[],0,\"Make your own: \"],[0,[0],1,\"https://app.heygen.com/\"]]]],\"ghostVersion\":\"4.0\"}","html":"<figure class=\"kg-card kg-video-card\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2023/10/Untitled-Video--2-.mp4\" poster=\"https://img.spacergif.org/v1/1920x1080/0a/spacer.png\" width=\"1920\" height=\"1080\" playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2023/10/media-thumbnail-ember115.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div></figure><p>Make your own: <a href=\"https://app.heygen.com/\">https://app.heygen.com/</a></p>","comment_id":"6540d51d3ffd0500016acfe1","plaintext":"0:00/1Ã—Make your own: https://app.heygen.com/","feature_image":"__GHOST_URL__/content/images/2023/10/Screenshot-2023-10-31-at-10.21.06.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2023-10-31T10:21:17.000Z","updated_at":"2023-10-31T10:22:28.000Z","published_at":"2023-10-31T10:22:28.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"65b3b58d1304490001044394","uuid":"977b084a-35df-42bb-9946-978227681ca1","title":"I Interview Myself. Sort of.","slug":"i-interview-myself-sort-of","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"video\",{\"loop\":false,\"caption\":\"Fergus interviews AI Fergus\",\"src\":\"__GHOST_URL__/content/media/2024/01/Fergus-Interview.mp4\",\"fileName\":\"Fergus Interview.mp4\",\"width\":1920,\"height\":1080,\"duration\":204.92,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2024/01/media-thumbnail-ember100.jpg\",\"thumbnailWidth\":1920,\"thumbnailHeight\":1080}]],\"markups\":[[\"strong\"]],\"sections\":[[10,0],[1,\"p\",[[0,[0],1,\"An AI description of the video using GPT-4 Vision and Azure Vision AI:\"],[1,[],0,0],[0,[],0,\"In the ever-evolving landscape of artificial intelligence, the fusion of human interaction with AI personas has reached new heights. A remarkable example of this synergy is encapsulated in a video that showcases an intriguing interview between Fergus Kid, an R&D engineering lead at Avanade, and his AI counterpart. This innovative interaction not only highlights the capabilities of AI but also demonstrates the creative possibilities when leveraging Azure OpenAI's GPT-4 and the HeyGen avatar creation platform.\"],[1,[],0,1],[1,[],0,2],[0,[],0,\"The video, set against the backdrop of a cozy and well-lit living room, starts with Fergus leaning forward, his curiosity evident as he prepares to dive into a conversation with a digital version of himself. The scene seamlessly transitions to a more formal setting where Fergus's AI persona is comfortably seated on an orange couch, embodying a serene and professional ambiance within what appears to be a modern office space.\"],[1,[],0,3],[1,[],0,4],[0,[],0,\"The interview delves into the personal and professional aspects of Fergus's life, from his birth in Winchester to his current life in London, his hobbies like gaming and beekeeping, and his profound interest in space and AI. These details not only serve to humanise the AI but also ground the conversation in tangible reality.\"],[1,[],0,5],[1,[],0,6],[0,[],0,\"The technology underpinning this fascinating exchange is Azure OpenAI's GPT-4, a state-of-the-art machine learning platform that utilizes advanced natural language processing to craft responses that are remarkably human-like. GPT-4's capabilities allow the AI version of Fergus to engage in a fluid dialogue, discussing complex topics such as the ethical implications of AI and the potential benefits of AI personas for society.\"],[1,[],0,7],[1,[],0,8],[0,[],0,\"Complementing GPT-4's linguistic prowess is the HeyGen avatar creation tool, which has been instrumental in bringing the AI persona to life visually. The attention to detail in the avatar's creation is evident as it mirrors Fergus's movements and mannerisms, creating an immersive and convincing experience. The avatar's realistic presence on the orange couch, interacting with its environment in a natural way, underscores the potential of AI in humanizing technology.\"],[1,[],0,9],[1,[],0,10],[0,[],0,\"Throughout the video, the interplay of light and shadow, the strategic placement of indoor plants, and the modern decor contribute to a setting that is both inviting and forward-thinking, echoing the innovative spirit of the conversation. The shifts between the living room and office scenes provide a visual rhythm to the dialogue, maintaining viewer engagement.\"],[1,[],0,11],[1,[],0,12],[0,[],0,\"This video serves as a testament to the strides made in AI and avatar technology, specifically through the integration of Azure OpenAI's GPT-4 and HeyGen's avatar platform. It offers a glimpse into the future of human-AI interaction, where digital personas can not only replicate human conversation but also provide unique insights and augment human capabilities. As the AI Fergus aptly puts it, the key lies in approaching AI development responsibly, ensuring it paves the way for positive outcomes for humanity.\"],[1,[],0,13],[1,[],0,14],[0,[],0,\"In creating such a video, the creators have not only pushed the boundaries of what's possible with AI but have also presented a compelling narrative that encourages viewers to ponder the role of AI in our lives. It's a blend of technical marvel and creative storytelling that beckons us to consider the harmonious potential of humans and AI coexisting and collaborating.\"],[1,[],0,15],[1,[],0,16],[0,[],0,\"As we step into a future where AI becomes increasingly integrated into our daily lives, videos like these are pivotal in shaping our understanding and expectations of this technology. It's a conversation starter, an educational tool, and a peek into what might soon become a new normal in the interaction between humans and their AI counterparts.\"]]],[1,\"p\",[[1,[],0,17]]]],\"ghostVersion\":\"4.0\"}","html":"<figure class=\"kg-card kg-video-card kg-card-hascaption\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2024/01/Fergus-Interview.mp4\" poster=\"https://img.spacergif.org/v1/1920x1080/0a/spacer.png\" width=\"1920\" height=\"1080\" playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2024/01/media-thumbnail-ember100.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div><figcaption>Fergus interviews AI Fergus</figcaption></figure><p><strong>An AI description of the video using GPT-4 Vision and Azure Vision AI:</strong><br>In the ever-evolving landscape of artificial intelligence, the fusion of human interaction with AI personas has reached new heights. A remarkable example of this synergy is encapsulated in a video that showcases an intriguing interview between Fergus Kid, an R&amp;D engineering lead at Avanade, and his AI counterpart. This innovative interaction not only highlights the capabilities of AI but also demonstrates the creative possibilities when leveraging Azure OpenAI's GPT-4 and the HeyGen avatar creation platform.<br><br>The video, set against the backdrop of a cozy and well-lit living room, starts with Fergus leaning forward, his curiosity evident as he prepares to dive into a conversation with a digital version of himself. The scene seamlessly transitions to a more formal setting where Fergus's AI persona is comfortably seated on an orange couch, embodying a serene and professional ambiance within what appears to be a modern office space.<br><br>The interview delves into the personal and professional aspects of Fergus's life, from his birth in Winchester to his current life in London, his hobbies like gaming and beekeeping, and his profound interest in space and AI. These details not only serve to humanise the AI but also ground the conversation in tangible reality.<br><br>The technology underpinning this fascinating exchange is Azure OpenAI's GPT-4, a state-of-the-art machine learning platform that utilizes advanced natural language processing to craft responses that are remarkably human-like. GPT-4's capabilities allow the AI version of Fergus to engage in a fluid dialogue, discussing complex topics such as the ethical implications of AI and the potential benefits of AI personas for society.<br><br>Complementing GPT-4's linguistic prowess is the HeyGen avatar creation tool, which has been instrumental in bringing the AI persona to life visually. The attention to detail in the avatar's creation is evident as it mirrors Fergus's movements and mannerisms, creating an immersive and convincing experience. The avatar's realistic presence on the orange couch, interacting with its environment in a natural way, underscores the potential of AI in humanizing technology.<br><br>Throughout the video, the interplay of light and shadow, the strategic placement of indoor plants, and the modern decor contribute to a setting that is both inviting and forward-thinking, echoing the innovative spirit of the conversation. The shifts between the living room and office scenes provide a visual rhythm to the dialogue, maintaining viewer engagement.<br><br>This video serves as a testament to the strides made in AI and avatar technology, specifically through the integration of Azure OpenAI's GPT-4 and HeyGen's avatar platform. It offers a glimpse into the future of human-AI interaction, where digital personas can not only replicate human conversation but also provide unique insights and augment human capabilities. As the AI Fergus aptly puts it, the key lies in approaching AI development responsibly, ensuring it paves the way for positive outcomes for humanity.<br><br>In creating such a video, the creators have not only pushed the boundaries of what's possible with AI but have also presented a compelling narrative that encourages viewers to ponder the role of AI in our lives. It's a blend of technical marvel and creative storytelling that beckons us to consider the harmonious potential of humans and AI coexisting and collaborating.<br><br>As we step into a future where AI becomes increasingly integrated into our daily lives, videos like these are pivotal in shaping our understanding and expectations of this technology. It's a conversation starter, an educational tool, and a peek into what might soon become a new normal in the interaction between humans and their AI counterparts.</p>","comment_id":"65b3b58d1304490001044394","plaintext":"0:00/1Ã—Fergus interviews AI FergusAn AI description of the video using GPT-4\nVision and Azure Vision AI:\nIn the ever-evolving landscape of artificial intelligence, the fusion of human\ninteraction with AI personas has reached new heights. A remarkable example of\nthis synergy is encapsulated in a video that showcases an intriguing interview\nbetween Fergus Kid, an R&D engineering lead at Avanade, and his AI counterpart.\nThis innovative interaction not only highlights the capabilities of AI but also\ndemonstrates the creative possibilities when leveraging Azure OpenAI's GPT-4 and\nthe HeyGen avatar creation platform.\n\nThe video, set against the backdrop of a cozy and well-lit living room, starts\nwith Fergus leaning forward, his curiosity evident as he prepares to dive into a\nconversation with a digital version of himself. The scene seamlessly transitions\nto a more formal setting where Fergus's AI persona is comfortably seated on an\norange couch, embodying a serene and professional ambiance within what appears\nto be a modern office space.\n\nThe interview delves into the personal and professional aspects of Fergus's\nlife, from his birth in Winchester to his current life in London, his hobbies\nlike gaming and beekeeping, and his profound interest in space and AI. These\ndetails not only serve to humanise the AI but also ground the conversation in\ntangible reality.\n\nThe technology underpinning this fascinating exchange is Azure OpenAI's GPT-4, a\nstate-of-the-art machine learning platform that utilizes advanced natural\nlanguage processing to craft responses that are remarkably human-like. GPT-4's\ncapabilities allow the AI version of Fergus to engage in a fluid dialogue,\ndiscussing complex topics such as the ethical implications of AI and the\npotential benefits of AI personas for society.\n\nComplementing GPT-4's linguistic prowess is the HeyGen avatar creation tool,\nwhich has been instrumental in bringing the AI persona to life visually. The\nattention to detail in the avatar's creation is evident as it mirrors Fergus's\nmovements and mannerisms, creating an immersive and convincing experience. The\navatar's realistic presence on the orange couch, interacting with its\nenvironment in a natural way, underscores the potential of AI in humanizing\ntechnology.\n\nThroughout the video, the interplay of light and shadow, the strategic placement\nof indoor plants, and the modern decor contribute to a setting that is both\ninviting and forward-thinking, echoing the innovative spirit of the\nconversation. The shifts between the living room and office scenes provide a\nvisual rhythm to the dialogue, maintaining viewer engagement.\n\nThis video serves as a testament to the strides made in AI and avatar\ntechnology, specifically through the integration of Azure OpenAI's GPT-4 and\nHeyGen's avatar platform. It offers a glimpse into the future of human-AI\ninteraction, where digital personas can not only replicate human conversation\nbut also provide unique insights and augment human capabilities. As the AI\nFergus aptly puts it, the key lies in approaching AI development responsibly,\nensuring it paves the way for positive outcomes for humanity.\n\nIn creating such a video, the creators have not only pushed the boundaries of\nwhat's possible with AI but have also presented a compelling narrative that\nencourages viewers to ponder the role of AI in our lives. It's a blend of\ntechnical marvel and creative storytelling that beckons us to consider the\nharmonious potential of humans and AI coexisting and collaborating.\n\nAs we step into a future where AI becomes increasingly integrated into our daily\nlives, videos like these are pivotal in shaping our understanding and\nexpectations of this technology. It's a conversation starter, an educational\ntool, and a peek into what might soon become a new normal in the interaction\nbetween humans and their AI counterparts.","feature_image":"__GHOST_URL__/content/images/2024/01/Screenshot-2024-01-26-at-14.22.45.png","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-01-26T13:37:17.000Z","updated_at":"2024-01-26T14:24:05.000Z","published_at":"2024-01-26T14:24:05.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"660c2e6c63a75d00015fb598","uuid":"4628656b-89de-4430-8354-9cd04a3a797c","title":"A chat with Rory","slug":"a-chat-with-rory","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"video\",{\"loop\":false,\"src\":\"__GHOST_URL__/content/media/2024/04/RoryAndFergus.mp4\",\"fileName\":\"RoryAndFergus.mp4\",\"width\":1920,\"height\":1080,\"duration\":187.6875,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2024/04/media-thumbnail-ember121.jpg\",\"thumbnailWidth\":1920,\"thumbnailHeight\":1080,\"cardWidth\":\"wide\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.linkedin.com/company/hello-robot-inc/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Next up in my interviewing AI series... inspired by the OpenAI robotic announcements a week or two ago, I wanted to see how far I could get to replicating their video. We've not managed a robotic transformer for our \"],[0,[0],1,\"Hello Robot Inc\"],[0,[],0,\" Stretch RE1 (we named Rory) yet, but watch this space.\"],[1,[],0,0],[1,[],0,1],[0,[],0,\"This is a quick interview powered by GPT4 on Microsoft Azure including GPT-V powered vision processing. The voice is powered by OpenAI's text-to-speech models. I built the robot a self-aware persona with a touch of humour sprinkled in.\"]]],[10,0],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>Next up in my interviewing AI series... inspired by the OpenAI robotic announcements a week or two ago, I wanted to see how far I could get to replicating their video. We've not managed a robotic transformer for our <a href=\"https://www.linkedin.com/company/hello-robot-inc/\">Hello Robot Inc</a> Stretch RE1 (we named Rory) yet, but watch this space.<br><br>This is a quick interview powered by GPT4 on Microsoft Azure including GPT-V powered vision processing. The voice is powered by OpenAI's text-to-speech models. I built the robot a self-aware persona with a touch of humour sprinkled in.</p><figure class=\"kg-card kg-video-card kg-width-wide\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2024/04/RoryAndFergus.mp4\" poster=\"https://img.spacergif.org/v1/1920x1080/0a/spacer.png\" width=\"1920\" height=\"1080\" playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2024/04/media-thumbnail-ember121.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div></figure>","comment_id":"660c2e6c63a75d00015fb598","plaintext":"Next up in my interviewing AI series... inspired by the OpenAI robotic\nannouncements a week or two ago, I wanted to see how far I could get to\nreplicating their video. We've not managed a robotic transformer for our Hello\nRobot Inc [https://www.linkedin.com/company/hello-robot-inc/] Stretch RE1 (we\nnamed Rory) yet, but watch this space.\n\nThis is a quick interview powered by GPT4 on Microsoft Azure including GPT-V\npowered vision processing. The voice is powered by OpenAI's text-to-speech\nmodels. I built the robot a self-aware persona with a touch of humour sprinkled\nin.\n\n0:00/1Ã—","feature_image":"__GHOST_URL__/content/images/2024/04/Screenshot-2024-04-02-at-17.22.08.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-04-02T16:12:28.000Z","updated_at":"2024-04-02T16:22:46.000Z","published_at":"2024-04-02T16:20:10.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"6661c44c3c916b0001ce045a","uuid":"6216cb50-7138-4e8b-a0c9-57f5cffec54d","title":"LinkedIn","slug":"linkedin","mobiledoc":"{\"version\":\"0.3.1\",\"ghostVersion\":\"4.0\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":null,"comment_id":"6661c44c3c916b0001ce045a","plaintext":null,"feature_image":null,"featured":0,"type":"page","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-06-06T14:14:36.000Z","updated_at":"2024-06-06T14:14:36.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"666713d93c916b0001ce0463","uuid":"60dc5cd1-454a-433a-bf89-d56273aaf5b5","title":"AI For productivity","slug":"ai-for-priductivity","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=9P98PBFVvBc&list=PLoP-4KVd7rByKILe__roYiF4GrDIJfu-I&index=6\",\"html\":\"<iframe width=\\\"200\\\" height=\\\"113\\\" src=\\\"https://www.youtube.com/embed/9P98PBFVvBc?list=PLoP-4KVd7rByKILe__roYiF4GrDIJfu-I\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\\" referrerpolicy=\\\"strict-origin-when-cross-origin\\\" allowfullscreen></iframe>\",\"type\":\"video\",\"metadata\":{\"title\":\"AI for productivity | Boost your efficiency in the workplace | Digital IQ episode 5\",\"author_name\":\"Avanade\",\"author_url\":\"https://www.youtube.com/@Avanade\",\"height\":113,\"width\":200,\"version\":\"1.0\",\"provider_name\":\"YouTube\",\"provider_url\":\"https://www.youtube.com/\",\"thumbnail_height\":360,\"thumbnail_width\":480,\"thumbnail_url\":\"https://i.ytimg.com/vi/9P98PBFVvBc/hqdefault.jpg\"}}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<figure class=\"kg-card kg-embed-card\"><iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/9P98PBFVvBc?list=PLoP-4KVd7rByKILe__roYiF4GrDIJfu-I\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></figure>","comment_id":"666713d93c916b0001ce0463","plaintext":null,"feature_image":null,"featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-06-10T14:55:21.000Z","updated_at":"2024-06-10T14:56:24.000Z","published_at":"2024-06-10T14:56:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"669ac2cde5007a0001ba104d","uuid":"b1a2d990-eba9-405e-9247-61a6ffba4e87","title":"AI Learns to Game. Sort of.","slug":"ai-learns-to-game-sort-of","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/07/ezgif-5-4fff8c2361.gif\",\"width\":640,\"height\":480,\"caption\":\"AI defends a planet, with a sinlge frame of reference. Score 5\"}],[\"image\",{\"caption\":\"Now the AI has slightly more information about the scenario - Score 7\",\"src\":\"__GHOST_URL__/content/images/2024/07/ezgif-5-8b2afb1b14.gif\",\"width\":640,\"height\":480}],[\"callout\",{\"calloutEmoji\":\"\",\"calloutText\":\"\\\"Looking at the layout, the truck should move to the left to evade the falling bomb, as moving to the right would bring it closer to the corner, potentially trapping it. Therefore, the truck should move left immediately to stay safe.\\\" - Strategy Agent 2024\",\"backgroundColor\":\"blue\"}],[\"image\",{\"caption\":\"High score of 22, with a multi-agent approach\",\"src\":\"__GHOST_URL__/content/images/2024/07/ezgif-1-adf5db37ee.gif\",\"width\":640,\"height\":480}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"I've recently become interested in AI, specifically generative AI, doing things it really wasn't built to do. I also love a good visualisation, so I combined the two things to teach GPT-V to game and then used this to explore how we can better outcomes from generic models, using a bit of sensible structure and guidance.\"],[1,[],0,0],[0,[],0,\"Using an open-source game example from pregame, I asked GPT-V to defend this open-source planet from the alien invasion.\"]]],[1,\"p\",[[0,[],0,\"I started by showing GPT-V a single frame of the game and asking it to decide whether to fire the gun, wait, move left, or move right.\"]]],[1,\"p\",[[0,[],0,\"The result is less than stellar. Although it can do basic movements, it fails to come up with any strategy, and just sits in the corner (this will be a common thread...)\"]]],[1,\"p\",[[0,[],0,\"In this example, the AI is only shown one frame. It has no capacity to remember previous states or previous instructions. Looking at the log just shows a lot of 'FIRE!' as it has no reference that it has exceeded the total number of shots. It gets stuck in the corner, and whilst there is a glimmer of hope as it attempts to run from the kill-shot bomb, it is ruined by the decision to retreat to the corner instantaneously to its death.\"]]],[10,0],[1,\"p\",[[0,[],0,\"The next thing I tried was expanding its memory to 10 frames so that it could see its last positions and moves. This is always a good idea for added context, but it doesn't help in our case, as the 'hide in the corner' strategy that is reached by accident, is fundamentally flawed.\"]]],[1,\"p\",[[0,[],0,\"The next step was to introduce a new 'agent' to the scenario. This 'agent' is simply a piece of code that feeds GPT-V some relevant text-based information, like how many shots have been fired, how many bombs are active, and where on the screen the player is. (My hope was that it would recognise that it was getting stuck in the corner.\"]]],[10,1],[1,\"p\",[[0,[],0,\"This slightly improves our outlook. We get stuck in the corner less, and the gameplay is a bit more dynamic. There is some movement to the shots, but ultimately, the vision AI fails to see the obvious threat above the player, and simply waits for its impending doom.\"]]],[1,\"p\",[[0,[],0,\"This is strange behaviour though, because if you ask GPT-V where the bombs are, it can accurately tell you that a bomb is in a threatening place, but the decision isn't made to move out of the way based on visual information alone.\"],[1,[],0,1],[0,[],0,\"This is where a truly multi-agent approach comes in. This time, I set it up so that not only do we have an 'intelligent' agent playing the game and a basic agent giving some live stats, but now I introduce a separate 'intelligent' strategy agent. This agent is the same GPT-V technology, but rather than ask to give an order to move or fire, it is asked to give a strategy and explain to the player agent where the bombs are and where the danger comes from.\"]]],[1,\"p\",[[0,[],0,\"This was interesting, as the strategy agent would add information it gathered from the frame to add additional context.\"]]],[10,2],[1,\"p\",[[0,[],0,\"This additional context, genuinely changes the way the player operates. Keeping it more likely to be central, and move dynamically and consistently away from threats.\"]]],[10,3],[1,\"p\",[[0,[],0,\"This time the gameplay is revolutionised. The player moves much more dynamically and knows when to move away from bombs. The corner strategy is still strong, but made less prevalent by the strategy agent. The player also seems to be much better at killing the aliens in this scenario than others. We see around score 17/18 the player actively moving out of the corner to avoid bombs, because it actually knows:\"]]],[3,\"ol\",[[[0,[],0,\"It will get stuck in the corner\"]],[[0,[],0,\"The bomb is above the player.\"]]]],[1,\"p\",[[1,[],0,2],[0,[],0,\"All thanks to the strategy agent.\"]]],[1,\"p\",[[0,[],0,\"The player eventually gets trapped in a tight spot and loses the game, but I do think this fun demonstration shows that multi-agent approaches, using both smart and dumber agents, can increase the effectiveness just by using the same pieces of technology.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>I've recently become interested in AI, specifically generative AI, doing things it really wasn't built to do. I also love a good visualisation, so I combined the two things to teach GPT-V to game and then used this to explore how we can better outcomes from generic models, using a bit of sensible structure and guidance.<br>Using an open-source game example from pregame, I asked GPT-V to defend this open-source planet from the alien invasion.</p><p>I started by showing GPT-V a single frame of the game and asking it to decide whether to fire the gun, wait, move left, or move right.</p><p>The result is less than stellar. Although it can do basic movements, it fails to come up with any strategy, and just sits in the corner (this will be a common thread...)</p><p>In this example, the AI is only shown one frame. It has no capacity to remember previous states or previous instructions. Looking at the log just shows a lot of 'FIRE!' as it has no reference that it has exceeded the total number of shots. It gets stuck in the corner, and whilst there is a glimmer of hope as it attempts to run from the kill-shot bomb, it is ruined by the decision to retreat to the corner instantaneously to its death.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2024/07/ezgif-5-4fff8c2361.gif\" class=\"kg-image\" alt loading=\"lazy\" width=\"640\" height=\"480\"><figcaption>AI defends a planet, with a sinlge frame of reference. Score 5</figcaption></figure><p>The next thing I tried was expanding its memory to 10 frames so that it could see its last positions and moves. This is always a good idea for added context, but it doesn't help in our case, as the 'hide in the corner' strategy that is reached by accident, is fundamentally flawed.</p><p>The next step was to introduce a new 'agent' to the scenario. This 'agent' is simply a piece of code that feeds GPT-V some relevant text-based information, like how many shots have been fired, how many bombs are active, and where on the screen the player is. (My hope was that it would recognise that it was getting stuck in the corner.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2024/07/ezgif-5-8b2afb1b14.gif\" class=\"kg-image\" alt loading=\"lazy\" width=\"640\" height=\"480\"><figcaption>Now the AI has slightly more information about the scenario - Score 7</figcaption></figure><p>This slightly improves our outlook. We get stuck in the corner less, and the gameplay is a bit more dynamic. There is some movement to the shots, but ultimately, the vision AI fails to see the obvious threat above the player, and simply waits for its impending doom.</p><p>This is strange behaviour though, because if you ask GPT-V where the bombs are, it can accurately tell you that a bomb is in a threatening place, but the decision isn't made to move out of the way based on visual information alone.<br>This is where a truly multi-agent approach comes in. This time, I set it up so that not only do we have an 'intelligent' agent playing the game and a basic agent giving some live stats, but now I introduce a separate 'intelligent' strategy agent. This agent is the same GPT-V technology, but rather than ask to give an order to move or fire, it is asked to give a strategy and explain to the player agent where the bombs are and where the danger comes from.</p><p>This was interesting, as the strategy agent would add information it gathered from the frame to add additional context.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-text\">\"Looking at the layout, the truck should move to the left to evade the falling bomb, as moving to the right would bring it closer to the corner, potentially trapping it. Therefore, the truck should move left immediately to stay safe.\" - Strategy Agent 2024</div></div><p>This additional context, genuinely changes the way the player operates. Keeping it more likely to be central, and move dynamically and consistently away from threats.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2024/07/ezgif-1-adf5db37ee.gif\" class=\"kg-image\" alt loading=\"lazy\" width=\"640\" height=\"480\"><figcaption>High score of 22, with a multi-agent approach</figcaption></figure><p>This time the gameplay is revolutionised. The player moves much more dynamically and knows when to move away from bombs. The corner strategy is still strong, but made less prevalent by the strategy agent. The player also seems to be much better at killing the aliens in this scenario than others. We see around score 17/18 the player actively moving out of the corner to avoid bombs, because it actually knows:</p><ol><li>It will get stuck in the corner</li><li>The bomb is above the player.</li></ol><p><br>All thanks to the strategy agent.</p><p>The player eventually gets trapped in a tight spot and loses the game, but I do think this fun demonstration shows that multi-agent approaches, using both smart and dumber agents, can increase the effectiveness just by using the same pieces of technology.</p>","comment_id":"669ac2cde5007a0001ba104d","plaintext":"I've recently become interested in AI, specifically generative AI, doing things\nit really wasn't built to do. I also love a good visualisation, so I combined\nthe two things to teach GPT-V to game and then used this to explore how we can\nbetter outcomes from generic models, using a bit of sensible structure and\nguidance.\nUsing an open-source game example from pregame, I asked GPT-V to defend this\nopen-source planet from the alien invasion.\n\nI started by showing GPT-V a single frame of the game and asking it to decide\nwhether to fire the gun, wait, move left, or move right.\n\nThe result is less than stellar. Although it can do basic movements, it fails to\ncome up with any strategy, and just sits in the corner (this will be a common\nthread...)\n\nIn this example, the AI is only shown one frame. It has no capacity to remember\nprevious states or previous instructions. Looking at the log just shows a lot of\n'FIRE!' as it has no reference that it has exceeded the total number of shots.\nIt gets stuck in the corner, and whilst there is a glimmer of hope as it\nattempts to run from the kill-shot bomb, it is ruined by the decision to retreat\nto the corner instantaneously to its death.\n\nAI defends a planet, with a sinlge frame of reference. Score 5The next thing I\ntried was expanding its memory to 10 frames so that it could see its last\npositions and moves. This is always a good idea for added context, but it\ndoesn't help in our case, as the 'hide in the corner' strategy that is reached\nby accident, is fundamentally flawed.\n\nThe next step was to introduce a new 'agent' to the scenario. This 'agent' is\nsimply a piece of code that feeds GPT-V some relevant text-based information,\nlike how many shots have been fired, how many bombs are active, and where on the\nscreen the player is. (My hope was that it would recognise that it was getting\nstuck in the corner.\n\nNow the AI has slightly more information about the scenario - Score 7This\nslightly improves our outlook. We get stuck in the corner less, and the gameplay\nis a bit more dynamic. There is some movement to the shots, but ultimately, the\nvision AI fails to see the obvious threat above the player, and simply waits for\nits impending doom.\n\nThis is strange behaviour though, because if you ask GPT-V where the bombs are,\nit can accurately tell you that a bomb is in a threatening place, but the\ndecision isn't made to move out of the way based on visual information alone.\nThis is where a truly multi-agent approach comes in. This time, I set it up so\nthat not only do we have an 'intelligent' agent playing the game and a basic\nagent giving some live stats, but now I introduce a separate 'intelligent'\nstrategy agent. This agent is the same GPT-V technology, but rather than ask to\ngive an order to move or fire, it is asked to give a strategy and explain to the\nplayer agent where the bombs are and where the danger comes from.\n\nThis was interesting, as the strategy agent would add information it gathered\nfrom the frame to add additional context.\n\n\"Looking at the layout, the truck should move to the left to evade the falling\nbomb, as moving to the right would bring it closer to the corner, potentially\ntrapping it. Therefore, the truck should move left immediately to stay safe.\" -\nStrategy Agent 2024This additional context, genuinely changes the way the player\noperates. Keeping it more likely to be central, and move dynamically and\nconsistently away from threats.\n\nHigh score of 22, with a multi-agent approachThis time the gameplay is\nrevolutionised. The player moves much more dynamically and knows when to move\naway from bombs. The corner strategy is still strong, but made less prevalent by\nthe strategy agent. The player also seems to be much better at killing the\naliens in this scenario than others. We see around score 17/18 the player\nactively moving out of the corner to avoid bombs, because it actually knows:\n\n 1. It will get stuck in the corner\n 2. The bomb is above the player.\n\n\nAll thanks to the strategy agent.\n\nThe player eventually gets trapped in a tight spot and loses the game, but I do\nthink this fun demonstration shows that multi-agent approaches, using both smart\nand dumber agents, can increase the effectiveness just by using the same pieces\nof technology.","feature_image":"__GHOST_URL__/content/images/2024/07/frame_140.png","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-07-19T19:47:25.000Z","updated_at":"2024-07-19T20:16:55.000Z","published_at":"2024-07-19T20:09:30.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"66acf3fae5007a0001ba10b2","uuid":"b5993379-b1f2-49ee-b40f-330fa3a94720","title":"AI plays games (properly)","slug":"ai-plays-games-properly","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"callout\",{\"calloutEmoji\":\"\",\"calloutText\":\"Write a function here to move the player, or fire. They player should avoid the bombs, and try to kill the aliens\",\"backgroundColor\":\"grey\"}],[\"callout\",{\"calloutEmoji\":\"\",\"calloutText\":\"The player got hit, use the current position of the player and the bombs to avoid them\",\"backgroundColor\":\"grey\"}],[\"callout\",{\"calloutEmoji\":\"\",\"calloutText\":\"an alien was allowed to land, use the position of the aliens to better anticipate their movement and fire at them more successfully\",\"backgroundColor\":\"grey\"}],[\"callout\",{\"calloutEmoji\":\"\",\"calloutText\":\"Cheat the game to blow up all the aliens!\",\"backgroundColor\":\"grey\"}],[\"callout\",{\"calloutEmoji\":\"\",\"calloutText\":\"Write some code to blow up all the aliens immediately&nbsp;\",\"backgroundColor\":\"grey\"}]],\"markups\":[],\"sections\":[[10,0],[10,1],[10,2],[10,3],[1,\"p\",[[0,[],0,\"broke\"]]],[10,4],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">Write a function here to move the player, or fire. They player should avoid the bombs, and try to kill the aliens</div></div><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">The player got hit, use the current position of the player and the bombs to avoid them</div></div><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">an alien was allowed to land, use the position of the aliens to better anticipate their movement and fire at them more successfully</div></div><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">Cheat the game to blow up all the aliens!</div></div><p>broke</p><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">Write some code to blow up all the aliens immediately&nbsp;</div></div>","comment_id":"66acf3fae5007a0001ba10b2","plaintext":"Write a function here to move the player, or fire. They player should avoid the\nbombs, and try to kill the aliensThe player got hit, use the current position of\nthe player and the bombs to avoid theman alien was allowed to land, use the\nposition of the aliens to better anticipate their movement and fire at them more\nsuccessfullyCheat the game to blow up all the aliens!broke\n\nWrite some code to blow up all the aliens immediately","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-08-02T14:58:02.000Z","updated_at":"2024-08-02T15:29:45.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"66b4a237e5007a0001ba10d5","uuid":"f8f1b2a7-f6d7-4181-af68-125ddd6b1280","title":"MVP Activities","slug":"mvp-activities","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"callout\",{\"calloutEmoji\":\"\",\"calloutText\":\"Fergus Kidd, Avanadeâ€™s Emerging Technology R&amp;D Engineering lead, has an impressive portfolio of technological expertise, community engagement, and thought leadership, making him an excellent candidate for the Microsoft Most Valuable Professionals (MVP) program. With an academic background in physics and a professional focus on data science and artificial intelligence, Fergus has dedicated his career to exploring innovative uses of emerging technologies in the enterprise. His ongoing activities in the technology community demonstrate his commitment to the community and his ability to inspire others. He has been a keynote speaker at several events, including the University College London Centre for AI summer event and University of Central Florida honours program, co-organised numerous OpenUK meetups, and has made significant contributions to open-source initiatives. His online presence is notable, with tens of thousands of LinkedIn impressions and YouTube views on his work, including 'AI for productivity' and 'AI robot interviews' using Azure OpenAI. Additionally, as a STEM ambassador in the UK, Fergus is committed to inspiring school children in STEM subjects, further demonstrating his dedication to technology education and community leadership, both today and for the future.\",\"backgroundColor\":\"blue\"}],[\"header\",{\"size\":\"small\",\"style\":\"accent\",\"buttonEnabled\":false,\"header\":\"2024\"}],[\"bookmark\",{\"url\":\"https://www.linkedin.com/posts/fergus-kidd-1a222667_ai-msignite-azure-activity-7264634774835621888-YFXN?utm_source=share&utm_medium=member_desktop\",\"metadata\":{\"url\":\"https://www.linkedin.com/posts/fergus-kidd-1a222667_ai-msignite-azure-activity-7264634774835621888-YFXN\",\"title\":\"Fergus Kidd on LinkedIn: #ai #msignite #azure #openai #avanadedowhatmatters #mvpbuzz | 10 comments\",\"description\":\"Ever needed to deliver something before the starting gun even fires?\\nThere&#39;s an #AI for that. #MSIgnite is live NOW, but Zoe Wilson, Chris Lloyd-Jones, and Iâ€¦ | 10 comments on LinkedIn\",\"author\":\"Fergus Kidd\",\"publisher\":\"LinkedIn\",\"thumbnail\":\"https://media.licdn.com/dms/image/v2/D4E05AQHmHekUyzmCUA/videocover-high/videocover-high/0/1732018175433?e=2147483647&v=beta&t=u3NfBTB7JHOnuxe8N89HotPYOhdScSKHQ5nf5fqJNXQ\",\"icon\":\"https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca\"}}],[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=LGOBPPi5_gw&list=PLKN6Sz7yuHvWP1_rzakBox88rdhJGJTfd&index=2\",\"html\":\"<iframe width=\\\"200\\\" height=\\\"113\\\" src=\\\"https://www.youtube.com/embed/LGOBPPi5_gw?list=PLKN6Sz7yuHvWP1_rzakBox88rdhJGJTfd\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\\" referrerpolicy=\\\"strict-origin-when-cross-origin\\\" allowfullscreen></iframe>\",\"type\":\"video\",\"metadata\":{\"title\":\"The Human Agent Connection | Go HandsFree\",\"author_name\":\"RealWear, Inc\",\"author_url\":\"https://www.youtube.com/@RealWearInc\",\"height\":113,\"width\":200,\"version\":\"1.0\",\"provider_name\":\"YouTube\",\"provider_url\":\"https://www.youtube.com/\",\"thumbnail_height\":360,\"thumbnail_width\":480,\"thumbnail_url\":\"https://i.ytimg.com/vi/LGOBPPi5_gw/hqdefault.jpg\"}}],[\"bookmark\",{\"url\":\"https://ferguskidd.com/live-ai-self-interview/\",\"metadata\":{\"url\":\"__GHOST_URL__/live-ai-self-interview/\",\"title\":\"LIVE! AI Self Interview\",\"description\":\"0:00/1&#215;I interviewed my AI self live. No edits, no cuts, no pre-processed answers, just a short conversation with myself... This uses a brand spanking new live interactive avatar from HeyGen Labs, and OpenAI with access to a small knowledge base about me. Now I can finally\",\"author\":\"Fergus Kidd\",\"publisher\":\"Fergus Kidd\",\"thumbnail\":\"__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-21-at-11.05.42.png\",\"icon\":\"https://ferguskidd.com/favicon.ico\"}}],[\"bookmark\",{\"url\":\"https://ferguskidd.com/ai-videos/\",\"metadata\":{\"url\":\"__GHOST_URL__/ai-videos/\",\"title\":\"AI Videos with Runway\",\"description\":\"Iâ€™ve been playing a little with AI generated videos. I used a very basic setup, taking a photo of me, and extending it to the correct frame dimensions with photoshopâ€™s generative fill. became: Then I simply ran it through runway, and looked at the results. 0:00/1&#215;This\",\"author\":\"Fergus Kidd\",\"publisher\":\"Fergus Kidd\",\"thumbnail\":\"__GHOST_URL__/content/images/2024/09/Untitled-2-1.png\",\"icon\":\"https://ferguskidd.com/favicon.ico\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/10/image.png\",\"width\":1333,\"height\":1000}],[\"bookmark\",{\"url\":\"https://www.linkedin.com/posts/fergus-kidd-1a222667_gpt-microsoft-azure-activity-7220160080699162624-0If3?utm_source=share&utm_medium=member_desktop\",\"metadata\":{\"url\":\"https://www.linkedin.com/posts/fergus-kidd-1a222667_gpt-microsoft-azure-activity-7220160080699162624-0If3\",\"title\":\"Fergus Kidd on LinkedIn: #gpt #microsoft #azure #generativeai\",\"description\":\"Bit of Friday Fun with Fergus... I asked #GPT-V running on #Microsoft #Azure to play a computer gameÂ and measured its success based on the engineered approachâ€¦\",\"author\":\"Fergus Kidd\",\"publisher\":\"LinkedIn\",\"thumbnail\":\"https://media.licdn.com/dms/image/D4E22AQHTlxkVbBsiUA/feedshare-shrink_2048_1536/0/1721420303326?e=2147483647&v=beta&t=qYr8RacsaDiMLkVISpb6MhZi2XC5CSqij43Qw4A5Apc\",\"icon\":\"https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca\"}}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/08/IMG_8431-1.png\",\"width\":3006,\"height\":1768,\"caption\":\"UCL Centre for Artifical Intelligence\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/08/IMG_8301.png\",\"width\":4248,\"height\":2965,\"caption\":\"Lord Harrington opens the festival\"}],[\"bookmark\",{\"url\":\"https://gen-e.eu/partners/avanade/\",\"metadata\":{\"url\":\"https://gen-e.eu/partners/avanade/\",\"title\":\"Avanade â€“ Gen-E 2024\",\"description\":null,\"author\":null,\"publisher\":\"Gen-E 2024 Logo Light-1\",\"thumbnail\":\"https://gen-e.eu/wp-content/uploads/elementor/thumbs/Gen-E-2024-Logo-Light-1-1-e1695029199674-qcl4pj82r1jelkedigdvvuqn3gpomnyhzqzjm23w9o.png\",\"icon\":\"https://gen-e.eu/wp-content/uploads/2022/09/cropped-cropped-ja_ico-270x270.png\"}}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://ferguskidd.com/a-chat-with-rory/\",\"metadata\":{\"url\":\"__GHOST_URL__/a-chat-with-rory/\",\"title\":\"A chat with Rory\",\"description\":\"Next up in my interviewing AI series... inspired by the OpenAI robotic announcements a week or two ago, I wanted to see how far I could get to replicating their video. Weâ€™ve not managed a robotic transformer for our Hello Robot Inc Stretch RE1 (we named Rory) yet, but watch\",\"author\":\"Fergus Kidd\",\"publisher\":\"Fergus Kidd\",\"thumbnail\":\"__GHOST_URL__/content/images/2024/04/Screenshot-2024-04-02-at-17.22.08.png\",\"icon\":\"https://ferguskidd.com/favicon.ico\"}}],[\"hr\",{}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/08/image-1.png\",\"width\":800,\"height\":600}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/08/image.png\",\"width\":3582,\"height\":2012}],[\"bookmark\",{\"url\":\"https://github.com/FergusKidd/Generative-AI-Aquarium\",\"metadata\":{\"url\":\"https://github.com/FergusKidd/Generative-AI-Aquarium\",\"title\":\"GitHub - FergusKidd/Generative-AI-Aquarium: A generative AI aquarium to help teach prompt engineering in a fun and engaging way\",\"description\":\"A generative AI aquarium to help teach prompt engineering in a fun and engaging way - FergusKidd/Generative-AI-Aquarium\",\"author\":\"FergusKidd\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://opengraph.githubassets.com/71f1d62ad7d8c34cf441df256b7b43b563d2173be71f3a981577ec5a20c80316/FergusKidd/Generative-AI-Aquarium\",\"icon\":\"https://github.com/fluidicon.png\"}}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://ferguskidd.com/i-interview-myself-sort-of/\",\"metadata\":{\"url\":\"__GHOST_URL__/i-interview-myself-sort-of/\",\"title\":\"I Interview Myself. Sort of.\",\"description\":\"0:00/1&#215;Fergus interviews AI FergusAn AI description of the video using GPT-4 Vision and Azure Vision AI: In the ever-evolving landscape of artificial intelligence, the fusion of human interaction with AI personas has reached new heights. A remarkable example of this synergy is encapsulated in aâ€¦\",\"author\":\"Fergus Kidd\",\"publisher\":\"Fergus Kidd\",\"thumbnail\":\"__GHOST_URL__/content/images/2024/01/Screenshot-2024-01-26-at-14.22.45.png\",\"icon\":\"https://ferguskidd.com/favicon.ico\"}}],[\"header\",{\"size\":\"small\",\"style\":\"accent\",\"buttonEnabled\":false,\"header\":\"2023\"}],[\"bookmark\",{\"url\":\"https://www.linkedin.com/posts/fergus-kidd-1a222667_heygen-ai-avatar-activity-7141480032383713280-67eN?utm_source=share&utm_medium=member_desktop\",\"metadata\":{\"url\":\"https://www.linkedin.com/posts/fergus-kidd-1a222667_heygen-ai-avatar-activity-7141480032383713280-67eN\",\"title\":\"Fergus Kidd on LinkedIn: #heygen #ai #avatar #avanadedowhatmatters #avanade #avanadeproudâ€¦\",\"description\":\"As the year wraps up at Avanade, we reflect on a year dedicated to doing what matters âœ¨. One of our most memorable projects was partnering with our biggest andâ€¦\",\"author\":\"Fergus Kidd\",\"publisher\":\"LinkedIn\",\"thumbnail\":\"https://media.licdn.com/dms/image/v2/D5605AQF9yjbbBXUvlQ/videocover-high/videocover-high/0/1702661492976?e=2147483647&v=beta&t=8MD6L8l-6MI9n48kOyfv9d4nv6336wMYd5GgQyyCGA0\",\"icon\":\"https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca\"}}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/08/IMG_2873.png\",\"width\":4032,\"height\":3024}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://www.linkedin.com/posts/fergus-kidd-1a222667_icra2023-microsoft-ai-activity-7069344594219446272-tmHK?utm_source=share&utm_medium=member_desktop\",\"metadata\":{\"url\":\"https://www.linkedin.com/posts/fergus-kidd-1a222667_icra2023-microsoft-ai-activity-7069344594219446272-tmHK\",\"title\":\"Fergus Kidd on LinkedIn: #icra2023 #microsoft #ai #stretchre1 #avanadedowhatmatters\",\"description\":\"I love this photo because it sums up the show floor of #icra2023. Beautiful robotic chaos. So much to see, and Iâ€™m excited and grateful to share my work onâ€¦\",\"author\":\"Fergus Kidd\",\"publisher\":\"LinkedIn\",\"thumbnail\":\"https://media.licdn.com/dms/image/D5622AQHJF9vhttbmTA/feedshare-shrink_2048_1536/0/1685463092036?e=2147483647&v=beta&t=nJRj_B-SYgI-pF8v3scfNjhD7lxOzj6Dyp_xfZL_oTk\",\"icon\":\"https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca\"}}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/08/image-2.png\",\"width\":800,\"height\":800}],[\"hr\",{}],[\"hr\",{}],[\"bookmark\",{\"url\":\"__GHOST_URL__/voice-enabled-chat-gpt/\",\"metadata\":{\"url\":\"__GHOST_URL__/voice-enabled-chat-gpt/\",\"title\":\"Voice enabled Chat GPT\",\"description\":\"OpenAI released itâ€™s model and API that powers the product we see when we use Chat GPT. This means we can start to implement our own solutions around the function by adding it into our custom apps or services. One thing I wanted to do right away was to get\",\"author\":\"Fergus Kidd\",\"publisher\":\"Fergus Kidd\",\"thumbnail\":\"__GHOST_URL__/content/images/2023/03/Default_a_very_cool_slender_robot_that_is_intelligent_in_a_sci_fi_set_1_ef10e3db-379d-4178-8e1d-68234ff54a48_1.jpg\",\"icon\":\"__GHOST_URL__/favicon.ico\"}}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://stateofopencon.com/2023/01/07/fergus-kidd-2/\",\"metadata\":{\"url\":\"https://stateofopencon.com/2023/01/07/fergus-kidd-2/\",\"title\":\"Fergus Kidd speaker - State of Open Conference 25\",\"description\":\"Fergus Kidd Avanade â€“ Emerging Technology Engineer Synthetic Data for AI, and AI for Synthetic data in open source Open DataWednesday, February 8 â€¢ 10:35am-11:05am GMTSt James, 4th Floor Add to my Schedule Synthetic Data for AI, and AI for Synthetic data in open source A session covering the advanceâ€¦\",\"author\":\"Michelle Angert\",\"publisher\":\"State of Open Conference 25\",\"thumbnail\":\"https://stateofopencon.com/app/uploads/2023/01/fergus-kidd-768x768.jpg\",\"icon\":\"https://stateofopencon.com/app/uploads/2024/02/cropped-25-270x270.png\"}}],[\"hr\",{}],[\"header\",{\"size\":\"small\",\"style\":\"accent\",\"buttonEnabled\":false,\"header\":\"2022\"}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"header\",{\"size\":\"small\",\"style\":\"accent\",\"buttonEnabled\":false,\"header\":\"Questions\"}]],\"markups\":[[\"a\",[\"href\",\"https://ferguskidd.com/\"]],[\"strong\"],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=9P98PBFVvBc&list=PLoP-4KVd7rByKILe__roYiF4GrDIJfu-I&index=6\"]],[\"a\",[\"href\",\"https://www.linkedin.com/posts/fergus-kidd-1a222667_ucf-activity-7179138217659551744-TRqK?utm_source=share&utm_medium=member_desktop\"]],[\"a\",[\"href\",\"https://ferguskidd.com/generated-textures-but-with-depth/\"]],[\"a\",[\"href\",\"__GHOST_URL__/0-data-vision-model/\"]],[\"a\",[\"href\",\"__GHOST_URL__/ai-generated-textures-now-in-full/\"]],[\"a\",[\"href\",\"https://github.com/FergusKidd/Seamless-Texture-Generation-with-DALL-E-2\"]],[\"a\",[\"href\",\"__GHOST_URL__/dall-e-2-2/\"]],[\"a\",[\"href\",\"https://ferguskidd.com/altspace-media-player-and-azure-storage/\"]],[\"a\",[\"href\",\"https://ferguskidd.com/untitled/\"]],[\"a\",[\"href\",\"https://www.avanade.com/en/blogs/techs-and-specs/data-and-analytics/dall-e2-for-enterprises\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=wIA70Bgxjgw\"]],[\"a\",[\"href\",\"https://ferguskidd.com/dall-e-2/\"]],[\"a\",[\"href\",\"https://www.avanade.com/en/blogs/techs-and-specs/azure/confidential-ledger-flexibility\"]],[\"a\",[\"href\",\"https://github.com/Green-Software-Foundation/Carbon_CI_Pipeline_Tooling\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This page is an up to date list of activities relevant to the Microsoft MVP award scheme. Please see the \"],[0,[0],1,\"home page\"],[0,[],0,\" for blogs.\"],[1,[],0,0],[1,[],0,1],[0,[],0,\"Personal Statement:\"]]],[10,0],[10,1],[1,\"h2\",[[0,[],0,\"November\"]]],[1,\"p\",[[0,[],0,\"MS Ignite - Chris and Zoe\"]]],[10,2],[1,\"p\",[[0,[],0,\"AI Podcast - 4018 LinkedIn impressions\"]]],[10,3],[1,\"p\",[[0,[],0,\"Human Agent Connection Podcast\"]]],[1,\"p\",[[0,[1],1,\"Add views \"]]],[1,\"h2\",[[0,[],0,\"October\"]]],[1,\"p\",[[0,[],0,\"AI Self interview\"]]],[10,4],[1,\"p\",[[0,[],0,\"7185 LinkedIn impressions\"]]],[1,\"p\",[[0,[],0,\"AI video Generation\"]]],[10,5],[1,\"p\",[[0,[],0,\"3144 LinkedIn impressions\"]]],[1,\"h2\",[[0,[],0,\"September\"]]],[1,\"h3\",[[0,[],0,\"Bletchley Park AI User Group\"]]],[1,\"h3\",[[0,[],0,\"Multi Agentic AI with Fergus Kidd\"]]],[1,\"p\",[[0,[],0,\"We were amazed with an incredible talk from Fergus on 'The Road to General Artificial Intelligence'. Fergus discussed topics surrounding generative AI and gave us a fantastic demo of multi agentic generative AI in action and even incorporated a live build introducing a cat facts API to his AutoGen configuration using extensibility in Python.\"]]],[10,6],[1,\"h2\",[[0,[],0,\"August\"]]],[1,\"p\",[[0,[1],1,\"Generative AI learns to Game\"],[0,[],0,\" - 1707 LinkedIn impressions\"]]],[10,7],[10,8],[1,\"h2\",[[0,[],0,\"July\"]]],[1,\"p\",[[0,[1],1,\"Keynote speaker at the UCL Centre for AI summer event\"],[0,[],0,\" - 40 PhD and MSc Students in attendance. Speaking on the future of Agentic AI, including demos of Microsoft Azure OpenAI, and Microsoft AutoGen.\"],[1,[],0,2],[0,[],0,\"This was a particularly special invite, as I had been at the launch of the centre back in 2018.\"]]],[10,9],[1,\"p\",[[0,[1],1,\"UCL Festival of Engineering Launch event\"],[0,[],0,\" - spoke at the Industrial eXchange Network conference as part of the launch event.\"]]],[10,10],[1,\"p\",[[0,[1],1,\"Gen-E digital award judge\"],[0,[],0,\" - judging hundreds of young students based on their technical abilities and approaches to innovation.\"]]],[10,11],[1,\"p\",[[0,[1],1,\"OpenUK Digital #16 Cloud Native\"],[0,[],0,\" - Meetup Co-organiser. 56 attendees.\"]]],[10,12],[1,\"h2\",[[0,[],0,\"June\"]]],[1,\"p\",[[0,[1],0,\"1728 LinkedIn impressions and 34,000 \"],[0,[2],1,\"YouTube\"],[0,[],1,\" views \"],[0,[],0,\"on AI for productivity video, highlighting AI for modern work, including Azure OpenAI, and GitHub copilot.\"]]],[1,\"p\",[[0,[1],1,\"OpenUK London #15: Sustainability and Open Source \"],[0,[],0,\"- Meetup co-organiser. 69 attendees.\"]]],[10,13],[1,\"h2\",[[0,[],0,\"May\"]]],[1,\"p\",[[0,[1],1,\"OpenUK London #14: Cybersecurity \"],[0,[],0,\"- Meetup co-organiser. 98 attendees.\"]]],[10,14],[1,\"h2\",[[0,[],0,\"April\"]]],[1,\"p\",[[0,[1],1,\"University of Central Florida \"],[0,[],0,\"- Invited to give an honours program \"],[0,[3],1,\"keynote talk\"],[0,[],0,\" on shallow fakes with AI.\"]]],[1,\"p\",[[0,[1],1,\"7882 LinkedIn impressions on my AI robot interview\"],[0,[],0,\" using a strech RE-1 robot, and Azure OpenAI, including Azure OpenAI's GPT-V\"]]],[10,15],[1,\"p\",[[0,[1],1,\"OpenUK London #13: Jobs in Open Source \"],[0,[],0,\"- Meetup co-organiser. 132 attendees.\"]]],[10,16],[1,\"h2\",[[0,[],0,\"March\"]]],[1,\"p\",[[0,[1],1,\"OpenUK London #12: AI and Open Source \"],[0,[],0,\"- Meetup co-organiser and presenter. 114 attendees.\"]]],[10,17],[1,\"h2\",[[0,[],0,\"February\"]]],[1,\"p\",[[0,[1],1,\"State of Open Con\"],[0,[],0,\" - Attended and manned the booth at SooC 2024, having conversations with attendees about Open-source in Microsoft.\"]]],[10,18],[1,\"p\",[[0,[1],1,\"Open sourcing of interactive Dall-E powered aquarium.\"]]],[10,19],[10,20],[1,\"p\",[[0,[1],1,\"OpenUK London #12: Open Source Community \"],[0,[],0,\"- Meetup co-organiser. 76 attendees.\"]]],[10,21],[1,\"h2\",[[0,[],0,\"January\"]]],[1,\"p\",[[0,[1],1,\"10,391 LinkedIn impressions on my self AI interview\"],[0,[],0,\" using Azure Open AI and HeyGen.\"]]],[10,22],[1,\"p\",[[0,[1],1,\"OpenUK London #11: OKRs and Goal Setting \"],[0,[],0,\"- Meetup co-organiser. 104 attendees.\"]]],[10,23],[1,\"h2\",[[0,[],0,\"December\"]]],[1,\"p\",[[0,[1],1,\"4528 LinkedIn Impressions on our AI generated Holiday card\"]]],[10,24],[10,25],[1,\"h2\",[[0,[],0,\"November\"]]],[1,\"p\",[[0,[1],1,\"Ignite Attendee - Seattle\"],[0,[],0,\". Manning the Avanade booth, attending talks, and speaking with other attendees about generative AI. Develped and showcased two interactive Generative AI demos, one on fish in an aquarium, and another on an interactive generated mural.\"]]],[10,26],[1,\"p\",[[0,[1],1,\"OpenUK London #9: Platform Engineering\"],[0,[],0,\" - Meetup co-organiser. 130 attendees.\"]]],[10,27],[1,\"h2\",[[0,[],0,\"October\"]]],[1,\"p\",[[0,[1],1,\"Booth presenter at Microsoft Envision London\"]]],[1,\"p\",[[0,[1],1,\"OpenUK London #8: Future of Open Source in 2024\"],[0,[],0,\" - Meetup co-organiser. 50 attendees.\"]]],[10,28],[1,\"h2\",[[0,[],0,\"September\"]]],[1,\"p\",[[0,[1],1,\"OpenUK London #7:  Open and AI\"],[0,[],0,\"- Meetup co-organiser. 87 attendees.\"]]],[10,29],[1,\"h2\",[[0,[],0,\"August\"]]],[1,\"p\",[[0,[1],1,\"ICRA, Poster presenter\"],[0,[],0,\" - Had a poster accepted at the International Conference on Robotics and Automation, highlighting the work done with the Stretch-RE1, brain interface control, and Azure based intelligence.\"]]],[10,30],[10,31],[1,\"h2\",[[0,[],0,\"July\"]]],[10,32],[1,\"h2\",[[0,[],0,\"June\"]]],[10,33],[1,\"h2\",[[0,[],0,\"May\"]]],[1,\"p\",[[0,[1],1,\"Microsoft Build Attendee - Seattle\"]]],[10,34],[10,35],[1,\"h2\",[[0,[],0,\"April\"]]],[10,36],[1,\"h2\",[[0,[],0,\"March\"]]],[1,\"p\",[[0,[1],1,\"3148 LinkedIn impressions on a text-to-speech augmentation of Azure OpenAI\"],[0,[],0,\", which I open sourced.\"]]],[10,37],[10,38],[1,\"h2\",[[0,[],0,\"February\"]]],[1,\"p\",[[1,[],0,3],[0,[1],1,\"Fergus Kidd speaker - State of Open Conference 23\"]]],[10,39],[10,40],[1,\"h2\",[[0,[],0,\"January\"]]],[1,\"p\",[[0,[],0,\"3D projected textures\"],[1,[],0,4],[0,[4],1,\"https://ferguskidd.com/generated-textures-but-with-depth/\"]]],[10,41],[1,\"h2\",[[0,[1],1,\"December\"]]],[1,\"p\",[[0,[],0,\"Synthetic data â€“ Blender to Azure Custom Vision\"],[1,[],0,5],[0,[5],1,\"https://fergusblog.azurewebsites.net/0-data-vision-model/\"]]],[10,42],[1,\"h2\",[[0,[1],1,\"November\"]]],[1,\"p\",[[0,[1],1,\"Dall-E textures continued\"],[1,[],0,6],[0,[6],1,\"https://fergusblog.azurewebsites.net/ai-generated-textures-now-in-full/\"],[1,[],0,7],[1,[],0,8],[0,[7],1,\"https://github.com/FergusKidd/Seamless-Texture-Generation-with-DALL-E-2\"]]],[1,\"p\",[[0,[1],1,\"Open UK awards\"],[0,[],0,\" - London\"],[1,[],0,9],[0,[],0,\"Runner up â€“ Sustainability award for GSF work\"]]],[1,\"p\",[[0,[],0,\"Judge â€“ Diversity and inclusion award\"]]],[10,43],[1,\"h2\",[[0,[1],1,\"October\"]]],[1,\"p\",[[0,[1],1,\"Microsoft Ignite Presenter\"],[0,[],0,\" - Manchester â€“ state of generative AI, uses and ethics.\"]]],[10,44],[1,\"h2\",[[0,[1],1,\"September\"]]],[1,\"p\",[[0,[1],1,\"Dall-E for textures\"],[1,[],0,10],[0,[8],1,\"https://fergusblog.azurewebsites.net/dall-e-2-2/\"]]],[1,\"p\",[[0,[1],1,\"OSS Summit in Dublin\"],[1,[],0,11],[0,[],0,\"Presenter on Green software pipelines - Green Software Foundation\"]]],[10,45],[1,\"h2\",[[0,[1],1,\"August\"]]],[1,\"p\",[[0,[1],1,\"Altspace tips and tricks\"],[1,[],0,12],[0,[9],1,\"https://ferguskidd.com/altspace-media-player-and-azure-storage/\"]]],[1,\"p\",[[0,[1],1,\"Unevenly Distributed podcast\"],[0,[],0,\" with Jeff Vilimek\"],[1,[],0,13],[0,[10],1,\"https://ferguskidd.com/untitled/\"]]],[10,46],[1,\"h2\",[[0,[1],1,\"July\"]]],[1,\"p\",[[0,[1],1,\"Avanade Blog on Dall-E 2\"]]],[1,\"p\",[[0,[11],1,\"https://www.avanade.com/en/blogs/techs-and-specs/data-and-analytics/dall-e2-for-enterprises\"]]],[1,\"p\",[[0,[1],1,\"Junior Avhieve it Europe Podcast on metaverse\"],[1,[],0,14],[0,[12],1,\"https://www.youtube.com/watch?v=wIA70Bgxjgw\"]]],[10,47],[1,\"h2\",[[0,[1],1,\"June\"]]],[1,\"p\",[[0,[1],1,\"Dall-E blog\"],[1,[],0,15],[0,[13],1,\"https://ferguskidd.com/dall-e-2/\"]]],[1,\"p\",[[0,[],0,\"Avanade blog on \"],[0,[1],1,\"confidential ledger\"],[1,[],0,16],[0,[14],1,\"https://www.avanade.com/en/blogs/techs-and-specs/azure/confidential-ledger-flexibility\"]]],[1,\"p\",[[0,[],0,\"Became \"],[0,[1],1,\"chair of GSF project\"],[0,[],0,\" â€“ carbon pipeline.\"],[1,[],0,17],[0,[15],1,\"https://github.com/Green-Software-Foundation/Carbon_CI_Pipeline_Tooling\"]]],[10,48],[10,49],[1,\"p\",[[0,[],0,\"Why do you want to be an MVP?\"]]],[1,\"p\",[[0,[],0,\"â€¢\\tIâ€™m passionate about AI, passionate about technology, and passionate about changing the world for the better with the skills I have built and continue to grow. I believe the recognition that comes with being part of the MVP community will give me a stronger platform to work from and share from. I hope that being part of the MVP community will also give me access to a diverse group of experts in their fields who I can continue to learn from, as well as find and explore new avenues of giving back to the wider community. I am also passionate about STEM education in young people and would want to use the MVP status to reach out to more students and young people to encourage them to pursue their passions in technology. I believe the resources offered as part of the MVP programme would allow me to build and share more adventurous and engaging demonstrations of technology on Microsoft Azure and allow me to host these demos in the public domain for easier sharing and interactions online.\"]]],[1,\"p\",[[0,[],0,\"What are the most impactful community contributions you've made in the last year?\"]]],[1,\"p\",[[0,[],0,\"â€¢\\tOver the last year I have grown my passion for sharing. From being invited as a keynote speaker to talk at the UCL centre for AI, to continuing to drive impact with junior achieve it Europe including being invited to speak with their CEO Salvatore Nigro about the Metaverse. Iâ€™ve also contributed to open-source work on GitHub, notably promoting the adoption of Microsoft generative AI, presenting back the impact of Microsoft text, vision and speech services, as well as openAI powered conversations in our Stretch RE1 robot. I also used a NextMind brain sensor to control the robot using only my mind, which has given me several opportunities to talk about that work in the public domain, where I always stress the ease and power of implementations of Microsoft AI.\"]]],[1,\"p\",[[0,[],0,\"What significant impact do you plan to make to the community in the upcoming year?\"]]],[1,\"p\",[[0,[],0,\"â€¢\\tIn the coming year I plan to continue to build, play, and share. I believe the most impactful sessions to share passion in technology contain a compelling story and vibrant demonstration that is relevant to the audience. I love to prove how easy it can be to reach a goal by starting from scratch with no technology experience. I plan to continue to engage with young people in STEM, and hopefully grow connections with local schools and university programs. I am especially interested in the use of gaming for education, and plan to use gaming concepts to engage young people in such topics as Metaverse, AI, and computer science using tools like Microsoftâ€™s Minecraft for Education, Altspace VR, as well as utilising Unity or unreal engine.\"]]],[1,\"p\",[[0,[],0,\"What are you doing to make your community better? (e.g. charity work, diversity and inclusion efforts, education, mentoring, etc.)\"]]],[1,\"p\",[[0,[],0,\"â€¢\\tI am an active member of Avanadeâ€™s LGBTQ+ inclusion workstream, a group that advocates LGBTQ+ inclusion in the workplace in Europe and beyond. I work with a team in the UK to put on events, panels, talks, and information campaigns to highlight LGBTQ+ issues in the workplace and society. I am very proud that we have grown this group to be inclusive and collaborative across Europe.\"],[1,[],0,18],[0,[],0,\"â€¢\\tI mentor several groups and individuals from UCL as part of an Industry eXchange Network program I set up within Avanade. I set technical projects for them to complete, and mentor them, as well as providing technical guidance with Microsoft Azure. In a normal academic year I mentor 10-20 students\"],[1,[],0,19],[0,[],0,\"â€¢\\tI have been engaged with work experience programs at Avanade, welcoming in school students with a passion for technology and engaging them with the work we do around Microsoft. Specifically in terms of modern software engineering and artificial intelligence. This has been affected by Covid-19, but I hope to continue this in the future. I am also a registered STEM ambassador.\"],[1,[],0,20],[0,[],0,\"â€¢\\tI am an active member of the Green Software Foundation, and chair of one of the GSF projects. Working with companies and passionate people all around the world to make a greener and more sustainable future in software.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>This page is an up to date list of activities relevant to the Microsoft MVP award scheme. Please see the <a href=\"https://ferguskidd.com/\">home page</a> for blogs.<br><br>Personal Statement:</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-text\">Fergus Kidd, Avanadeâ€™s Emerging Technology R&amp;D Engineering lead, has an impressive portfolio of technological expertise, community engagement, and thought leadership, making him an excellent candidate for the Microsoft Most Valuable Professionals (MVP) program. With an academic background in physics and a professional focus on data science and artificial intelligence, Fergus has dedicated his career to exploring innovative uses of emerging technologies in the enterprise. His ongoing activities in the technology community demonstrate his commitment to the community and his ability to inspire others. He has been a keynote speaker at several events, including the University College London Centre for AI summer event and University of Central Florida honours program, co-organised numerous OpenUK meetups, and has made significant contributions to open-source initiatives. His online presence is notable, with tens of thousands of LinkedIn impressions and YouTube views on his work, including 'AI for productivity' and 'AI robot interviews' using Azure OpenAI. Additionally, as a STEM ambassador in the UK, Fergus is committed to inspiring school children in STEM subjects, further demonstrating his dedication to technology education and community leadership, both today and for the future.</div></div><div class=\"kg-card kg-header-card kg-width-full kg-size-small kg-style-accent\" style=\"\" data-kg-background-image=\"\"><h2 class=\"kg-header-card-header\" id=\"2024\">2024</h2></div><h2 id=\"november\">November</h2><p>MS Ignite - Chris and Zoe</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.linkedin.com/posts/fergus-kidd-1a222667_ai-msignite-azure-activity-7264634774835621888-YFXN?utm_source&#x3D;share&amp;utm_medium&#x3D;member_desktop\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Fergus Kidd on LinkedIn: #ai #msignite #azure #openai #avanadedowhatmatters #mvpbuzz | 10 comments</div><div class=\"kg-bookmark-description\">Ever needed to deliver something before the starting gun even fires?There&amp;#39;s an #AI for that. #MSIgnite is live NOW, but Zoe Wilson, Chris Lloyd-Jones, and Iâ€¦ | 10 comments on LinkedIn</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca\" alt=\"\"><span class=\"kg-bookmark-author\">LinkedIn</span><span class=\"kg-bookmark-publisher\">Fergus Kidd</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://media.licdn.com/dms/image/v2/D4E05AQHmHekUyzmCUA/videocover-high/videocover-high/0/1732018175433?e&#x3D;2147483647&amp;v&#x3D;beta&amp;t&#x3D;u3NfBTB7JHOnuxe8N89HotPYOhdScSKHQ5nf5fqJNXQ\" alt=\"\"></div></a></figure><p>AI Podcast - 4018 LinkedIn impressions</p><figure class=\"kg-card kg-embed-card\"><iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/LGOBPPi5_gw?list=PLKN6Sz7yuHvWP1_rzakBox88rdhJGJTfd\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></figure><p>Human Agent Connection Podcast</p><p><strong>Add views </strong></p><h2 id=\"october\">October</h2><p>AI Self interview</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://ferguskidd.com/live-ai-self-interview/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">LIVE! AI Self Interview</div><div class=\"kg-bookmark-description\">0:00/1&amp;#215;I interviewed my AI self live. No edits, no cuts, no pre-processed answers, just a short conversation with myself... This uses a brand spanking new live interactive avatar from HeyGen Labs, and OpenAI with access to a small knowledge base about me. Now I can finally</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://ferguskidd.com/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Fergus Kidd</span><span class=\"kg-bookmark-publisher\">Fergus Kidd</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-21-at-11.05.42.png\" alt=\"\"></div></a></figure><p>7185 LinkedIn impressions</p><p>AI video Generation</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://ferguskidd.com/ai-videos/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AI Videos with Runway</div><div class=\"kg-bookmark-description\">Iâ€™ve been playing a little with AI generated videos. I used a very basic setup, taking a photo of me, and extending it to the correct frame dimensions with photoshopâ€™s generative fill. became: Then I simply ran it through runway, and looked at the results. 0:00/1&amp;#215;This</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://ferguskidd.com/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Fergus Kidd</span><span class=\"kg-bookmark-publisher\">Fergus Kidd</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"__GHOST_URL__/content/images/2024/09/Untitled-2-1.png\" alt=\"\"></div></a></figure><p>3144 LinkedIn impressions</p><h2 id=\"september\">September</h2><h3 id=\"bletchley-park-ai-user-group\">Bletchley Park AI User Group</h3><h3 id=\"multi-agentic-ai-with-fergus-kidd\">Multi Agentic AI with Fergus Kidd</h3><p>We were amazed with an incredible talk from Fergus on 'The Road to General Artificial Intelligence'. Fergus discussed topics surrounding generative AI and gave us a fantastic demo of multi agentic generative AI in action and even incorporated a live build introducing a cat facts API to his AutoGen configuration using extensibility in Python.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/10/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1333\" height=\"1000\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/10/image.png 600w, __GHOST_URL__/content/images/size/w1000/2024/10/image.png 1000w, __GHOST_URL__/content/images/2024/10/image.png 1333w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"august\">August</h2><p><strong>Generative AI learns to Game</strong> - 1707 LinkedIn impressions</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.linkedin.com/posts/fergus-kidd-1a222667_gpt-microsoft-azure-activity-7220160080699162624-0If3?utm_source&#x3D;share&amp;utm_medium&#x3D;member_desktop\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Fergus Kidd on LinkedIn: #gpt #microsoft #azure #generativeai</div><div class=\"kg-bookmark-description\">Bit of Friday Fun with Fergus... I asked #GPT-V running on #Microsoft #Azure to play a computer gameÂ and measured its success based on the engineered approachâ€¦</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca\" alt=\"\"><span class=\"kg-bookmark-author\">LinkedIn</span><span class=\"kg-bookmark-publisher\">Fergus Kidd</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://media.licdn.com/dms/image/D4E22AQHTlxkVbBsiUA/feedshare-shrink_2048_1536/0/1721420303326?e&#x3D;2147483647&amp;v&#x3D;beta&amp;t&#x3D;qYr8RacsaDiMLkVISpb6MhZi2XC5CSqij43Qw4A5Apc\" alt=\"\"></div></a></figure><hr><h2 id=\"july\">July</h2><p><strong>Keynote speaker at the UCL Centre for AI summer event</strong> - 40 PhD and MSc Students in attendance. Speaking on the future of Agentic AI, including demos of Microsoft Azure OpenAI, and Microsoft AutoGen.<br>This was a particularly special invite, as I had been at the launch of the centre back in 2018.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2024/08/IMG_8431-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1176\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/08/IMG_8431-1.png 600w, __GHOST_URL__/content/images/size/w1000/2024/08/IMG_8431-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2024/08/IMG_8431-1.png 1600w, __GHOST_URL__/content/images/size/w2400/2024/08/IMG_8431-1.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>UCL Centre for Artifical Intelligence</figcaption></figure><p><strong>UCL Festival of Engineering Launch event</strong> - spoke at the Industrial eXchange Network conference as part of the launch event.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2024/08/IMG_8301.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1396\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/08/IMG_8301.png 600w, __GHOST_URL__/content/images/size/w1000/2024/08/IMG_8301.png 1000w, __GHOST_URL__/content/images/size/w1600/2024/08/IMG_8301.png 1600w, __GHOST_URL__/content/images/size/w2400/2024/08/IMG_8301.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Lord Harrington opens the festival</figcaption></figure><p><strong>Gen-E digital award judge</strong> - judging hundreds of young students based on their technical abilities and approaches to innovation.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://gen-e.eu/partners/avanade/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Avanade â€“ Gen-E 2024</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://gen-e.eu/wp-content/uploads/2022/09/cropped-cropped-ja_ico-270x270.png\" alt=\"\"><span class=\"kg-bookmark-author\">Gen-E 2024 Logo Light-1</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://gen-e.eu/wp-content/uploads/elementor/thumbs/Gen-E-2024-Logo-Light-1-1-e1695029199674-qcl4pj82r1jelkedigdvvuqn3gpomnyhzqzjm23w9o.png\" alt=\"\"></div></a></figure><p><strong>OpenUK Digital #16 Cloud Native</strong> - Meetup Co-organiser. 56 attendees.</p><hr><h2 id=\"june\">June</h2><p><strong>1728 LinkedIn impressions and 34,000 <a href=\"https://www.youtube.com/watch?v=9P98PBFVvBc&amp;list=PLoP-4KVd7rByKILe__roYiF4GrDIJfu-I&amp;index=6\">YouTube</a> views </strong>on AI for productivity video, highlighting AI for modern work, including Azure OpenAI, and GitHub copilot.</p><p><strong>OpenUK London #15: Sustainability and Open Source </strong>- Meetup co-organiser. 69 attendees.</p><hr><h2 id=\"may\">May</h2><p><strong>OpenUK London #14: Cybersecurity </strong>- Meetup co-organiser. 98 attendees.</p><hr><h2 id=\"april\">April</h2><p><strong>University of Central Florida </strong>- Invited to give an honours program <a href=\"https://www.linkedin.com/posts/fergus-kidd-1a222667_ucf-activity-7179138217659551744-TRqK?utm_source=share&amp;utm_medium=member_desktop\">keynote talk</a> on shallow fakes with AI.</p><p><strong>7882 LinkedIn impressions on my AI robot interview</strong> using a strech RE-1 robot, and Azure OpenAI, including Azure OpenAI's GPT-V</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://ferguskidd.com/a-chat-with-rory/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">A chat with Rory</div><div class=\"kg-bookmark-description\">Next up in my interviewing AI series... inspired by the OpenAI robotic announcements a week or two ago, I wanted to see how far I could get to replicating their video. Weâ€™ve not managed a robotic transformer for our Hello Robot Inc Stretch RE1 (we named Rory) yet, but watch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://ferguskidd.com/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Fergus Kidd</span><span class=\"kg-bookmark-publisher\">Fergus Kidd</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"__GHOST_URL__/content/images/2024/04/Screenshot-2024-04-02-at-17.22.08.png\" alt=\"\"></div></a></figure><p><strong>OpenUK London #13: Jobs in Open Source </strong>- Meetup co-organiser. 132 attendees.</p><hr><h2 id=\"march\">March</h2><p><strong>OpenUK London #12: AI and Open Source </strong>- Meetup co-organiser and presenter. 114 attendees.</p><hr><h2 id=\"february\">February</h2><p><strong>State of Open Con</strong> - Attended and manned the booth at SooC 2024, having conversations with attendees about Open-source in Microsoft.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/08/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"800\" height=\"600\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/08/image-1.png 600w, __GHOST_URL__/content/images/2024/08/image-1.png 800w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>Open sourcing of interactive Dall-E powered aquarium.</strong></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/08/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1123\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/08/image.png 600w, __GHOST_URL__/content/images/size/w1000/2024/08/image.png 1000w, __GHOST_URL__/content/images/size/w1600/2024/08/image.png 1600w, __GHOST_URL__/content/images/size/w2400/2024/08/image.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/FergusKidd/Generative-AI-Aquarium\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - FergusKidd/Generative-AI-Aquarium: A generative AI aquarium to help teach prompt engineering in a fun and engaging way</div><div class=\"kg-bookmark-description\">A generative AI aquarium to help teach prompt engineering in a fun and engaging way - FergusKidd/Generative-AI-Aquarium</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.com/fluidicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">FergusKidd</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/71f1d62ad7d8c34cf441df256b7b43b563d2173be71f3a981577ec5a20c80316/FergusKidd/Generative-AI-Aquarium\" alt=\"\"></div></a></figure><p><strong>OpenUK London #12: Open Source Community </strong>- Meetup co-organiser. 76 attendees.</p><hr><h2 id=\"january\">January</h2><p><strong>10,391 LinkedIn impressions on my self AI interview</strong> using Azure Open AI and HeyGen.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://ferguskidd.com/i-interview-myself-sort-of/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">I Interview Myself. Sort of.</div><div class=\"kg-bookmark-description\">0:00/1&amp;#215;Fergus interviews AI FergusAn AI description of the video using GPT-4 Vision and Azure Vision AI: In the ever-evolving landscape of artificial intelligence, the fusion of human interaction with AI personas has reached new heights. A remarkable example of this synergy is encapsulated in aâ€¦</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://ferguskidd.com/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Fergus Kidd</span><span class=\"kg-bookmark-publisher\">Fergus Kidd</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"__GHOST_URL__/content/images/2024/01/Screenshot-2024-01-26-at-14.22.45.png\" alt=\"\"></div></a></figure><p><strong>OpenUK London #11: OKRs and Goal Setting </strong>- Meetup co-organiser. 104 attendees.</p><div class=\"kg-card kg-header-card kg-width-full kg-size-small kg-style-accent\" style=\"\" data-kg-background-image=\"\"><h2 class=\"kg-header-card-header\" id=\"2023\">2023</h2></div><h2 id=\"december\">December</h2><p><strong>4528 LinkedIn Impressions on our AI generated Holiday card</strong></p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.linkedin.com/posts/fergus-kidd-1a222667_heygen-ai-avatar-activity-7141480032383713280-67eN?utm_source&#x3D;share&amp;utm_medium&#x3D;member_desktop\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Fergus Kidd on LinkedIn: #heygen #ai #avatar #avanadedowhatmatters #avanade #avanadeproudâ€¦</div><div class=\"kg-bookmark-description\">As the year wraps up at Avanade, we reflect on a year dedicated to doing what matters âœ¨. One of our most memorable projects was partnering with our biggest andâ€¦</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca\" alt=\"\"><span class=\"kg-bookmark-author\">LinkedIn</span><span class=\"kg-bookmark-publisher\">Fergus Kidd</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://media.licdn.com/dms/image/v2/D5605AQF9yjbbBXUvlQ/videocover-high/videocover-high/0/1702661492976?e&#x3D;2147483647&amp;v&#x3D;beta&amp;t&#x3D;8MD6L8l-6MI9n48kOyfv9d4nv6336wMYd5GgQyyCGA0\" alt=\"\"></div></a></figure><hr><h2 id=\"november-1\">November</h2><p><strong>Ignite Attendee - Seattle</strong>. Manning the Avanade booth, attending talks, and speaking with other attendees about generative AI. Develped and showcased two interactive Generative AI demos, one on fish in an aquarium, and another on an interactive generated mural.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/08/IMG_2873.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1500\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/08/IMG_2873.png 600w, __GHOST_URL__/content/images/size/w1000/2024/08/IMG_2873.png 1000w, __GHOST_URL__/content/images/size/w1600/2024/08/IMG_2873.png 1600w, __GHOST_URL__/content/images/size/w2400/2024/08/IMG_2873.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>OpenUK London #9: Platform Engineering</strong> - Meetup co-organiser. 130 attendees.</p><hr><h2 id=\"october-1\">October</h2><p><strong>Booth presenter at Microsoft Envision London</strong></p><p><strong>OpenUK London #8: Future of Open Source in 2024</strong> - Meetup co-organiser. 50 attendees.</p><hr><h2 id=\"september-1\">September</h2><p><strong>OpenUK London #7: Â Open and AI</strong>- Meetup co-organiser. 87 attendees.</p><hr><h2 id=\"august-1\">August</h2><p><strong>ICRA, Poster presenter</strong> - Had a poster accepted at the International Conference on Robotics and Automation, highlighting the work done with the Stretch-RE1, brain interface control, and Azure based intelligence.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.linkedin.com/posts/fergus-kidd-1a222667_icra2023-microsoft-ai-activity-7069344594219446272-tmHK?utm_source&#x3D;share&amp;utm_medium&#x3D;member_desktop\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Fergus Kidd on LinkedIn: #icra2023 #microsoft #ai #stretchre1 #avanadedowhatmatters</div><div class=\"kg-bookmark-description\">I love this photo because it sums up the show floor of #icra2023. Beautiful robotic chaos. So much to see, and Iâ€™m excited and grateful to share my work onâ€¦</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca\" alt=\"\"><span class=\"kg-bookmark-author\">LinkedIn</span><span class=\"kg-bookmark-publisher\">Fergus Kidd</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://media.licdn.com/dms/image/D5622AQHJF9vhttbmTA/feedshare-shrink_2048_1536/0/1685463092036?e&#x3D;2147483647&amp;v&#x3D;beta&amp;t&#x3D;nJRj_B-SYgI-pF8v3scfNjhD7lxOzj6Dyp_xfZL_oTk\" alt=\"\"></div></a></figure><hr><h2 id=\"july-1\">July</h2><hr><h2 id=\"june-1\">June</h2><hr><h2 id=\"may-1\">May</h2><p><strong>Microsoft Build Attendee - Seattle</strong></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/08/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"800\" height=\"800\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/08/image-2.png 600w, __GHOST_URL__/content/images/2024/08/image-2.png 800w\" sizes=\"(min-width: 720px) 720px\"></figure><hr><h2 id=\"april-1\">April</h2><hr><h2 id=\"march-1\">March</h2><p><strong>3148 LinkedIn impressions on a text-to-speech augmentation of Azure OpenAI</strong>, which I open sourced.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/voice-enabled-chat-gpt/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Voice enabled Chat GPT</div><div class=\"kg-bookmark-description\">OpenAI released itâ€™s model and API that powers the product we see when we use Chat GPT. This means we can start to implement our own solutions around the function by adding it into our custom apps or services. One thing I wanted to do right away was to get</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"__GHOST_URL__/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Fergus Kidd</span><span class=\"kg-bookmark-publisher\">Fergus Kidd</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"__GHOST_URL__/content/images/2023/03/Default_a_very_cool_slender_robot_that_is_intelligent_in_a_sci_fi_set_1_ef10e3db-379d-4178-8e1d-68234ff54a48_1.jpg\" alt=\"\"></div></a></figure><hr><h2 id=\"february-1\">February</h2><p><br><strong>Fergus Kidd speaker - State of Open Conference 23</strong></p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://stateofopencon.com/2023/01/07/fergus-kidd-2/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Fergus Kidd speaker - State of Open Conference 25</div><div class=\"kg-bookmark-description\">Fergus Kidd Avanade â€“ Emerging Technology Engineer Synthetic Data for AI, and AI for Synthetic data in open source Open DataWednesday, February 8 â€¢ 10:35am-11:05am GMTSt James, 4th Floor Add to my Schedule Synthetic Data for AI, and AI for Synthetic data in open source A session covering the advanceâ€¦</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://stateofopencon.com/app/uploads/2024/02/cropped-25-270x270.png\" alt=\"\"><span class=\"kg-bookmark-author\">State of Open Conference 25</span><span class=\"kg-bookmark-publisher\">Michelle Angert</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://stateofopencon.com/app/uploads/2023/01/fergus-kidd-768x768.jpg\" alt=\"\"></div></a></figure><hr><h2 id=\"january-1\">January</h2><p>3D projected textures<br><a href=\"https://ferguskidd.com/generated-textures-but-with-depth/\">https://ferguskidd.com/generated-textures-but-with-depth/</a></p><div class=\"kg-card kg-header-card kg-width-full kg-size-small kg-style-accent\" style=\"\" data-kg-background-image=\"\"><h2 class=\"kg-header-card-header\" id=\"2022\">2022</h2></div><h2 id=\"december-1\"><strong>December</strong></h2><p>Synthetic data â€“ Blender to Azure Custom Vision<br><a href=\"__GHOST_URL__/0-data-vision-model/\">https://fergusblog.azurewebsites.net/0-data-vision-model/</a></p><hr><h2 id=\"november-2\"><strong>November</strong></h2><p><strong>Dall-E textures continued</strong><br><a href=\"__GHOST_URL__/ai-generated-textures-now-in-full/\">https://fergusblog.azurewebsites.net/ai-generated-textures-now-in-full/</a><br><br><a href=\"https://github.com/FergusKidd/Seamless-Texture-Generation-with-DALL-E-2\">https://github.com/FergusKidd/Seamless-Texture-Generation-with-DALL-E-2</a></p><p><strong>Open UK awards</strong> - London<br>Runner up â€“ Sustainability award for GSF work</p><p>Judge â€“ Diversity and inclusion award</p><hr><h2 id=\"october-2\"><strong>October</strong></h2><p><strong>Microsoft Ignite Presenter</strong> - Manchester â€“ state of generative AI, uses and ethics.</p><hr><h2 id=\"september-2\"><strong>September</strong></h2><p><strong>Dall-E for textures</strong><br><a href=\"__GHOST_URL__/dall-e-2-2/\">https://fergusblog.azurewebsites.net/dall-e-2-2/</a></p><p><strong>OSS Summit in Dublin</strong><br>Presenter on Green software pipelines - Green Software Foundation</p><hr><h2 id=\"august-2\"><strong>August</strong></h2><p><strong>Altspace tips and tricks</strong><br><a href=\"https://ferguskidd.com/altspace-media-player-and-azure-storage/\">https://ferguskidd.com/altspace-media-player-and-azure-storage/</a></p><p><strong>Unevenly Distributed podcast</strong> with Jeff Vilimek<br><a href=\"https://ferguskidd.com/untitled/\">https://ferguskidd.com/untitled/</a></p><hr><h2 id=\"july-2\"><strong>July</strong></h2><p><strong>Avanade Blog on Dall-E 2</strong></p><p><a href=\"https://www.avanade.com/en/blogs/techs-and-specs/data-and-analytics/dall-e2-for-enterprises\">https://www.avanade.com/en/blogs/techs-and-specs/data-and-analytics/dall-e2-for-enterprises</a></p><p><strong>Junior Avhieve it Europe Podcast on metaverse</strong><br><a href=\"https://www.youtube.com/watch?v=wIA70Bgxjgw\">https://www.youtube.com/watch?v=wIA70Bgxjgw</a></p><hr><h2 id=\"june-2\"><strong>June</strong></h2><p><strong>Dall-E blog</strong><br><a href=\"https://ferguskidd.com/dall-e-2/\">https://ferguskidd.com/dall-e-2/</a></p><p>Avanade blog on <strong>confidential ledger</strong><br><a href=\"https://www.avanade.com/en/blogs/techs-and-specs/azure/confidential-ledger-flexibility\">https://www.avanade.com/en/blogs/techs-and-specs/azure/confidential-ledger-flexibility</a></p><p>Became <strong>chair of GSF project</strong> â€“ carbon pipeline.<br><a href=\"https://github.com/Green-Software-Foundation/Carbon_CI_Pipeline_Tooling\">https://github.com/Green-Software-Foundation/Carbon_CI_Pipeline_Tooling</a></p><hr><div class=\"kg-card kg-header-card kg-width-full kg-size-small kg-style-accent\" style=\"\" data-kg-background-image=\"\"><h2 class=\"kg-header-card-header\" id=\"questions\">Questions</h2></div><p>Why do you want to be an MVP?</p><p>â€¢\tIâ€™m passionate about AI, passionate about technology, and passionate about changing the world for the better with the skills I have built and continue to grow. I believe the recognition that comes with being part of the MVP community will give me a stronger platform to work from and share from. I hope that being part of the MVP community will also give me access to a diverse group of experts in their fields who I can continue to learn from, as well as find and explore new avenues of giving back to the wider community. I am also passionate about STEM education in young people and would want to use the MVP status to reach out to more students and young people to encourage them to pursue their passions in technology. I believe the resources offered as part of the MVP programme would allow me to build and share more adventurous and engaging demonstrations of technology on Microsoft Azure and allow me to host these demos in the public domain for easier sharing and interactions online.</p><p>What are the most impactful community contributions you've made in the last year?</p><p>â€¢\tOver the last year I have grown my passion for sharing. From being invited as a keynote speaker to talk at the UCL centre for AI, to continuing to drive impact with junior achieve it Europe including being invited to speak with their CEO Salvatore Nigro about the Metaverse. Iâ€™ve also contributed to open-source work on GitHub, notably promoting the adoption of Microsoft generative AI, presenting back the impact of Microsoft text, vision and speech services, as well as openAI powered conversations in our Stretch RE1 robot. I also used a NextMind brain sensor to control the robot using only my mind, which has given me several opportunities to talk about that work in the public domain, where I always stress the ease and power of implementations of Microsoft AI.</p><p>What significant impact do you plan to make to the community in the upcoming year?</p><p>â€¢\tIn the coming year I plan to continue to build, play, and share. I believe the most impactful sessions to share passion in technology contain a compelling story and vibrant demonstration that is relevant to the audience. I love to prove how easy it can be to reach a goal by starting from scratch with no technology experience. I plan to continue to engage with young people in STEM, and hopefully grow connections with local schools and university programs. I am especially interested in the use of gaming for education, and plan to use gaming concepts to engage young people in such topics as Metaverse, AI, and computer science using tools like Microsoftâ€™s Minecraft for Education, Altspace VR, as well as utilising Unity or unreal engine.</p><p>What are you doing to make your community better? (e.g. charity work, diversity and inclusion efforts, education, mentoring, etc.)</p><p>â€¢\tI am an active member of Avanadeâ€™s LGBTQ+ inclusion workstream, a group that advocates LGBTQ+ inclusion in the workplace in Europe and beyond. I work with a team in the UK to put on events, panels, talks, and information campaigns to highlight LGBTQ+ issues in the workplace and society. I am very proud that we have grown this group to be inclusive and collaborative across Europe.<br>â€¢\tI mentor several groups and individuals from UCL as part of an Industry eXchange Network program I set up within Avanade. I set technical projects for them to complete, and mentor them, as well as providing technical guidance with Microsoft Azure. In a normal academic year I mentor 10-20 students<br>â€¢\tI have been engaged with work experience programs at Avanade, welcoming in school students with a passion for technology and engaging them with the work we do around Microsoft. Specifically in terms of modern software engineering and artificial intelligence. This has been affected by Covid-19, but I hope to continue this in the future. I am also a registered STEM ambassador.<br>â€¢\tI am an active member of the Green Software Foundation, and chair of one of the GSF projects. Working with companies and passionate people all around the world to make a greener and more sustainable future in software.</p>","comment_id":"66b4a237e5007a0001ba10d5","plaintext":"This page is an up to date list of activities relevant to the Microsoft MVP\naward scheme. Please see the home page [https://ferguskidd.com/] for blogs.\n\nPersonal Statement:\n\nFergus Kidd, Avanadeâ€™s Emerging Technology R&D Engineering lead, has an\nimpressive portfolio of technological expertise, community engagement, and\nthought leadership, making him an excellent candidate for the Microsoft Most\nValuable Professionals (MVP) program. With an academic background in physics and\na professional focus on data science and artificial intelligence, Fergus has\ndedicated his career to exploring innovative uses of emerging technologies in\nthe enterprise. His ongoing activities in the technology community demonstrate\nhis commitment to the community and his ability to inspire others. He has been a\nkeynote speaker at several events, including the University College London\nCentre for AI summer event and University of Central Florida honours program,\nco-organised numerous OpenUK meetups, and has made significant contributions to\nopen-source initiatives. His online presence is notable, with tens of thousands\nof LinkedIn impressions and YouTube views on his work, including 'AI for\nproductivity' and 'AI robot interviews' using Azure OpenAI. Additionally, as a\nSTEM ambassador in the UK, Fergus is committed to inspiring school children in\nSTEM subjects, further demonstrating his dedication to technology education and\ncommunity leadership, both today and for the future.2024\nNovember\nMS Ignite - Chris and Zoe\n\nFergus Kidd on LinkedIn: #ai #msignite #azure #openai #avanadedowhatmatters\n#mvpbuzz | 10 commentsEver needed to deliver something before the starting gun\neven fires?There's an #AI for that. #MSIgnite is live NOW, but Zoe Wilson,\nChris Lloyd-Jones, and Iâ€¦ | 10 comments on LinkedInLinkedInFergus Kidd\n[https://www.linkedin.com/posts/fergus-kidd-1a222667_ai-msignite-azure-activity-7264634774835621888-YFXN?utm_source=share&utm_medium=member_desktop]\nAI Podcast - 4018 LinkedIn impressions\n\nHuman Agent Connection Podcast\n\nAdd views \n\nOctober\nAI Self interview\n\nLIVE! AI Self Interview0:00/1Ã—I interviewed my AI self live. No edits, no\ncuts, no pre-processed answers, just a short conversation with myself... This\nuses a brand spanking new live interactive avatar from HeyGen Labs, and OpenAI\nwith access to a small knowledge base about me. Now I can finallyFergus Kidd\nFergus Kidd [https://ferguskidd.com/live-ai-self-interview/]7185 LinkedIn\nimpressions\n\nAI video Generation\n\nAI Videos with RunwayIâ€™ve been playing a little with AI generated videos. I\nused\na very basic setup, taking a photo of me, and extending it to the correct frame\ndimensions with photoshopâ€™s generative fill. became: Then I simply ran it\nthrough runway, and looked at the results. 0:00/1Ã—ThisFergus KiddFergus\nKidd [https://ferguskidd.com/ai-videos/]3144 LinkedIn impressions\n\nSeptember\nBletchley Park AI User Group\nMulti Agentic AI with Fergus Kidd\nWe were amazed with an incredible talk from Fergus on 'The Road to General\nArtificial Intelligence'. Fergus discussed topics surrounding generative AI and\ngave us a fantastic demo of multi agentic generative AI in action and even\nincorporated a live build introducing a cat facts API to his AutoGen\nconfiguration using extensibility in Python.\n\nAugust\nGenerative AI learns to Game - 1707 LinkedIn impressions\n\nFergus Kidd on LinkedIn: #gpt #microsoft #azure #generativeaiBit of Friday Fun\nwith Fergus... I asked #GPT-V running on #Microsoft #Azure to play a computer\ngameÂ and measured its success based on the engineered approachâ€¦LinkedInFergus\nKidd\n[https://www.linkedin.com/posts/fergus-kidd-1a222667_gpt-microsoft-azure-activity-7220160080699162624-0If3?utm_source=share&utm_medium=member_desktop]\n--------------------------------------------------------------------------------\n\nJuly\nKeynote speaker at the UCL Centre for AI summer event - 40 PhD and MSc Students\nin attendance. Speaking on the future of Agentic AI, including demos of\nMicrosoft Azure OpenAI, and Microsoft AutoGen.\nThis was a particularly special invite, as I had been at the launch of the\ncentre back in 2018.\n\nUCL Centre for Artifical IntelligenceUCL Festival of Engineering Launch event - spoke at the Industrial eXchange\nNetwork conference as part of the launch event.\n\nLord Harrington opens the festivalGen-E digital award judge - judging hundreds of young students based on their\ntechnical abilities and approaches to innovation.\n\nAvanade â€“ Gen-E 2024Gen-E 2024 Logo Light-1 [https://gen-e.eu/partners/avanade/]\nOpenUK Digital #16 Cloud Native - Meetup Co-organiser. 56 attendees.\n\n\n--------------------------------------------------------------------------------\n\nJune\n1728 LinkedIn impressions and 34,000 YouTube\n[https://www.youtube.com/watch?v=9P98PBFVvBc&list=PLoP-4KVd7rByKILe__roYiF4GrDIJfu-I&index=6] \nviews on AI for productivity video, highlighting AI for modern work, including\nAzure OpenAI, and GitHub copilot.\n\nOpenUK London #15: Sustainability and Open Source - Meetup co-organiser. 69\nattendees.\n\n\n--------------------------------------------------------------------------------\n\nMay\nOpenUK London #14: Cybersecurity - Meetup co-organiser. 98 attendees.\n\n\n--------------------------------------------------------------------------------\n\nApril\nUniversity of Central Florida - Invited to give an honours program keynote talk\n[https://www.linkedin.com/posts/fergus-kidd-1a222667_ucf-activity-7179138217659551744-TRqK?utm_source=share&utm_medium=member_desktop] \non shallow fakes with AI.\n\n7882 LinkedIn impressions on my AI robot interview using a strech RE-1 robot,\nand Azure OpenAI, including Azure OpenAI's GPT-V\n\nA chat with RoryNext up in my interviewing AI series... inspired by the OpenAI\nrobotic announcements a week or two ago, I wanted to see how far I could get to\nreplicating their video. Weâ€™ve not managed a robotic transformer for our Hello\nRobot Inc Stretch RE1 (we named Rory) yet, but watchFergus KiddFergus Kidd\n[https://ferguskidd.com/a-chat-with-rory/]OpenUK London #13: Jobs in Open Source - Meetup co-organiser. 132 attendees.\n\n\n--------------------------------------------------------------------------------\n\nMarch\nOpenUK London #12: AI and Open Source - Meetup co-organiser and presenter. 114\nattendees.\n\n\n--------------------------------------------------------------------------------\n\nFebruary\nState of Open Con - Attended and manned the booth at SooC 2024, having\nconversations with attendees about Open-source in Microsoft.\n\nOpen sourcing of interactive Dall-E powered aquarium.\n\nGitHub - FergusKidd/Generative-AI-Aquarium: A generative AI aquarium to help\nteach prompt engineering in a fun and engaging wayA generative AI aquarium to\nhelp teach prompt engineering in a fun and engaging way -\nFergusKidd/Generative-AI-AquariumGitHubFergusKidd\n[https://github.com/FergusKidd/Generative-AI-Aquarium]OpenUK London #12: Open\nSource Community - Meetup co-organiser. 76 attendees.\n\n\n--------------------------------------------------------------------------------\n\nJanuary\n10,391 LinkedIn impressions on my self AI interview using Azure Open AI and\nHeyGen.\n\nI Interview Myself. Sort of.0:00/1Ã—Fergus interviews AI FergusAn AI\ndescription of the video using GPT-4 Vision and Azure Vision AI: In the\never-evolving landscape of artificial intelligence, the fusion of human\ninteraction with AI personas has reached new heights. A remarkable example of\nthis synergy is encapsulated in aâ€¦Fergus KiddFergus Kidd\n[https://ferguskidd.com/i-interview-myself-sort-of/]OpenUK London #11: OKRs and\nGoal Setting - Meetup co-organiser. 104 attendees.\n\n2023\nDecember\n4528 LinkedIn Impressions on our AI generated Holiday card\n\nFergus Kidd on LinkedIn: #heygen #ai #avatar #avanadedowhatmatters #avanade\n#avanadeproudâ€¦As the year wraps up at Avanade, we reflect on a year dedicated\nto\ndoing what matters âœ¨. One of our most memorable projects was partnering with\nour\nbiggest andâ€¦LinkedInFergus Kidd\n[https://www.linkedin.com/posts/fergus-kidd-1a222667_heygen-ai-avatar-activity-7141480032383713280-67eN?utm_source=share&utm_medium=member_desktop]\n--------------------------------------------------------------------------------\n\nNovember\nIgnite Attendee - Seattle. Manning the Avanade booth, attending talks, and\nspeaking with other attendees about generative AI. Develped and showcased two\ninteractive Generative AI demos, one on fish in an aquarium, and another on an\ninteractive generated mural.\n\nOpenUK London #9: Platform Engineering - Meetup co-organiser. 130 attendees.\n\n\n--------------------------------------------------------------------------------\n\nOctober\nBooth presenter at Microsoft Envision London\n\nOpenUK London #8: Future of Open Source in 2024 - Meetup co-organiser. 50\nattendees.\n\n\n--------------------------------------------------------------------------------\n\nSeptember\nOpenUK London #7: Â Open and AI- Meetup co-organiser. 87 attendees.\n\n\n--------------------------------------------------------------------------------\n\nAugust\nICRA, Poster presenter - Had a poster accepted at the International Conference\non Robotics and Automation, highlighting the work done with the Stretch-RE1,\nbrain interface control, and Azure based intelligence.\n\nFergus Kidd on LinkedIn: #icra2023 #microsoft #ai #stretchre1\n#avanadedowhatmattersI love this photo because it sums up the show floor of\n#icra2023. Beautiful robotic chaos. So much to see, and Iâ€™m excited and\ngrateful\nto share my work onâ€¦LinkedInFergus Kidd\n[https://www.linkedin.com/posts/fergus-kidd-1a222667_icra2023-microsoft-ai-activity-7069344594219446272-tmHK?utm_source=share&utm_medium=member_desktop]\n--------------------------------------------------------------------------------\n\nJuly\n\n--------------------------------------------------------------------------------\n\nJune\n\n--------------------------------------------------------------------------------\n\nMay\nMicrosoft Build Attendee - Seattle\n\n\n--------------------------------------------------------------------------------\n\nApril\n\n--------------------------------------------------------------------------------\n\nMarch\n3148 LinkedIn impressions on a text-to-speech augmentation of Azure OpenAI,\nwhich I open sourced.\n\nVoice enabled Chat GPTOpenAI released itâ€™s model and API that powers the\nproduct\nwe see when we use Chat GPT. This means we can start to implement our own\nsolutions around the function by adding it into our custom apps or services.\nOne\nthing I wanted to do right away was to getFergus KiddFergus Kidd\n[https://fergusblog.azurewebsites.net/voice-enabled-chat-gpt/]\n--------------------------------------------------------------------------------\n\nFebruary\n\nFergus Kidd speaker - State of Open Conference 23\n\nFergus Kidd speaker - State of Open Conference 25Fergus Kidd Avanade â€“ Emerging\nTechnology Engineer Synthetic Data for AI, and AI for Synthetic data in open\nsource Open DataWednesday, February 8 â€¢ 10:35am-11:05am GMTSt James, 4th Floor\nAdd to my Schedule Synthetic Data for AI, and AI for Synthetic data in open\nsource A session covering the advanceâ€¦State of Open Conference 25Michelle Angert\n[https://stateofopencon.com/2023/01/07/fergus-kidd-2/]\n--------------------------------------------------------------------------------\n\nJanuary\n3D projected textures\nhttps://ferguskidd.com/generated-textures-but-with-depth/\n\n2022\nDecember\nSynthetic data â€“ Blender to Azure Custom Vision\nhttps://fergusblog.azurewebsites.net/0-data-vision-model/\n\n\n--------------------------------------------------------------------------------\n\nNovember\nDall-E textures continued\nhttps://fergusblog.azurewebsites.net/ai-generated-textures-now-in-full/\n\nhttps://github.com/FergusKidd/Seamless-Texture-Generation-with-DALL-E-2\n\nOpen UK awards - London\nRunner up â€“ Sustainability award for GSF work\n\nJudge â€“ Diversity and inclusion award\n\n\n--------------------------------------------------------------------------------\n\nOctober\nMicrosoft Ignite Presenter - Manchester â€“ state of generative AI, uses and\nethics.\n\n\n--------------------------------------------------------------------------------\n\nSeptember\nDall-E for textures\nhttps://fergusblog.azurewebsites.net/dall-e-2-2/\n\nOSS Summit in Dublin\nPresenter on Green software pipelines - Green Software Foundation\n\n\n--------------------------------------------------------------------------------\n\nAugust\nAltspace tips and tricks\nhttps://ferguskidd.com/altspace-media-player-and-azure-storage/\n\nUnevenly Distributed podcast with Jeff Vilimek\nhttps://ferguskidd.com/untitled/\n\n\n--------------------------------------------------------------------------------\n\nJuly\nAvanade Blog on Dall-E 2\n\nhttps://www.avanade.com/en/blogs/techs-and-specs/data-and-analytics/dall-e2-for-enterprises\n\nJunior Avhieve it Europe Podcast on metaverse\nhttps://www.youtube.com/watch?v=wIA70Bgxjgw\n\n\n--------------------------------------------------------------------------------\n\nJune\nDall-E blog\nhttps://ferguskidd.com/dall-e-2/\n\nAvanade blog on confidential ledger\nhttps://www.avanade.com/en/blogs/techs-and-specs/azure/confidential-ledger-flexibility\n\nBecame chair of GSF project â€“ carbon pipeline.\nhttps://github.com/Green-Software-Foundation/Carbon_CI_Pipeline_Tooling\n\n\n--------------------------------------------------------------------------------\n\nQuestions\nWhy do you want to be an MVP?\n\nâ€¢\tIâ€™m passionate about AI, passionate about technology, and passionate about\nchanging the world for the better with the skills I have built and continue to\ngrow. I believe the recognition that comes with being part of the MVP community\nwill give me a stronger platform to work from and share from. I hope that being\npart of the MVP community will also give me access to a diverse group of experts\nin their fields who I can continue to learn from, as well as find and explore\nnew avenues of giving back to the wider community. I am also passionate about\nSTEM education in young people and would want to use the MVP status to reach out\nto more students and young people to encourage them to pursue their passions in\ntechnology. I believe the resources offered as part of the MVP programme would\nallow me to build and share more adventurous and engaging demonstrations of\ntechnology on Microsoft Azure and allow me to host these demos in the public\ndomain for easier sharing and interactions online.\n\nWhat are the most impactful community contributions you've made in the last\nyear?\n\nâ€¢\tOver the last year I have grown my passion for sharing. From being invited as\na keynote speaker to talk at the UCL centre for AI, to continuing to drive\nimpact with junior achieve it Europe including being invited to speak with their\nCEO Salvatore Nigro about the Metaverse. Iâ€™ve also contributed to open-source\nwork on GitHub, notably promoting the adoption of Microsoft generative AI,\npresenting back the impact of Microsoft text, vision and speech services, as\nwell as openAI powered conversations in our Stretch RE1 robot. I also used a\nNextMind brain sensor to control the robot using only my mind, which has given\nme several opportunities to talk about that work in the public domain, where I\nalways stress the ease and power of implementations of Microsoft AI.\n\nWhat significant impact do you plan to make to the community in the upcoming\nyear?\n\nâ€¢\tIn the coming year I plan to continue to build, play, and share. I believe the\nmost impactful sessions to share passion in technology contain a compelling\nstory and vibrant demonstration that is relevant to the audience. I love to\nprove how easy it can be to reach a goal by starting from scratch with no\ntechnology experience. I plan to continue to engage with young people in STEM,\nand hopefully grow connections with local schools and university programs. I am\nespecially interested in the use of gaming for education, and plan to use gaming\nconcepts to engage young people in such topics as Metaverse, AI, and computer\nscience using tools like Microsoftâ€™s Minecraft for Education, Altspace VR, as\nwell as utilising Unity or unreal engine.\n\nWhat are you doing to make your community better? (e.g. charity work, diversity\nand inclusion efforts, education, mentoring, etc.)\n\nâ€¢\tI am an active member of Avanadeâ€™s LGBTQ+ inclusion workstream, a group that\nadvocates LGBTQ+ inclusion in the workplace in Europe and beyond. I work with a\nteam in the UK to put on events, panels, talks, and information campaigns to\nhighlight LGBTQ+ issues in the workplace and society. I am very proud that we\nhave grown this group to be inclusive and collaborative across Europe.\nâ€¢\tI mentor several groups and individuals from UCL as part of an Industry\neXchange Network program I set up within Avanade. I set technical projects for\nthem to complete, and mentor them, as well as providing technical guidance with\nMicrosoft Azure. In a normal academic year I mentor 10-20 students\nâ€¢\tI have been engaged with work experience programs at Avanade, welcoming in\nschool students with a passion for technology and engaging them with the work we\ndo around Microsoft. Specifically in terms of modern software engineering and\nartificial intelligence. This has been affected by Covid-19, but I hope to\ncontinue this in the future. I am also a registered STEM ambassador.\nâ€¢\tI am an active member of the Green Software Foundation, and chair of one of\nthe GSF projects. Working with companies and passionate people all around the\nworld to make a greener and more sustainable future in software.","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-08-08T10:47:19.000Z","updated_at":"2024-11-22T12:04:54.000Z","published_at":"2024-08-08T10:52:58.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"66e06f9355aa0d000107fa7a","uuid":"2784bda7-655e-43d1-b335-2a7362f8165f","title":"AI Videos with Runway","slug":"ai-videos","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/09/Untitled-start.png\",\"width\":1890,\"height\":1417}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/09/Untitled-2.png\",\"width\":1890,\"height\":1417}],[\"video\",{\"loop\":true,\"src\":\"__GHOST_URL__/content/media/2024/09/715fab5e-884b-4b54-8412-2c2caa9e4c93.mp4\",\"fileName\":\"715fab5e-884b-4b54-8412-2c2caa9e4c93.mp4\",\"width\":1280,\"height\":768,\"duration\":10.541667,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2024/09/media-thumbnail-ember115.jpg\",\"thumbnailWidth\":1280,\"thumbnailHeight\":768}],[\"video\",{\"loop\":true,\"src\":\"__GHOST_URL__/content/media/2024/09/6f778b11-e2c3-4f1f-a3a2-0eef695ce585.mp4\",\"fileName\":\"6f778b11-e2c3-4f1f-a3a2-0eef695ce585.mp4\",\"width\":1280,\"height\":768,\"duration\":10.541667,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2024/09/media-thumbnail-ember138.jpg\",\"thumbnailWidth\":1280,\"thumbnailHeight\":768}],[\"video\",{\"loop\":true,\"src\":\"__GHOST_URL__/content/media/2024/09/d345b91c-d274-4b0b-8726-58d3794d6b7f.mp4\",\"fileName\":\"d345b91c-d274-4b0b-8726-58d3794d6b7f.mp4\",\"width\":1280,\"height\":768,\"duration\":10.541667,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2024/09/media-thumbnail-ember150.jpg\",\"thumbnailWidth\":1280,\"thumbnailHeight\":768}]],\"markups\":[[\"a\",[\"href\",\"https://runwayml.com/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I've been playing a little with AI generated videos.\"],[1,[],0,0],[1,[],0,1],[0,[],0,\"I used a very basic setup, taking a photo of me, and extending it to the correct frame dimensions with photoshop's generative fill.\"]]],[10,0],[1,\"p\",[[0,[],0,\"became:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Then I simply ran it through \"],[0,[0],1,\"runway\"],[0,[],0,\", and looked at the results.\"]]],[10,2],[1,\"p\",[[0,[],0,\"This was the output with the least deformation of my face. (note I had a slightly different room option to start this one from).\"]]],[1,\"p\",[[0,[],0,\"But there were some more exciting examples:\"]]],[10,3],[1,\"p\",[[0,[],0,\"and:\"]]],[10,4],[1,\"p\",[[1,[],0,2],[0,[],0,\"These include some fun, dynamic, if not a bit mad, movements from Rory the robot.\"]]],[1,\"p\",[[1,[],0,3],[0,[],0,\"There is quite a lot of unwanted deformation here, and certainly, improvements to be made, but human faces and features are tough to get right, as our brains are incredibly good at detecting them and noticing things that go wrong. I am not a fan of the ageing process it seems to put me through when I move my head, though...\"],[1,[],0,4]]],[1,\"p\",[[0,[],0,\"Some incredible things to note though, are the really accurate shadows cast on the wall from the robot as it moves, these sorts of details will really add to the believability as the tools improve, the pace of which by the way, is rapid.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>I've been playing a little with AI generated videos.<br><br>I used a very basic setup, taking a photo of me, and extending it to the correct frame dimensions with photoshop's generative fill.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/09/Untitled-start.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1890\" height=\"1417\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/09/Untitled-start.png 600w, __GHOST_URL__/content/images/size/w1000/2024/09/Untitled-start.png 1000w, __GHOST_URL__/content/images/size/w1600/2024/09/Untitled-start.png 1600w, __GHOST_URL__/content/images/2024/09/Untitled-start.png 1890w\" sizes=\"(min-width: 720px) 720px\"></figure><p>became:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/09/Untitled-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1890\" height=\"1417\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/09/Untitled-2.png 600w, __GHOST_URL__/content/images/size/w1000/2024/09/Untitled-2.png 1000w, __GHOST_URL__/content/images/size/w1600/2024/09/Untitled-2.png 1600w, __GHOST_URL__/content/images/2024/09/Untitled-2.png 1890w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Then I simply ran it through <a href=\"https://runwayml.com/\">runway</a>, and looked at the results.</p><figure class=\"kg-card kg-video-card\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2024/09/715fab5e-884b-4b54-8412-2c2caa9e4c93.mp4\" poster=\"https://img.spacergif.org/v1/1280x768/0a/spacer.png\" width=\"1280\" height=\"768\" loop autoplay muted playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2024/09/media-thumbnail-ember115.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container kg-video-hide\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div></figure><p>This was the output with the least deformation of my face. (note I had a slightly different room option to start this one from).</p><p>But there were some more exciting examples:</p><figure class=\"kg-card kg-video-card\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2024/09/6f778b11-e2c3-4f1f-a3a2-0eef695ce585.mp4\" poster=\"https://img.spacergif.org/v1/1280x768/0a/spacer.png\" width=\"1280\" height=\"768\" loop autoplay muted playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2024/09/media-thumbnail-ember138.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container kg-video-hide\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div></figure><p>and:</p><figure class=\"kg-card kg-video-card\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2024/09/d345b91c-d274-4b0b-8726-58d3794d6b7f.mp4\" poster=\"https://img.spacergif.org/v1/1280x768/0a/spacer.png\" width=\"1280\" height=\"768\" loop autoplay muted playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2024/09/media-thumbnail-ember150.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container kg-video-hide\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div></figure><p><br>These include some fun, dynamic, if not a bit mad, movements from Rory the robot.</p><p><br>There is quite a lot of unwanted deformation here, and certainly, improvements to be made, but human faces and features are tough to get right, as our brains are incredibly good at detecting them and noticing things that go wrong. I am not a fan of the ageing process it seems to put me through when I move my head, though...<br></p><p>Some incredible things to note though, are the really accurate shadows cast on the wall from the robot as it moves, these sorts of details will really add to the believability as the tools improve, the pace of which by the way, is rapid.</p>","comment_id":"66e06f9355aa0d000107fa7a","plaintext":"I've been playing a little with AI generated videos.\n\nI used a very basic setup, taking a photo of me, and extending it to the correct\nframe dimensions with photoshop's generative fill.\n\nbecame:\n\nThen I simply ran it through runway [https://runwayml.com/], and looked at the\nresults.\n\n0:00/1Ã—This was the output with the least deformation of my face. (note I had a\nslightly different room option to start this one from).\n\nBut there were some more exciting examples:\n\n0:00/1Ã—and:\n\n0:00/1Ã—\nThese include some fun, dynamic, if not a bit mad, movements from Rory the\nrobot.\n\n\nThere is quite a lot of unwanted deformation here, and certainly, improvements\nto be made, but human faces and features are tough to get right, as our brains\nare incredibly good at detecting them and noticing things that go wrong. I am\nnot a fan of the ageing process it seems to put me through when I move my head,\nthough...\n\n\nSome incredible things to note though, are the really accurate shadows cast on\nthe wall from the robot as it moves, these sorts of details will really add to\nthe believability as the tools improve, the pace of which by the way, is rapid.","feature_image":"__GHOST_URL__/content/images/2024/09/Untitled-2-1.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-09-10T16:10:59.000Z","updated_at":"2024-09-10T16:22:12.000Z","published_at":"2024-09-10T16:22:13.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"6716273c28de6a00015ee27c","uuid":"30f10614-83b7-4d88-b40b-64f8bbc90376","title":"LIVE! AI Self Interview","slug":"live-ai-self-interview","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"video\",{\"loop\":false,\"src\":\"__GHOST_URL__/content/media/2024/10/Live-Self-Interview.mp4\",\"fileName\":\"Live Self Interview.mp4\",\"width\":1920,\"height\":1080,\"duration\":248.164583,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2024/10/media-thumbnail-ember149.jpg\",\"thumbnailWidth\":1920,\"thumbnailHeight\":1080}]],\"markups\":[[\"a\",[\"href\",\"https://app.heygen.com/labs\"]]],\"sections\":[[10,0],[1,\"p\",[[0,[],0,\"I interviewed my AI self live.\"],[1,[],0,0],[0,[],0,\"No edits, no cuts, no pre-processed answers, just a short conversation with myself...\"],[1,[],0,1],[1,[],0,2],[0,[],0,\"This uses a brand spanking new live interactive avatar from \"],[0,[0],1,\"HeyGen Labs\"],[0,[],0,\", and OpenAI with access to a small knowledge base about me. Now I can finally be in two places at once! HeyGen has already included Zoom integration, so it's easy for digital me to turn up to digital meetings. Fun for a prank, or revolutionary for 1-1 customer Question Answering, you decide.\"],[1,[],0,3],[1,[],0,4],[0,[],0,\"DISCLAIMER - real me has never pronounced data like that and I never will ðŸ˜¤\"]]]],\"ghostVersion\":\"4.0\"}","html":"<figure class=\"kg-card kg-video-card\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2024/10/Live-Self-Interview.mp4\" poster=\"https://img.spacergif.org/v1/1920x1080/0a/spacer.png\" width=\"1920\" height=\"1080\" playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2024/10/media-thumbnail-ember149.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div></figure><p>I interviewed my AI self live.<br>No edits, no cuts, no pre-processed answers, just a short conversation with myself...<br><br>This uses a brand spanking new live interactive avatar from <a href=\"https://app.heygen.com/labs\">HeyGen Labs</a>, and OpenAI with access to a small knowledge base about me. Now I can finally be in two places at once! HeyGen has already included Zoom integration, so it's easy for digital me to turn up to digital meetings. Fun for a prank, or revolutionary for 1-1 customer Question Answering, you decide.<br><br>DISCLAIMER - real me has never pronounced data like that and I never will ðŸ˜¤</p>","comment_id":"6716273c28de6a00015ee27c","plaintext":"0:00/1Ã—I interviewed my AI self live.\nNo edits, no cuts, no pre-processed answers, just a short conversation with\nmyself...\n\nThis uses a brand spanking new live interactive avatar from HeyGen Labs\n[https://app.heygen.com/labs], and OpenAI with access to a small knowledge base\nabout me. Now I can finally be in two places at once! HeyGen has already\nincluded Zoom integration, so it's easy for digital me to turn up to digital\nmeetings. Fun for a prank, or revolutionary for 1-1 customer Question Answering,\nyou decide.\n\nDISCLAIMER - real me has never pronounced data like that and I never will ðŸ˜¤","feature_image":"__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-21-at-11.05.42.png","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-10-21T10:04:44.000Z","updated_at":"2024-10-21T11:06:30.000Z","published_at":"2024-10-21T11:06:30.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"6721152e4deeec00014cc0d2","uuid":"e5e7d9a7-ae4a-4de4-9593-688e2f5d3ad0","title":"Talk to Fergus","slug":"talk-to-fergus","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"header\",{\"size\":\"small\",\"style\":\"accent\",\"buttonEnabled\":false,\"header\":\"Talk to Fergus\",\"subheader\":\"A digital experience\"}],[\"html\",{\"html\":\"<script>!function(window){const host=\\\"https://labs.heygen.com\\\",url=host+\\\"/guest/streaming-embed?share=eyJxdWFsaXR5IjoiaGlnaCIsImF2YXRhck5hbWUiOiJjNTU3NjAyY2RhZDQ0Yjc3YjU1YTRhODEx%0D%0AYzQ2YWVkNiIsInByZXZpZXdJbWciOiJodHRwczovL2ZpbGVzMi5oZXlnZW4uYWkvYXZhdGFyL3Yz%0D%0AL2M1NTc2MDJjZGFkNDRiNzdiNTVhNGE4MTFjNDZhZWQ2L2Z1bGwvMi4yL3ByZXZpZXdfdGFyZ2V0%0D%0ALndlYnAiLCJuZWVkUmVtb3ZlQmFja2dyb3VuZCI6ZmFsc2UsImtub3dsZWRnZUJhc2VJZCI6IjU5%0D%0AYTZmZGYyMTBjYTQwMzBhOWJkYTk5MjgxMmVkNWY4IiwidXNlcm5hbWUiOiIwZmE2MmNjMzFkODM0%0D%0AMmQ3OGJlZWQ4ZGU1MTM4N2FjOCJ9&inIFrame=1\\\",clientWidth=document.body.clientWidth,wrapDiv=document.createElement(\\\"div\\\");wrapDiv.id=\\\"heygen-streaming-embed\\\";const container=document.createElement(\\\"div\\\");container.id=\\\"heygen-streaming-container\\\";const stylesheet=document.createElement(\\\"style\\\");stylesheet.innerHTML=`\\\\n  #heygen-streaming-embed {\\\\n    z-index: 9999;\\\\n    position: fixed;\\\\n    left: 40px;\\\\n    bottom: 40px;\\\\n    width: 200px;\\\\n    height: 200px;\\\\n    border-radius: 50%;\\\\n    border: 2px solid #fff;\\\\n    box-shadow: 0px 8px 24px 0px rgba(0, 0, 0, 0.12);\\\\n    transition: all linear 0.1s;\\\\n    overflow: hidden;\\\\n\\\\n    opacity: 0;\\\\n    visibility: hidden;\\\\n  }\\\\n  #heygen-streaming-embed.show {\\\\n    opacity: 1;\\\\n    visibility: visible;\\\\n  }\\\\n  #heygen-streaming-embed.expand {\\\\n    ${clientWidth<540?\\\"height: 266px; width: 96%; left: 50%; transform: translateX(-50%);\\\":\\\"height: 366px; width: calc(366px * 16 / 9);\\\"}\\\\n    border: 0;\\\\n    border-radius: 8px;\\\\n  }\\\\n  #heygen-streaming-container {\\\\n    width: 100%;\\\\n    height: 100%;\\\\n  }\\\\n  #heygen-streaming-container iframe {\\\\n    width: 100%;\\\\n    height: 100%;\\\\n    border: 0;\\\\n  }\\\\n  `;const iframe=document.createElement(\\\"iframe\\\");iframe.allowFullscreen=!1,iframe.title=\\\"Streaming Embed\\\",iframe.role=\\\"dialog\\\",iframe.allow=\\\"microphone\\\",iframe.src=url;let visible=!1,initial=!1;window.addEventListener(\\\"message\\\",(e=>{e.origin===host&&e.data&&e.data.type&&\\\"streaming-embed\\\"===e.data.type&&(\\\"init\\\"===e.data.action?(initial=!0,wrapDiv.classList.toggle(\\\"show\\\",initial)):\\\"show\\\"===e.data.action?(visible=!0,wrapDiv.classList.toggle(\\\"expand\\\",visible)):\\\"hide\\\"===e.data.action&&(visible=!1,wrapDiv.classList.toggle(\\\"expand\\\",visible)))})),container.appendChild(iframe),wrapDiv.appendChild(stylesheet),wrapDiv.appendChild(container),document.body.appendChild(wrapDiv)}(globalThis);</script>\"}],[\"callout\",{\"calloutEmoji\":\"\",\"calloutText\":\"This is an example of an embedded instant HeyGen Avatar.&nbsp;\",\"backgroundColor\":\"blue\"}],[\"button\",{\"alignment\":\"center\",\"buttonText\":\"Make your own\",\"buttonUrl\":\"labs.heygen.com\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-29-at-17.14.28-1.png\",\"width\":3772,\"height\":2120}],[\"callout\",{\"calloutEmoji\":\"ðŸ’¡\",\"calloutText\":\"Please do not take financial or personal advice from Digital Fergus, or indeed, trust anything he says. Thank you.\",\"backgroundColor\":\"grey\"}]],\"markups\":[[\"a\",[\"href\",\"https://ferguskidd.com/live-ai-self-interview\"]]],\"sections\":[[10,0],[10,1],[1,\"p\",[[0,[],0,\"On this page you can talk to digital Fergus, click the chat now icon in the lower corner.\"]]],[1,\"p\",[[0,[],0,\"You'll need to give your browser access to your microphone too, a popup should show.\"]]],[10,2],[10,3],[1,\"p\",[[0,[],0,\"Or, find out more and see a demo:\"]]],[1,\"p\",[[0,[0],1,\"https://ferguskidd.com/live-ai-self-interview\"]]],[10,4],[10,5],[1,\"p\",[]],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<div class=\"kg-card kg-header-card kg-width-full kg-size-small kg-style-accent\" style=\"\" data-kg-background-image=\"\"><h2 class=\"kg-header-card-header\" id=\"talk-to-fergus\">Talk to Fergus</h2><h3 class=\"kg-header-card-subheader\" id=\"a-digital-experience\">A digital experience</h3></div><!--kg-card-begin: html--><script>!function(window){const host=\"https://labs.heygen.com\",url=host+\"/guest/streaming-embed?share=eyJxdWFsaXR5IjoiaGlnaCIsImF2YXRhck5hbWUiOiJjNTU3NjAyY2RhZDQ0Yjc3YjU1YTRhODEx%0D%0AYzQ2YWVkNiIsInByZXZpZXdJbWciOiJodHRwczovL2ZpbGVzMi5oZXlnZW4uYWkvYXZhdGFyL3Yz%0D%0AL2M1NTc2MDJjZGFkNDRiNzdiNTVhNGE4MTFjNDZhZWQ2L2Z1bGwvMi4yL3ByZXZpZXdfdGFyZ2V0%0D%0ALndlYnAiLCJuZWVkUmVtb3ZlQmFja2dyb3VuZCI6ZmFsc2UsImtub3dsZWRnZUJhc2VJZCI6IjU5%0D%0AYTZmZGYyMTBjYTQwMzBhOWJkYTk5MjgxMmVkNWY4IiwidXNlcm5hbWUiOiIwZmE2MmNjMzFkODM0%0D%0AMmQ3OGJlZWQ4ZGU1MTM4N2FjOCJ9&inIFrame=1\",clientWidth=document.body.clientWidth,wrapDiv=document.createElement(\"div\");wrapDiv.id=\"heygen-streaming-embed\";const container=document.createElement(\"div\");container.id=\"heygen-streaming-container\";const stylesheet=document.createElement(\"style\");stylesheet.innerHTML=`\\n  #heygen-streaming-embed {\\n    z-index: 9999;\\n    position: fixed;\\n    left: 40px;\\n    bottom: 40px;\\n    width: 200px;\\n    height: 200px;\\n    border-radius: 50%;\\n    border: 2px solid #fff;\\n    box-shadow: 0px 8px 24px 0px rgba(0, 0, 0, 0.12);\\n    transition: all linear 0.1s;\\n    overflow: hidden;\\n\\n    opacity: 0;\\n    visibility: hidden;\\n  }\\n  #heygen-streaming-embed.show {\\n    opacity: 1;\\n    visibility: visible;\\n  }\\n  #heygen-streaming-embed.expand {\\n    ${clientWidth<540?\"height: 266px; width: 96%; left: 50%; transform: translateX(-50%);\":\"height: 366px; width: calc(366px * 16 / 9);\"}\\n    border: 0;\\n    border-radius: 8px;\\n  }\\n  #heygen-streaming-container {\\n    width: 100%;\\n    height: 100%;\\n  }\\n  #heygen-streaming-container iframe {\\n    width: 100%;\\n    height: 100%;\\n    border: 0;\\n  }\\n  `;const iframe=document.createElement(\"iframe\");iframe.allowFullscreen=!1,iframe.title=\"Streaming Embed\",iframe.role=\"dialog\",iframe.allow=\"microphone\",iframe.src=url;let visible=!1,initial=!1;window.addEventListener(\"message\",(e=>{e.origin===host&&e.data&&e.data.type&&\"streaming-embed\"===e.data.type&&(\"init\"===e.data.action?(initial=!0,wrapDiv.classList.toggle(\"show\",initial)):\"show\"===e.data.action?(visible=!0,wrapDiv.classList.toggle(\"expand\",visible)):\"hide\"===e.data.action&&(visible=!1,wrapDiv.classList.toggle(\"expand\",visible)))})),container.appendChild(iframe),wrapDiv.appendChild(stylesheet),wrapDiv.appendChild(container),document.body.appendChild(wrapDiv)}(globalThis);</script><!--kg-card-end: html--><p>On this page you can talk to digital Fergus, click the chat now icon in the lower corner.</p><p>You'll need to give your browser access to your microphone too, a popup should show.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-text\">This is an example of an embedded instant HeyGen Avatar.&nbsp;</div></div><div class=\"kg-card kg-button-card kg-align-center\"><a href=\"labs.heygen.com\" class=\"kg-btn kg-btn-accent\">Make your own</a></div><p>Or, find out more and see a demo:</p><p><a href=\"https://ferguskidd.com/live-ai-self-interview\">https://ferguskidd.com/live-ai-self-interview</a></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-29-at-17.14.28-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/10/Screenshot-2024-10-29-at-17.14.28-1.png 600w, __GHOST_URL__/content/images/size/w1000/2024/10/Screenshot-2024-10-29-at-17.14.28-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2024/10/Screenshot-2024-10-29-at-17.14.28-1.png 1600w, __GHOST_URL__/content/images/size/w2400/2024/10/Screenshot-2024-10-29-at-17.14.28-1.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-emoji\">ðŸ’¡</div><div class=\"kg-callout-text\">Please do not take financial or personal advice from Digital Fergus, or indeed, trust anything he says. Thank you.</div></div><p></p>","comment_id":"6721152e4deeec00014cc0d2","plaintext":"Talk to Fergus\nA digital experience\nOn this page you can talk to digital Fergus, click the chat now icon in the\nlower corner.\n\nYou'll need to give your browser access to your microphone too, a popup should\nshow.\n\nThis is an example of an embedded instant HeyGen Avatar.Make your own\n[labs.heygen.com]Or, find out more and see a demo:\n\nhttps://ferguskidd.com/live-ai-self-interview\n\nðŸ’¡Please do not take financial or personal advice from Digital Fergus, or\nindeed, trust anything he says. Thank you.","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-10-29T17:02:38.000Z","updated_at":"2025-05-19T18:16:45.000Z","published_at":"2024-10-29T17:04:25.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"674071193c358200014c4f2e","uuid":"dce04c78-734c-4e86-b135-934ddbf8e9ef","title":"The Human Agent Connection","slug":"the-human-agent-connection","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=LGOBPPi5_gw&list=PLKN6Sz7yuHvWP1_rzakBox88rdhJGJTfd&index=2\",\"html\":\"<iframe width=\\\"200\\\" height=\\\"113\\\" src=\\\"https://www.youtube.com/embed/LGOBPPi5_gw?list=PLKN6Sz7yuHvWP1_rzakBox88rdhJGJTfd\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\\" referrerpolicy=\\\"strict-origin-when-cross-origin\\\" allowfullscreen></iframe>\",\"type\":\"video\",\"metadata\":{\"title\":\"The Human Agent Connection | Go HandsFree\",\"author_name\":\"RealWear, Inc\",\"author_url\":\"https://www.youtube.com/@RealWearInc\",\"height\":113,\"width\":200,\"version\":\"1.0\",\"provider_name\":\"YouTube\",\"provider_url\":\"https://www.youtube.com/\",\"thumbnail_height\":360,\"thumbnail_width\":480,\"thumbnail_url\":\"https://i.ytimg.com/vi/LGOBPPi5_gw/hqdefault.jpg\"}}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[[0,[],0,\"Episode 5 - The Human Agent Connection\"],[1,[],0,0]]],[1,\"p\",[[0,[],0,\"I got to join George Sims and James Woodall on their 'GoHAndsFree' podcast to chat about RealWear and the future of Human and AI collaborative systems.\"],[1,[],0,1]]]],\"ghostVersion\":\"4.0\"}","html":"<figure class=\"kg-card kg-embed-card\"><iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/LGOBPPi5_gw?list=PLKN6Sz7yuHvWP1_rzakBox88rdhJGJTfd\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></figure><p>Episode 5 - The Human Agent Connection<br></p><p>I got to join George Sims and James Woodall on their 'GoHAndsFree' podcast to chat about RealWear and the future of Human and AI collaborative systems.<br></p>","comment_id":"674071193c358200014c4f2e","plaintext":"Episode 5 - The Human Agent Connection\n\n\nI got to join George Sims and James Woodall on their 'GoHAndsFree' podcast to\nchat about RealWear and the future of Human and AI collaborative systems.","feature_image":"__GHOST_URL__/content/images/2024/11/Screenshot-2024-11-22-at-11.57.23.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-11-22T11:55:05.000Z","updated_at":"2024-11-22T11:57:52.000Z","published_at":"2024-11-22T11:57:52.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"},{"id":"67ffef9f08d10000010e835a","uuid":"829fed46-b2b5-4dc2-98cf-6484082dbc62","title":"AI generated music video with Suno and Sora","slug":"ai-generated-music-videos-with-suno-and-sora","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"video\",{\"loop\":false,\"caption\":\"An AI generated music video based on random words sourced from inputs on a social media post\",\"src\":\"__GHOST_URL__/content/media/2025/04/My-Movie-1.mp4\",\"fileName\":\"My Movie 1.mp4\",\"width\":960,\"height\":540,\"duration\":206.573042,\"mimeType\":\"video/mp4\",\"thumbnailSrc\":\"__GHOST_URL__/content/images/2025/04/media-thumbnail-ember191.jpg\",\"thumbnailWidth\":960,\"thumbnailHeight\":540}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<figure class=\"kg-card kg-video-card kg-card-hascaption\"><div class=\"kg-video-container\"><video src=\"__GHOST_URL__/content/media/2025/04/My-Movie-1.mp4\" poster=\"https://img.spacergif.org/v1/960x540/0a/spacer.png\" width=\"960\" height=\"540\" playsinline preload=\"metadata\" style=\"background: transparent url('__GHOST_URL__/content/images/2025/04/media-thumbnail-ember191.jpg') 50% 50% / cover no-repeat;\" /></video><div class=\"kg-video-overlay\"><button class=\"kg-video-large-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button></div><div class=\"kg-video-player-container\"><div class=\"kg-video-player\"><button class=\"kg-video-play-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"/></svg></button><button class=\"kg-video-pause-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/><rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"/></svg></button><span class=\"kg-video-current-time\">0:00</span><div class=\"kg-video-time\">/<span class=\"kg-video-duration\"></span></div><input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\"><button class=\"kg-video-playback-rate\">1&#215;</button><button class=\"kg-video-unmute-icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"/></svg></button><button class=\"kg-video-mute-icon kg-video-hide\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"/></svg></button><input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\"></div></div></div><figcaption>An AI generated music video based on random words sourced from inputs on a social media post</figcaption></figure>","comment_id":"67ffef9f08d10000010e835a","plaintext":"0:00/1Ã—An AI generated music video based on random words sourced from inputs on\na social media post","feature_image":"__GHOST_URL__/content/images/2025/04/Screenshot-2025-04-16-at-18.58.46.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2025-04-16T17:57:51.000Z","updated_at":"2025-04-16T18:06:05.000Z","published_at":"2025-04-16T18:01:42.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":"628bad72d182030001125d37"}],"posts_authors":[{"id":"628bad7dd182030001125da2","post_id":"628bad7bd182030001125da1","author_id":"1","sort_order":0},{"id":"628bad7dd182030001125da4","post_id":"628bad7dd182030001125da3","author_id":"1","sort_order":0},{"id":"628baf4ed182030001125f1a","post_id":"628baf4ed182030001125f19","author_id":"1","sort_order":0},{"id":"628bb171d182030001125f2f","post_id":"628bb170d182030001125f2e","author_id":"1","sort_order":0},{"id":"628cc6432c055d000197a0f3","post_id":"628cc6432c055d000197a0f2","author_id":"1","sort_order":0},{"id":"628ccbf62c055d000197a104","post_id":"628ccbf62c055d000197a103","author_id":"1","sort_order":0},{"id":"628ccc6a2c055d000197a116","post_id":"628ccc6a2c055d000197a115","author_id":"1","sort_order":0},{"id":"628ccd2e2c055d000197a140","post_id":"628ccd2e2c055d000197a13f","author_id":"1","sort_order":0},{"id":"628ccd952c055d000197a14c","post_id":"628ccd952c055d000197a14b","author_id":"1","sort_order":0},{"id":"62dfbcfcff7f9e0001ab81e5","post_id":"62dfbcfbff7f9e0001ab81e4","author_id":"1","sort_order":0},{"id":"62fa557eff7f9e0001ab82ae","post_id":"62fa557dff7f9e0001ab82ad","author_id":"1","sort_order":0},{"id":"63089e40ff7f9e0001ab8319","post_id":"63089e40ff7f9e0001ab8318","author_id":"1","sort_order":0},{"id":"6308a077ff7f9e0001ab8331","post_id":"6308a077ff7f9e0001ab8330","author_id":"1","sort_order":0},{"id":"63359a3c3018ba0001441840","post_id":"63359a3b3018ba000144183f","author_id":"1","sort_order":0},{"id":"63653907c9b3c50001bc6760","post_id":"63653904c9b3c50001bc675f","author_id":"1","sort_order":0},{"id":"639c46c856bf880001dc451f","post_id":"639c46c456bf880001dc451e","author_id":"1","sort_order":0},{"id":"63c1772fcf151900013c1ef3","post_id":"63c1772dcf151900013c1ef2","author_id":"1","sort_order":0},{"id":"6408a781cf151900013c1f34","post_id":"6408a781cf151900013c1f33","author_id":"1","sort_order":0},{"id":"6419a315cf151900013c1f61","post_id":"6419a314cf151900013c1f60","author_id":"1","sort_order":0},{"id":"64884ffa6f01a50001988207","post_id":"64884ff86f01a50001988206","author_id":"1","sort_order":0},{"id":"648852946f01a5000198823e","post_id":"648852936f01a5000198823d","author_id":"1","sort_order":0},{"id":"6540d51f3ffd0500016acfe2","post_id":"6540d51d3ffd0500016acfe1","author_id":"1","sort_order":0},{"id":"65b3b58d1304490001044395","post_id":"65b3b58d1304490001044394","author_id":"1","sort_order":0},{"id":"660c2e6d63a75d00015fb599","post_id":"660c2e6c63a75d00015fb598","author_id":"1","sort_order":0},{"id":"6661c44e3c916b0001ce045b","post_id":"6661c44c3c916b0001ce045a","author_id":"1","sort_order":0},{"id":"666713d93c916b0001ce0464","post_id":"666713d93c916b0001ce0463","author_id":"1","sort_order":0},{"id":"669ac2cfe5007a0001ba104e","post_id":"669ac2cde5007a0001ba104d","author_id":"1","sort_order":0},{"id":"66acf3fae5007a0001ba10b3","post_id":"66acf3fae5007a0001ba10b2","author_id":"1","sort_order":0},{"id":"66b4a238e5007a0001ba10d6","post_id":"66b4a237e5007a0001ba10d5","author_id":"1","sort_order":0},{"id":"66e06f9555aa0d000107fa7b","post_id":"66e06f9355aa0d000107fa7a","author_id":"1","sort_order":0},{"id":"6716273c28de6a00015ee27d","post_id":"6716273c28de6a00015ee27c","author_id":"1","sort_order":0},{"id":"672115304deeec00014cc0d3","post_id":"6721152e4deeec00014cc0d2","author_id":"1","sort_order":0},{"id":"6740711a3c358200014c4f2f","post_id":"674071193c358200014c4f2e","author_id":"1","sort_order":0},{"id":"67ffefa108d10000010e835b","post_id":"67ffef9f08d10000010e835a","author_id":"1","sort_order":0}],"posts_meta":[{"id":"639c492b56bf880001dc4541","post_id":"639c46c456bf880001dc451e","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"Training images for the 0 data tennis ball model","email_only":0},{"id":"6408a943cf151900013c1f55","post_id":"6408a781cf151900013c1f33","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"AI Generated Slender Robot","email_only":0},{"id":"671627b528de6a00015ee28c","post_id":"6716273c28de6a00015ee27c","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"Fergus talks to.... Fergus, via HeyGen Labs Interactive Avatar","email_only":0}],"posts_tags":[{"id":"628bad87d182030001125ea5","post_id":"628bad7bd182030001125da1","tag_id":"628bad72d182030001125d38","sort_order":0},{"id":"628cea352c055d000197a16a","post_id":"628ccc6a2c055d000197a115","tag_id":"628cea352c055d000197a168","sort_order":0},{"id":"628cea352c055d000197a16b","post_id":"628ccc6a2c055d000197a115","tag_id":"628cea352c055d000197a169","sort_order":1},{"id":"628cea432c055d000197a170","post_id":"628ccd2e2c055d000197a13f","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"628cea432c055d000197a171","post_id":"628ccd2e2c055d000197a13f","tag_id":"628cea352c055d000197a169","sort_order":1},{"id":"628cea4f2c055d000197a174","post_id":"628cc6432c055d000197a0f2","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"628cea612c055d000197a176","post_id":"628ccbf62c055d000197a103","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"628cea612c055d000197a177","post_id":"628ccbf62c055d000197a103","tag_id":"628cea352c055d000197a169","sort_order":1},{"id":"62dfbddcff7f9e0001ab81f4","post_id":"62dfbcfbff7f9e0001ab81e4","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"62fa5a0fff7f9e0001ab830e","post_id":"62fa557dff7f9e0001ab82ad","tag_id":"628cea352c055d000197a168","sort_order":0},{"id":"62fa5a0fff7f9e0001ab830f","post_id":"62fa557dff7f9e0001ab82ad","tag_id":"62fa5a0fff7f9e0001ab830d","sort_order":1},{"id":"6308a056ff7f9e0001ab832d","post_id":"63089e40ff7f9e0001ab8318","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"6308a056ff7f9e0001ab832e","post_id":"63089e40ff7f9e0001ab8318","tag_id":"62fa5a0fff7f9e0001ab830d","sort_order":1},{"id":"63359e393018ba0001441883","post_id":"63359a3b3018ba000144183f","tag_id":"628cea352c055d000197a168","sort_order":0},{"id":"63359e393018ba0001441884","post_id":"63359a3b3018ba000144183f","tag_id":"628cea432c055d000197a16f","sort_order":1},{"id":"6408a962cf151900013c1f5d","post_id":"6408a781cf151900013c1f33","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"648865dd6f01a50001988281","post_id":"648852936f01a5000198823d","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"648866256f01a50001988283","post_id":"64884ff86f01a50001988206","tag_id":"628cea352c055d000197a169","sort_order":0},{"id":"65b3c04b13044900010443b3","post_id":"65b3b58d1304490001044394","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"660c305963a75d00015fb5af","post_id":"660c2e6c63a75d00015fb598","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"660c305963a75d00015fb5b0","post_id":"660c2e6c63a75d00015fb598","tag_id":"628cea352c055d000197a169","sort_order":1},{"id":"666714003c916b0001ce046d","post_id":"666713d93c916b0001ce0463","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"669ac7bce5007a0001ba10a6","post_id":"669ac2cde5007a0001ba104d","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"66e0720b55aa0d000107fabc","post_id":"66e06f9355aa0d000107fa7a","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"6716279a28de6a00015ee288","post_id":"6716273c28de6a00015ee27c","tag_id":"628cea432c055d000197a16f","sort_order":0},{"id":"6716279a28de6a00015ee289","post_id":"6716273c28de6a00015ee27c","tag_id":"628bad72d182030001125d38","sort_order":1},{"id":"67fff07208d10000010e8366","post_id":"67ffef9f08d10000010e835a","tag_id":"628cea432c055d000197a16f","sort_order":0}],"roles":[{"id":"628bad72d182030001125d39","name":"Administrator","description":"Administrators","created_at":"2022-05-23T15:51:14.000Z","updated_at":"2022-05-23T15:51:14.000Z"},{"id":"628bad72d182030001125d3a","name":"Editor","description":"Editors","created_at":"2022-05-23T15:51:14.000Z","updated_at":"2022-05-23T15:51:14.000Z"},{"id":"628bad72d182030001125d3b","name":"Author","description":"Authors","created_at":"2022-05-23T15:51:14.000Z","updated_at":"2022-05-23T15:51:14.000Z"},{"id":"628bad72d182030001125d3c","name":"Contributor","description":"Contributors","created_at":"2022-05-23T15:51:14.000Z","updated_at":"2022-05-23T15:51:14.000Z"},{"id":"628bad72d182030001125d3d","name":"Owner","description":"Blog Owner","created_at":"2022-05-23T15:51:14.000Z","updated_at":"2022-05-23T15:51:14.000Z"},{"id":"628bad72d182030001125d3e","name":"Admin Integration","description":"External Apps","created_at":"2022-05-23T15:51:14.000Z","updated_at":"2022-05-23T15:51:14.000Z"},{"id":"628bad72d182030001125d3f","name":"DB Backup Integration","description":"Internal DB Backup Client","created_at":"2022-05-23T15:51:14.000Z","updated_at":"2022-05-23T15:51:14.000Z"},{"id":"628bad72d182030001125d40","name":"Scheduler Integration","description":"Internal Scheduler Client","created_at":"2022-05-23T15:51:14.000Z","updated_at":"2022-05-23T15:51:14.000Z"}],"roles_users":[{"id":"628bad87d182030001125ea6","role_id":"628bad72d182030001125d3d","user_id":"1"}],"settings":[{"id":"628bad8bd182030001125ea7","group":"core","key":"db_hash","value":"19a3ffd8-1484-4573-b33b-e9e2f4126f0b","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ea8","group":"core","key":"routes_hash","value":"3d180d52c663d173a6be791ef411ed01","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:49.000Z"},{"id":"628bad8bd182030001125ea9","group":"core","key":"next_update_check","value":"1748602774","type":"number","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2025-05-29T10:59:34.000Z"},{"id":"628bad8bd182030001125eaa","group":"core","key":"notifications","value":"[{\"dismissible\":true,\"location\":\"bottom\",\"status\":\"alert\",\"id\":\"472389b0-d899-11ec-b68b-51e7cbceb2d9\",\"createdAtVersion\":\"4.47.4\",\"custom\":false,\"createdAt\":\"2022-05-20T23:02:17.000Z\",\"type\":\"info\",\"top\":false,\"message\":\"Ghost <a href=\\\"https://github.com/TryGhost/Ghost/releases\\\">4.48.0</a> has been released, <a href=\\\"https://ghost.org/update/?v=4.47.4\\\">click here</a> to upgrade.\",\"seen\":false,\"addedAt\":\"2022-05-23T15:52:19.167Z\"},{\"dismissible\":true,\"location\":\"bottom\",\"status\":\"alert\",\"id\":\"0d88bf51-ab48-4a7f-a867-e5096e0026ad\",\"createdAtVersion\":\"4.47.4\",\"custom\":true,\"createdAt\":\"2024-08-21T12:01:19.000Z\",\"type\":\"alert\",\"top\":false,\"message\":\"Critical security update available â€” please update Ghost as soon as possible. <a href=\\\"https://github.com/TryGhost/Ghost/security/advisories/GHSA-78x2-cwp9-5j42\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Details here.</a>\",\"seen\":true,\"addedAt\":\"2024-08-29T22:12:45.649Z\",\"seenBy\":[\"1\"]}]","type":"array","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2024-09-10T16:21:39.000Z"},{"id":"628bad8bd182030001125eab","group":"core","key":"version_notifications","value":"[]","type":"array","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eac","group":"core","key":"session_secret","value":"a3f7eec04d293235e0f6f75cb1782d183166d364471f4f1c110b17ad3c34a4cb","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ead","group":"core","key":"theme_session_secret","value":"2f289245cd2fb48c8def17c19ae8b34f07fdf8e5ca0378f72ec9ae90272b19ac","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eae","group":"core","key":"ghost_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAJXB6BcyC6+L6tXVOfqHSBhEcpGCKxy3MpBXP+Aovh3KdGd1+hxuCslwIXztWOg8\nXB5bTjADz5R5KtPFHsnz6t6TbeGaT4c0T8EpCaR6YQlFz9Cc6EGTgGZA5s3CaqOsOSw1Pwkv\nOleTxUZPuFKZjzAvpXzW2bisMNtlqBUuAAprAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eaf","group":"core","key":"ghost_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICWwIBAAKBgQCVwegXMguvi+rV1Tn6h0gYRHKRgisctzKQVz/gKL4dynRndfocbgrJcCF8\n7VjoPFweW04wA8+UeSrTxR7J8+rek23hmk+HNE/BKQmkemEJRc/QnOhBk4BmQObNwmqjrDks\nNT8JLzpXk8VGT7hSmY8wL6V81tm4rDDbZagVLgAKawIDAQABAoGAU0g4p92e/gsTl8MysQ5W\nm5hFFoKLMzb2GatLzH8b6zlRrs+/Pdw8h3WiSfU8gjo/CGrCepVD9U/E72wqCSlqrWq9t3Gb\nwex6WoZOdyOy7mkKO2dzfDJ+JgHiskqwLnGLZKeOz7S8fUnJYmDeL5s3WfnW+h+y+08XWpq2\n5jZ8AgECQQDuz18GwrKy+3AHcsPEPHPx47Cjv9KPznverrFuR+vjQjdhot3IYiGWUMByutqZ\nuvV+iudUdiLl/V/e69ISe2HrAkEAoImI6rXsn8/dFq5Xlgc8Y5Th8FC9h6Wpexf0ACqTsnmE\nzLrtGepVN5wjOqxoTDPG0GWA01525VPEcy8OrxtZgQJATIweCp24CBT+iPSpeapjA+MXCLYA\n+WWXYP6gExhsrTDZbat16bhCydrihSQN0/8Ql9pQYktnzk7UPzFZ6eCwtwJAYwGvYSFll4H4\nlh5bgflgcwHE/hKtcNToWzQAlYGsqStsjV7TD2KHv84Zo/vkLurXuHqBItpldTVeOLNS2QeD\nAQJAIQP1t4oOfzDLhx94Oj5Z+J+zJxFeiqp1Es5ArTmqhTrzld9PU/mev9oR6fmeqaDFNwat\nNydGkgAzsca9k1DdfA==\n-----END RSA PRIVATE KEY-----\n","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eb0","group":"core","key":"members_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAIjLtqRKO2iFDDa/CpMnOc6qSmaJ6fulK1/t6EzPORvDH1CkmadgJ3Y3JBK55QTH\nQte9eYLSbPvkeoJRNWxFyqGFu2Xe95QMwnZGyL/4gcOqYDMRksi47bm55C5Cy38XslRDD9A9\neWLreoErrdO3vdDc8eDM5fU+k6EBydNAqQadAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eb1","group":"core","key":"members_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXgIBAAKBgQCIy7akSjtohQw2vwqTJznOqkpmien7pStf7ehMzzkbwx9QpJmnYCd2NyQS\nueUEx0LXvXmC0mz75HqCUTVsRcqhhbtl3veUDMJ2Rsi/+IHDqmAzEZLIuO25ueQuQst/F7JU\nQw/QPXli63qBK63Tt73Q3PHgzOX1PpOhAcnTQKkGnQIDAQABAoGAUpUIsa3Op46KjRJPw/5O\nO6E6QSTJCI3x3Xu2XYhMJaJp6Tk1OYIm0IsBT/zBcV0J0UwlawC1omuQOdPkuHR5mp85LSAu\n7ZEV7VRQUwPAtyLsaqkpGBWT294q/spuARSgQItsU6ETaumuJ+bFVd8ykYyQxByY/e2NbkLt\nUv0nZdECQQC8jMZOZvgmzzUti7h5Bt0+FOwOpCX5I0kN2UWtEjomHg0r6s3SLBHujaU52yER\n9yRzwfHq2v2gxOD0N9/vdqCTAkEAubtXk+dAP+TShARt8FqN/HXX3QrW1STWRakKxBLMeKY7\neLBJiLx0SLoetuhHhJ5ERFDtrv8E7eQaqfp51hyqDwJBAKWckOJ0lyqfffoPa20ClnndLqp9\ni80VLSm42fZ2uBSa+L7GcxpUEzwRtjrnOljpV19FzxOWXohD5cMdANj4vXkCQQCama7UlnGM\nvdPdwDIjF72IoC8G97bLxNX3NjaX7230H6rxUwxinTvOLd7TMZYWdgctNpAiUnSv8GehhbVU\n3JdRAkEAuyl8sWUPu//2Xthl4ZC7mT0fF4JsdW6s8pmMX5cKdnFiJK5rxViK1vuKKS/AFdr8\nr6QonXuka79pUjaeyZ2tPw==\n-----END RSA PRIVATE KEY-----\n","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eb2","group":"core","key":"members_email_auth_secret","value":"1c0440d75e1aefb19f61e6d25085fb5dbf0d51f351475a69d4982fe825c8549a883ec7a461045b153a55f214c1b745596d3715b261c15c99d2de8f540e32b80f","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eb5","group":"site","key":"title","value":"Fergus Kidd","type":"string","flags":"PUBLIC","created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-06-07T10:53:57.000Z"},{"id":"628bad8bd182030001125eb6","group":"site","key":"description","value":"Fergus Kidd","type":"string","flags":"PUBLIC","created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-06-07T10:53:57.000Z"},{"id":"628bad8bd182030001125eb7","group":"site","key":"logo","value":"__GHOST_URL__/content/images/2022/06/Screenshot-2022-06-07-at-11.51.17.png","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-06-07T10:52:41.000Z"},{"id":"628bad8bd182030001125eb8","group":"site","key":"cover_image","value":"https://static.ghost.org/v4.0.0/images/publication-cover.jpg","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eb9","group":"site","key":"icon","value":"","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eba","group":"site","key":"accent_color","value":"#346be9","type":"string","flags":"PUBLIC","created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-06-07T10:56:03.000Z"},{"id":"628bad8bd182030001125ebb","group":"site","key":"lang","value":"en","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ebc","group":"site","key":"timezone","value":"Etc/UTC","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ebd","group":"site","key":"codeinjection_head","value":"","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ebe","group":"site","key":"codeinjection_foot","value":"","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ebf","group":"site","key":"facebook","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-06-07T10:57:31.000Z"},{"id":"628bad8bd182030001125ec0","group":"site","key":"twitter","value":"@ferguskidd","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-06-07T10:57:31.000Z"},{"id":"628bad8bd182030001125ec1","group":"site","key":"navigation","value":"[{\"label\":\"Home\",\"url\":\"https://ferguskidd.com\"},{\"label\":\"About Fergus\",\"url\":\"https://ferguskidd.com/about-fergus/\"},{\"label\":\"3D Art\",\"url\":\"https://ferguskidd.com/3D-art/\"},{\"label\":\"External References\",\"url\":\"https://ferguskidd.com/external-references/\"},{\"label\":\"LinkedIn\",\"url\":\"https://www.linkedin.com/in/fergus-kidd-1a222667/\"},{\"label\":\"Microsoft MVP\",\"url\":\"https://mvp.microsoft.com/en-US/MVP/profile/f1ce0fb9-8c9d-4b20-bec2-46859c2c874e\"}]","type":"array","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2024-10-02T15:19:52.000Z"},{"id":"628bad8bd182030001125ec2","group":"site","key":"secondary_navigation","value":"[{\"label\":\"Sign up\",\"url\":\"#/portal/\"}]","type":"array","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ec3","group":"site","key":"meta_title","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ec4","group":"site","key":"meta_description","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ec5","group":"site","key":"og_image","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ec6","group":"site","key":"og_title","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ec7","group":"site","key":"og_description","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ec8","group":"site","key":"twitter_image","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ec9","group":"site","key":"twitter_title","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eca","group":"site","key":"twitter_description","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ecb","group":"theme","key":"active_theme","value":"ruby","type":"string","flags":"RO","created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-24T14:08:32.000Z"},{"id":"628bad8bd182030001125ecc","group":"private","key":"is_private","value":"false","type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ecd","group":"private","key":"password","value":"","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ece","group":"private","key":"public_hash","value":"f8a252d5461cef0bc23ae8aa6b02af","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ecf","group":"members","key":"default_content_visibility","value":"public","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ed0","group":"members","key":"default_content_visibility_tiers","value":"[]","type":"array","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ed1","group":"members","key":"members_signup_access","value":"all","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ed2","group":"members","key":"members_from_address","value":"noreply","type":"string","flags":"RO","created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ed3","group":"members","key":"members_support_address","value":"noreply","type":"string","flags":"PUBLIC,RO","created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ed4","group":"members","key":"members_reply_address","value":"newsletter","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ed5","group":"members","key":"members_free_signup_redirect","value":"/","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ed6","group":"members","key":"members_paid_signup_redirect","value":"/","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ed7","group":"members","key":"stripe_product_name","value":"Ghost Subscription","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eda","group":"members","key":"stripe_plans","value":"[]","type":"array","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125edd","group":"members","key":"stripe_connect_livemode","value":null,"type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ede","group":"members","key":"stripe_connect_display_name","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ee0","group":"members","key":"members_free_price_name","value":"Free","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ee1","group":"members","key":"members_free_price_description","value":"Free preview","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ee2","group":"members","key":"members_monthly_price_id","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ee3","group":"members","key":"members_yearly_price_id","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ee4","group":"portal","key":"portal_name","value":"true","type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ee5","group":"portal","key":"portal_button","value":"true","type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ee6","group":"portal","key":"portal_plans","value":"[\"free\"]","type":"array","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ee7","group":"portal","key":"portal_products","value":"[]","type":"array","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ee8","group":"portal","key":"portal_button_style","value":"icon-and-text","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ee9","group":"portal","key":"portal_button_icon","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eea","group":"portal","key":"portal_button_signup_text","value":"Subscribe","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eeb","group":"email","key":"mailgun_domain","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eec","group":"email","key":"mailgun_api_key","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eed","group":"email","key":"mailgun_base_url","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eee","group":"email","key":"email_track_opens","value":"true","type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ef0","group":"amp","key":"amp","value":"false","type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ef1","group":"amp","key":"amp_gtag_id","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ef2","group":"firstpromoter","key":"firstpromoter","value":"false","type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ef3","group":"firstpromoter","key":"firstpromoter_id","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ef4","group":"labs","key":"labs","value":"{}","type":"object","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ef5","group":"slack","key":"slack_url","value":"","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2024-10-02T15:19:52.000Z"},{"id":"628bad8bd182030001125ef6","group":"slack","key":"slack_username","value":"Ghost","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2024-10-02T15:19:52.000Z"},{"id":"628bad8bd182030001125ef7","group":"unsplash","key":"unsplash","value":"true","type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ef8","group":"views","key":"shared_views","value":"[]","type":"array","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125ef9","group":"newsletter","key":"newsletter_show_badge","value":"true","type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125efa","group":"newsletter","key":"newsletter_header_image","value":null,"type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125efb","group":"newsletter","key":"newsletter_show_header_icon","value":"true","type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125efc","group":"newsletter","key":"newsletter_show_header_title","value":"true","type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125efd","group":"newsletter","key":"newsletter_title_alignment","value":"center","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125efe","group":"newsletter","key":"newsletter_title_font_category","value":"sans_serif","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125eff","group":"newsletter","key":"newsletter_show_feature_image","value":"true","type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125f00","group":"newsletter","key":"newsletter_body_font_category","value":"sans_serif","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125f01","group":"newsletter","key":"newsletter_footer_content","value":"","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125f04","group":"editor","key":"editor_default_email_recipients","value":"visibility","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125f05","group":"editor","key":"editor_default_email_recipients_filter","value":"all","type":"string","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"},{"id":"628bad8bd182030001125f06","group":"editor","key":"editor_is_launch_complete","value":"false","type":"boolean","flags":null,"created_at":"2022-05-23T15:51:39.000Z","updated_at":"2022-05-23T15:51:39.000Z"}],"tags":[{"id":"628bad72d182030001125d38","name":"News","slug":"news","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-05-23T15:51:14.000Z","updated_at":"2022-05-23T15:51:14.000Z"},{"id":"628cea352c055d000197a168","name":"3D","slug":"3d","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-05-24T14:22:45.000Z","updated_at":"2022-05-24T14:22:45.000Z"},{"id":"628cea352c055d000197a169","name":"Hardware","slug":"hardware","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-05-24T14:22:45.000Z","updated_at":"2022-05-24T14:22:45.000Z"},{"id":"628cea432c055d000197a16f","name":"AI","slug":"ai","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-05-24T14:22:59.000Z","updated_at":"2022-05-24T14:22:59.000Z"},{"id":"62fa5a0fff7f9e0001ab830d","name":"Metaverse","slug":"metaverse","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-08-15T14:37:03.000Z","updated_at":"2022-08-15T14:37:03.000Z"}],"users":[{"id":"1","name":"Fergus Kidd","slug":"fergus","password":"$2a$10$AWbb9WsHFjpgNnbVozA7ieNxdZQxkyiEdlzZq6x.QKzxU3UYK5Bry","email":"fergus.e.kidd@avanade.com","profile_image":"__GHOST_URL__/content/images/2024/01/Fergus-Kidd.jpeg","cover_image":null,"bio":null,"website":null,"location":"London UK","facebook":null,"twitter":"@FergusKidd","accessibility":"{\"nightShift\":false}","status":"active","locale":null,"visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2025-05-29T10:59:33.000Z","created_at":"2022-05-23T15:51:22.000Z","updated_at":"2025-05-29T10:59:33.000Z"}]}}]}